CPUs are today much more sophisticated than they were
only 25 years ago. In those days, the frequency of the
CPU core was at a level equivalent to that of the mem-
ory bus. Memory access was only a bit slower than reg-
ister access. But this changed dramatically in the early
90s, when CPU designers increased the frequency of the
CPU core but the frequency of the memory bus and the
performance of RAM chips did not increase proportion-
ally. This is not due to the fact that faster RAM could
not be built, as explained in the previous section. It is
possible but it is not economical. RAM as fast as current
CPU cores is orders of magnitude more expensive than
any dynamic RAM.
If the choice is between a machine with very little, very
fast RAM and a machine with a lot of relatively fast
RAM, the second will always win given a working set
size which exceeds the small RAM size and the cost of
accessing secondary storage media such as hard drives.
The problem here is the speed of secondary storage, usu-
ally hard disks, which must be used to hold the swapped
out part of the working set. Accessing those disks is or-
ders of magnitude slower than even DRAM access.
Fortunately it does not have to be an all-or-nothing deci-
sion. A computer can have a small amount of high-speed
SRAM in addition to the large amount of DRAM. One
possible implementation would be to dedicate a certain
area of the address space of the processor as containing
the SRAM and the rest the DRAM. The task of the op-
erating system would then be to optimally distribute data
to make use of the SRAM. Basically, the SRAM serves
in this situation as an extension of the register set of the
processor.
While this is a possible implementation it is not viable.
Ignoring the problem of mapping the physical resources
of such SRAM-backed memory to the virtual address
spaces of the processes (which by itself is terribly hard)
this approach would require each process to administer
in software the allocation of this memory region. The
size of the memory region can vary from processor to
processor (i.e., processors have different amounts of the
expensive SRAM-backed memory). Each module which
makes up part of a program will claim its share of theWhat Every Programmer Should Know About Memory
Ulrich Drepper
Red Hat, Inc.
drepper@redhat.com
November 21, 2007
1
Abstract
As CPU cores become both faster and more numerous, the limiting factor for most programs is
now, and will be for some time, memory access. Hardware designers have come up with ever
more sophisticated memory handling and acceleration techniques–such as CPU caches–but
these cannot work optimally without some help from the programmer. Unfortunately, neither
the structure nor the cost of using the memory subsystem of a computer or the caches on CPUs
is well understood by most programmers. This paper explains the structure of memory subsys-
tems in use on modern commodity hardware, illustrating why CPU caches were developed, how
they work, and what programs should do to achieve optimal performance by utilizing them.
Introduction
 day these changes mainly come in the following forms:
In the early days computers were much simpler. The var-
ious components of a system, such as the CPU, memory,
mass storage, and network interfaces, were developed to-
gether and, as a result, were quite balanced in their per-
formance. For example, the memory and network inter-
faces were not (much) faster than the CPU at providing
data.
This situation changed once the basic structure of com-
puters stabilized and hardware developers concentrated
on optimizing individual subsystems. Suddenly the per-
formance of some components of the computer fell sig-
nificantly behind and bottlenecks developed. This was
especially true for mass storage and memory subsystems
which, for cost reasons, improved more slowly relative
to other components.
The slowness of mass storage has mostly been dealt with
using software techniques: operating systems keep most
often used (and most likely to be used) data in main mem-
ory, which can be accessed at a rate orders of magnitude
faster than the hard disk. Cache storage was added to the
storage devices themselves, which requires no changes in
the operating system to increase performance.1 For the
purposes of this paper, we will not go into more details
of software optimizations for the mass storage access.
Unlike storage subsystems, removing the main memory
as a bottleneck has proven much more difficult and al-
most all solutions require changes to the hardware. To-
1 Changes are needed, however, to guarantee data integrity when
using storage device caches.
Copyright © 2007 Ulrich Drepper
All rights reserved. No redistribution allowed.
• RAM hardware design (speed and parallelism).
• Memory controller designs.
• CPU caches.
• Direct memory access (DMA) for devices.
For the most part, this document will deal with CPU
caches and some effects of memory controller design.
In the process of exploring these topics, we will explore
DMA and bring it into the larger picture. However, we
will start with an overview of the design for today’s com-
modity hardware. This is a prerequisite to understand-
ing the problems and the limitations of efficiently us-
ing memory subsystems. We will also learn about, in
some detail, the different types of RAM and illustrate
why these differences still exist.
This document is in no way all inclusive and final. It is
limited to commodity hardware and further limited to a
subset of that hardware. Also, many topics will be dis-
cussed in just enough detail for the goals of this paper.
For such topics, readers are recommended to find more
detailed documentation.
When it comes to operating-system-specific details and
solutions, the text exclusively describes Linux. At no
time will it contain any information about other OSes.
The author has no interest in discussing the implications
for other OSes. If the reader thinks s/he has to use a
different OS they have to go to their vendors and demand
they write documents similar to this one.
One last comment before the start. The text contains a
number of occurrences of the term “usually” and other,
similar qualifiers. The technology discussed here exists
in many, many variations in the real world and this paper
only addresses the most common, mainstream versions.
It is rare that absolute statements can be made about this
technology, thus the qualifiers.
Document Structure
This document is mostly for software developers. It does
not go into enough technical details of the hardware to be
useful for hardware-oriented readers. But before we can
go into the practical information for developers a lot of
groundwork must be laid.
To that end, the second section describes random-access
memory (RAM) in technical detail. This section’s con-
tent is nice to know but not absolutely critical to be able
to understand the later sections. Appropriate back refer-
ences to the section are added in places where the content
is required so that the anxious reader could skip most of
this section at first.
The third section goes into a lot of details of CPU cache
behavior. Graphs have been used to keep the text from
being as dry as it would otherwise be. This content is es-
sential for an understanding of the rest of the document.
Section 4 describes briefly how virtual memory is imple-
mented. This is also required groundwork for the rest.
Section 5 goes into a lot of detail about Non Uniform
Memory Access (NUMA) systems.
Section 6 is the central section of this paper. It brings to-
gether all the previous sections’ information and gives
programmers advice on how to write code which per-
forms well in the various situations. The very impatient
reader could start with this section and, if necessary, go
back to the earlier sections to freshen up the knowledge
of the underlying technology.
Section 7 introduces tools which can help the program-
mer do a better job. Even with a complete understanding
of the technology it is far from obvious where in a non-
trivial software project the problems are. Some tools are
necessary.
In section 8 we finally give an outlook of technology
which can be expected in the near future or which might
just simply be good to have.
Thanks
I would like to thank Johnray Fuller and the crew at LWN
(especially Jonathan Corbet for taking on the daunting
task of transforming the author’s form of English into
something more traditional. Markus Armbruster provided
a lot of valuable input on problems and omissions in the
text.
About this Document
The title of this paper is an homage to David Goldberg’s
classic paper “What Every Computer Scientist Should
Know About Floating-Point Arithmetic” [12]. This pa-
per is still not widely known, although it should be a
prerequisite for anybody daring to touch a keyboard for
serious programming.
One word on the PDF: xpdf draws some of the diagrams
rather poorly. It is recommended it be viewed with evince
or, if really necessary, Adobe’s programs. If you use
evince be advised that hyperlinks are used extensively
throughout the document even though the viewer does
not indicate them like others do.
Reporting Problems
The author intends to update this document for some
time. This includes updates made necessary by advances
in technology but also to correct mistakes. Readers will-
ing to report problems are encouraged to send email to
the author. They are asked to include exact version in-
formation in the report. The version information can be
found on the last page of the document.
2
 Version 1.0
 What Every Programmer Should Know About Memory
2
 Commodity Hardware Today
It is important to understand commodity hardware be-
cause specialized hardware is in retreat. Scaling these
days is most often achieved horizontally instead of verti-
cally, meaning today it is more cost-effective to use many
smaller, connected commodity computers instead of a
few really large and exceptionally fast (and expensive)
systems. This is the case because fast and inexpensive
network hardware is widely available. There are still sit-
uations where the large specialized systems have their
place and these systems still provide a business opportu-
nity, but the overall market is dwarfed by the commodity
hardware market. Red Hat, as of 2007, expects that for
future products, the “standard building blocks” for most
data centers will be a computer with up to four sockets,
each filled with a quad core CPU that, in the case of Intel
CPUs, will be hyper-threaded.2 This means the standard
system in the data center will have up to 64 virtual pro-
cessors. Bigger machines will be supported, but the quad
socket, quad CPU core case is currently thought to be the
sweet spot and most optimizations are targeted for such
machines.
Large differences exist in the structure of computers built
of commodity parts. That said, we will cover more than
90% of such hardware by concentrating on the most im-
portant differences. Note that these technical details tend
to change rapidly, so the reader is advised to take the date
of this writing into account.
Over the years personal computers and smaller servers
standardized on a chipset with two parts: the Northbridge
and Southbridge. Figure 2.1 shows this structure.
RAM
PCI-E
CPU1
 CPU2
FSB
Northbridge
Southbridge
SATA
USB
Figure 2.1: Structure with Northbridge and Southbridge
All CPUs (two in the previous example, but there can be
more) are connected via a common bus (the Front Side
Bus, FSB) to the Northbridge. The Northbridge contains,
among other things, the memory controller, and its im-
plementation determines the type of RAM chips used for
the computer. Different types of RAM, such as DRAM,
Rambus, and SDRAM, require different memory con-
trollers.
To reach all other system devices, the Northbridge must
communicate with the Southbridge. The Southbridge,
often referred to as the I/O bridge, handles communica-
2Hyper-threading enables a single processor core to be used for two
or more concurrent executions with just a little extra hardware.
tion with devices through a variety of different buses. To-
day the PCI, PCI Express, SATA, and USB buses are of
most importance, but PATA, IEEE 1394, serial, and par-
allel ports are also supported by the Southbridge. Older
systems had AGP slots which were attached to the North-
bridge. This was done for performance reasons related to
insufficiently fast connections between the Northbridge
and Southbridge. However, today the PCI-E slots are all
connected to the Southbridge.
Such a system structure has a number of noteworthy con-
sequences:
• All data communication from one CPU to another
must travel over the same bus used to communicate
with the Northbridge.
• All communication with RAM must pass through
the Northbridge.
3
• The RAM has only a single port.• Communication between a CPU and a device at-
tached to the Southbridge is routed through the
Northbridge.
A couple of bottlenecks are immediately apparent in this
design. One such bottleneck involves access to RAM for
devices. In the earliest days of the PC, all communica-
tion with devices on either bridge had to pass through the
CPU, negatively impacting overall system performance.
To work around this problem some devices became ca-
pable of direct memory access (DMA). DMA allows de-
vices, with the help of the Northbridge, to store and re-
ceive data in RAM directly without the intervention of
the CPU (and its inherent performance cost). Today all
high-performance devices attached to any of the buses
can utilize DMA. While this greatly reduces the work-
load on the CPU, it also creates contention for the band-
width of the Northbridge as DMA requests compete with
RAM access from the CPUs. This problem, therefore,
must be taken into account.
A second bottleneck involves the bus from the North-
bridge to the RAM. The exact details of the bus depend
on the memory types deployed. On older systems there
is only one bus to all the RAM chips, so parallel ac-
cess is not possible. Recent RAM types require two sep-
arate buses (or channels as they are called for DDR2,
see page 8) which doubles the available bandwidth. The
Northbridge interleaves memory access across the chan-
nels. More recent memory technologies (FB-DRAM, for
instance) add more channels.
With limited bandwidth available, it is important for per-
formance to schedule memory access in ways that mini-
mize delays. As we will see, processors are much faster
3 We will not discuss multi-port RAM in this document as this type
of RAM is not found in commodity hardware, at least not in places
where the programmer has access to it. It can be found in specialized
hardware such as network routers which depend on utmost speed.
Ulrich Drepper
 Version 1.0
 3
and must wait to access memory, despite the use of CPU
caches. If multiple hyper-threads, cores, or processors
access memory at the same time, the wait times for mem-
ory access are even longer. This is also true for DMA
operations.
There is more to accessing memory than concurrency,
however. Access patterns themselves also greatly influ-
ence the performance of the memory subsystem, espe-
cially with multiple memory channels. In section 2.2 we
wil cover more details of RAM access patterns.
On some more expensive systems, the Northbridge does
not actually contain the memory controller. Instead the
Northbridge can be connected to a number of external
memory controllers (in the following example, four of
them).
CPU1
 CPU2
RAM
 MC1
 MC3
 RAM
Northbridge
RAM
 MC2
 MC4
 RAM
SATA
PCI-E
 Southbridge
USB
Figure 2.2: Northbridge with External Controllers
The advantage of this architecture is that more than one
memory bus exists and therefore total available band-
width increases. This design also supports more memory.
Concurrent memory access patterns reduce delays by si-
multaneously accessing different memory banks. This
is especially true when multiple processors are directly
connected to the Northbridge, as in Figure 2.2. For such
a design, the primary limitation is the internal bandwidth
of the Northbridge, which is phenomenal for this archi-
tecture (from Intel).4
Using multiple external memory controllers is not the
only way to increase memory bandwidth. One other in-
creasingly popular way is to integrate memory controllers
into the CPUs and attach memory to each CPU. This
architecture is made popular by SMP systems based on
AMD’s Opteron processor. Figure 2.3 shows such a sys-
tem. Intel will have support for the Common System In-
terface (CSI) starting with the Nehalem processors; this
is basically the same approach: an integrated memory
controller with the possibility of local memory for each
processor.
With an architecture like this there are as many memory
banks available as there are processors. On a quad-CPU
machine the memory bandwidth is quadrupled without
the need for a complicated Northbridge with enormous
bandwidth. Having a memory controller integrated into
the CPU has some additional advantages; we will not dig
4 For completeness it should be mentioned that such a memory con-
troller arrangement can be used for other purposes such as “memory
RAID” which is useful in combination with hotplug memory.
RAM
 CPU1
 CPU2
 RAM
RAM
 CPU3
 CPU4
 RAM
SATA
PCI-E
 Southbridge
USB
Figure 2.3: Integrated Memory Controller
deeper into this technology here.
There are disadvantages to this architecture, too. First of
all, because the machine still has to make all the mem-
ory of the system accessible to all processors, the mem-
ory is not uniform anymore (hence the name NUMA -
Non-Uniform Memory Architecture - for such an archi-
tecture). Local memory (memory attached to a proces-
sor) can be accessed with the usual speed. The situation
is different when memory attached to another processor
is accessed. In this case the interconnects between the
processors have to be used. To access memory attached
to CPU2 from CPU1 requires communication across one
interconnect. When the same CPU accesses memory at-
tached to CPU4 two interconnects have to be crossed.
Each such communication has an associated cost. We
talk about “NUMA factors” when we describe the ex-
tra time needed to access remote memory. The example
architecture in Figure 2.3 has two levels for each CPU:
immediately adjacent CPUs and one CPU which is two
interconnects away. With more complicated machines
the number of levels can grow significantly. There are
also machine architectures (for instance IBM’s x445 and
SGI’s Altix series) where there is more than one type
of connection. CPUs are organized into nodes; within a
node the time to access the memory might be uniform or
have only small NUMA factors. The connection between
nodes can be very expensive, though, and the NUMA
factor can be quite high.
Commodity NUMA machines exist today and will likely
play an even greater role in the future. It is expected that,
from late 2008 on, every SMP machine will use NUMA.
The costs associated with NUMA make it important to
recognize when a program is running on a NUMA ma-
chine. In section 5 we will discuss more machine archi-
tectures and some technologies the Linux kernel provides
for these programs.
Beyond the technical details described in the remainder
of this section, there are several additional factors which
influence the performance of RAM. They are not con-
trollable by software, which is why they are not covered
in this section. The interested reader can learn about
some of these factors in section 2.1. They are really only
needed to get a more complete picture of RAM technol-
ogy and possibly to make better decisions when purchas-
ing computers.
4
 Version 1.0
 What Every Programmer Should Know About Memory
The following two sections discuss hardware details at
the gate level and the access protocol between the mem-
ory controller and the DRAM chips. Programmers will
likely find this information enlightening since these de-
tails explain why RAM access works the way it does. It
is optional knowledge, though, and the reader anxious to
get to topics with more immediate relevance for everyday
life can jump ahead to section 2.2.5.
2.1
 RAM Types
There have been many types of RAM over the years and
each type varies, sometimes significantly, from the other.
The older types are today really only interesting to the
historians. We will not explore the details of those. In-
stead we will concentrate on modern RAM types; we will
only scrape the surface, exploring some details which
are visible to the kernel or application developer through
their performance characteristics.
The first interesting details are centered around the ques-
tion why there are different types of RAM in the same
machine. More specifically, why are there both static
RAM (SRAM5) and dynamic RAM (DRAM). The for-
mer is much faster and provides the same functionality.
Why is not all RAM in a machine SRAM? The answer
is, as one might expect, cost. SRAM is much more ex-
pensive to produce and to use than DRAM. Both these
cost factors are important, the second one increasing in
importance more and more. To understand these differ-
ences we look at the implementation of a bit of storage
for both SRAM and DRAM.
In the remainder of this section we will discuss some
low-level details of the implementation of RAM. We will
keep the level of detail as low as possible. To that end,
we will discuss the signals at a “logic level” and not at a
level a hardware designer would have to use. That level
of detail is unnecessary for our purpose here.
2.1.1
 Static RAM
BL
M2
M5
M1
WL
Vdd
M4
M6
M3
BL
Figure 2.4: 6-T Static RAM
Figure 2.4 shows the structure of a 6 transistor SRAM
cell. The core of this cell is formed by the four transistors
M1 to M4 which form two cross-coupled inverters. They
have two stable states, representing 0 and 1 respectively.
The state is stable as long as power on Vdd is available.
5 In other contexts SRAM might mean “synchronous RAM”.
If access to the state of the cell is needed the word access
line WL is raised. This makes the state of the cell imme-
diately available for reading on BL and BL. If the cell
state must be overwritten the BL and BL lines are first
set to the desired values and then WL is raised. Since the
outside drivers are stronger than the four transistors (M1
through M4 ) this allows the old state to be overwritten.
See [20] for a more detailed description of the way the
cell works. For the following discussion it is important
to note that
• one cell requires six transistors. There are variants
with four transistors but they have disadvantages.
• maintaining the state of the cell requires constant
power.
• the cell state is available for reading almost im-
mediately once the word access line WL is raised.
The signal is as rectangular (changing quickly be-
tween the two binary states) as other transistor-
controlled signals.
• the cell state is stable, no refresh cycles are needed.
There are other, slower and less power-hungry, SRAM
forms available, but those are not of interest here since
we are looking at fast RAM. These slow variants are
mainly interesting because they can be more easily used
in a system than dynamic RAM because of their simpler
interface.
2.1.2
 Dynamic RAM
Dynamic RAM is, in its structure, much simpler than
static RAM. Figure 2.5 shows the structure of a usual
DRAM cell design. All it consists of is one transistor
and one capacitor. This huge difference in complexity of
course means that it functions very differently than static
RAM.
DL
AL
M
C
Figure 2.5: 1-T Dynamic RAM
A dynamic RAM cell keeps its state in the capacitor C.
The transistor M is used to guard the access to the state.
To read the state of the cell the access line AL is raised;
this either causes a current to flow on the data line DL or
not, depending on the charge in the capacitor. To write
to the cell the data line DL is appropriately set and then
AL is raised for a time long enough to charge or drain
the capacitor.
There are a number of complications with the design of
dynamic RAM. The use of a capacitor means that reading
Ulrich Drepper
 Version 1.0
 5
the cell discharges the capacitor. The procedure cannot
be repeated indefinitely, the capacitor must be recharged
at some point. Even worse, to accommodate the huge
number of cells (chips with 109 or more cells are now
common) the capacity to the capacitor must be low (in
the femto-farad range or lower). A fully charged capac-
itor holds a few 10’s of thousands of electrons. Even
though the resistance of the capacitor is high (a couple of
tera-ohms) it only takes a short time for the capacity to
dissipate. This problem is called “leakage”.
This leakage is why a DRAM cell must be constantly
refreshed. For most DRAM chips these days this refresh
must happen every 64ms. During the refresh cycle no
access to the memory is possible since a refresh is simply
a memory read operation where the result is discarded.
For some workloads this overhead might stall up to 50%
of the memory accesses (see [3]).
A second problem resulting from the tiny charge is that
the information read from the cell is not directly usable.
The data line must be connected to a sense amplifier
which can distinguish between a stored 0 or 1 over the
whole range of charges which still have to count as 1.
A third problem is that reading a cell causes the charge
of the capacitor to be depleted. This means every read
operation must be followed by an operation to recharge
the capacitor. This is done automatically by feeding the
output of the sense amplifier back into the capacitor. It
does mean, though, the reading memory content requires
additional energy and, more importantly, time.
A fourth problem is that charging and draining a capac-
itor is not instantaneous. The signals received by the
sense amplifier are not rectangular, so a conservative es-
timate as to when the output of the cell is usable has to
be used. The formulas for charging and discharging a
capacitor are
QCharge (t)
QDischarge (t)
==Q0 (1 − e− RC t
 )
Q0 e− RC
 t
This means it takes some time (determined by the capac-
ity C and resistance R) for the capacitor to be charged and
discharged. It also means that the current which can be
detected by the sense amplifiers is not immediately avail-
able. Figure 2.6 shows the charge and discharge curves.
The X–axis is measured in units of RC (resistance multi-
plied by capacitance) which is a unit of time.
Unlike the static RAM case where the output is immedi-
ately available when the word access line is raised, it will
always take a bit of time until the capacitor discharges
sufficiently. This delay severely limits how fast DRAM
can be.
The simple approach has its advantages, too. The main
advantage is size. The chip real estate needed for one
DRAM cell is many times smaller than that of an SRAM
100
e
grahCegatnecreP90
80
70
60
50
40
30
20
10
0
Charge
 Discharge
1RC 2RC 3RC 4RC 5RC 6RC 7RC 8RC 9RC
Figure 2.6: Capacitor Charge and Discharge Timing
cell. The SRAM cells also need individual power for
the transistors maintaining the state. The structure of
the DRAM cell is also simpler and more regular which
means packing many of them close together on a die is
simpler.
Overall, the (quite dramatic) difference in cost wins. Ex-
cept in specialized hardware – network routers, for exam-
ple – we have to live with main memory which is based
on DRAM. This has huge implications on the program-
mer which we will discuss in the remainder of this paper.
But first we need to look into a few more details of the
actual use of DRAM cells.
2.1.3
 DRAM Access
A program selects a memory location using a virtual ad-
dress. The processor translates this into a physical ad-
dress and finally the memory controller selects the RAM
chip corresponding to that address. To select the individ-
ual memory cell on the RAM chip, parts of the physical
address are passed on in the form of a number of address
lines.
It would be completely impractical to address memory
locations individually from the memory controller: 4GB
of RAM would require 232 address lines. Instead the
address is passed encoded as a binary number using a
smaller set of address lines. The address passed to the
DRAM chip this way must be demultiplexed first. A
demultiplexer with N address lines will have 2N output
lines. These output lines can be used to select the mem-
ory cell. Using this direct approach is no big problem for
chips with small capacities.
But if the number of cells grows this approach is not suit-
able anymore. A chip with 1Gbit6 capacity would need
30 address lines and 230 select lines. The size of a de-
multiplexer increases exponentially with the number of
input lines when speed is not to be sacrificed. A demulti-
plexer for 30 address lines needs a whole lot of chip real
estate in addition to the complexity (size and time) of
the demultiplexer. Even more importantly, transmitting
6 I hate those SI prefixes. For me a giga-bit will always be 230 and
not 109 bits.
6
 Version 1.0
 What Every Programmer Should Know About Memory
30 impulses on the address lines synchronously is much
harder than transmitting “only” 15 impulses. Fewer lines
have to be laid out at exactly the same length or timed
appropriately.7
a0
a1
n
oitceleSsserddAwoRa2
a3
Column Address Selection
Data
Figure 2.7: Dynamic RAM Schematic
Figure 2.7 shows a DRAM chip at a very high level. The
DRAM cells are organized in rows and columns. They
could all be aligned in one row but then the DRAM chip
would need a huge demultiplexer. With the array ap-
proach the design can get by with one demultiplexer and
one multiplexer of half the size.8 This is a huge saving
on all fronts. In the example the address lines a0 and a1
through the row address selection (RAS)9 demultiplexer
select the address lines of a whole row of cells. When
reading, the content of all cells is thusly made available to
the column address selection (CAS)9 multiplexer. Based
on the address lines a2 and a3 the content of one col-
umn is then made available to the data pin of the DRAM
chip. This happens many times in parallel on a number
of DRAM chips to produce a total number of bits corre-
sponding to the width of the data bus.
For writing, the new cell value is put on the data bus and,
when the cell is selected using the RAS and CAS, it is
stored in the cell. A pretty straightforward design. There
are in reality – obviously – many more complications.
There need to be specifications for how much delay there
is after the signal before the data will be available on the
data bus for reading. The capacitors do not unload instan-
taneously, as described in the previous section. The sig-
nal from the cells is so weak that it needs to be amplified.
For writing it must be specified how long the data must
be available on the bus after the RAS and CAS is done to
successfully store the new value in the cell (again, capac-
7 Modern DRAM types like DDR3 can automatically adjust the tim-
ing but there is a limit as to what can be tolerated.
8 Multiplexers and demultiplexers are equivalent and the multiplexer
here needs to work as a demultiplexer when writing. So we will drop
the differentiation from now on.
9 The line over the name indicates that the signal is negated.
itors do not fill or drain instantaneously). These timing
constants are crucial for the performance of the DRAM
chip. We will talk about this in the next section.
A secondary scalability problem is that having 30 address
lines connected to every RAM chip is not feasible either.
Pins of a chip are precious resources. It is “bad” enough
that the data must be transferred as much as possible in
parallel (e.g., in 64 bit batches). The memory controller
must be able to address each RAM module (collection of
RAM chips). If parallel access to multiple RAM mod-
ules is required for performance reasons and each RAM
module requires its own set of 30 or more address lines,
then the memory controller needs to have, for 8 RAM
modules, a whopping 240+ pins only for the address han-
dling.
To counter these secondary scalability problems DRAM
chips have, for a long time, multiplexed the address it-
self. That means the address is transferred in two parts.
The first part consisting of address bits (a0 and a1 in the
example in Figure 2.7) select the row. This selection re-
mains active until revoked. Then the second part, address
bits a2 and a3 , select the column. The crucial difference
is that only two external address lines are needed. A few
more lines are needed to indicate when the RAS and CAS
signals are available but this is a small price to pay for
cutting the number of address lines in half. This address
multiplexing brings its own set of problems, though. We
will discuss them in section 2.2.
2.1.4
 Conclusions
Do not worry if the details in this section are a bit over-
whelming. The important things to take away from this
section are:
• there are reasons why not all memory is SRAM
• memory cells need to be individually selected to
be used
• the number of address lines is directly responsi-
ble for the cost of the memory controller, mother-
boards, DRAM module, and DRAM chip
• it takes a while before the results of the read or
write operation are available
The following section will go into more details about the
actual process of accessing DRAM memory. We are not
going into more details of accessing SRAM, which is
usually directly addressed. This happens for speed and
because the SRAM memory is limited in size. SRAM
is currently used in CPU caches and on-die where the
connections are small and fully under control of the CPU
designer. CPU caches are a topic which we discuss later
but all we need to know is that SRAM cells have a certain
maximum speed which depends on the effort spent on the
SRAM. The speed can vary from only slightly slower
Ulrich Drepper
 Version 1.0
 7
than the CPU core to one or two orders of magnitude
slower.
2.2
 DRAM Access Technical Details
In the section introducing DRAM we saw that DRAM
chips multiplex the addresses in order to save resources
int the form of address pins. We also saw that access-
ing DRAM cells takes time since the capacitors in those
cells do not discharge instantaneously to produce a stable
signal; we also saw that DRAM cells must be refreshed.
Now it is time to put this all together and see how all
these factors determine how the DRAM access has to
happen.
We will concentrate on current technology; we will not
discuss asynchronous DRAM and its variants as they are
simply not relevant anymore. Readers interested in this
topic are referred to [3] and [19]. We will also not talk
about Rambus DRAM (RDRAM) even though the tech-
nology is not obsolete. It is just not widely used for sys-
tem memory. We will concentrate exclusively on Syn-
chronous DRAM (SDRAM) and its successors Double
Data Rate DRAM (DDR).
Synchronous DRAM, as the name suggests, works rel-
ative to a time source. The memory controller provides
a clock, the frequency of which determines the speed of
the Front Side Bus (FSB) – the memory controller in-
terface used by the DRAM chips. As of this writing,
frequencies of 800MHz, 1,066MHz, or even 1,333MHz
are available with higher frequencies (1,600MHz) being
announced for the next generation. This does not mean
the frequency used on the bus is actually this high. In-
stead, today’s buses are double- or quad-pumped, mean-
ing that data is transported two or four times per cy-
cle. Higher numbers sell so the manufacturers like to
advertise a quad-pumped 200MHz bus as an “effective”
800MHz bus.
For SDRAM today each data transfer consists of 64 bits
– 8 bytes. The transfer rate of the FSB is therefore 8
bytes multiplied by the effective bus frequency (6.4GB/s
for the quad-pumped 200MHz bus). That sounds a lot
but it is the burst speed, the maximum speed which will
never be surpassed. As we will see now the protocol for
talking to the RAM modules has a lot of downtime when
no data can be transmitted. It is exactly this downtime
which we must understand and minimize to achieve the
best performance.
2.2.1
 Read Access Protocol
Figure 2.8 shows the activity on some of the connectors
of a DRAM module which happens in three differently
colored phases. As usual, time flows from left to right.
A lot of details are left out. Here we only talk about the
bus clock, RAS and CAS signals, and the address and
data buses. A read cycle begins with the memory con-
troller making the row address available on the address
CLK
RAS
CAS
Row
 Col
Address
Addr
 Addr
t CL
RCD
Data
 Data
 Data
 Data
DQ
Out
 Out
 Out
 Out
Figure 2.8: SDRAM Read Access Timing
bus and lowering the RAS signal. All signals are read on
the rising edge of the clock (CLK) so it does not matter if
the signal is not completely square as long as it is stable
at the time it is read. Setting the row address causes the
RAM chip to start latching the addressed row.
The CAS signal can be sent after tRCD (RAS-to-CAS
Delay) clock cycles. The column address is then trans-
mitted by making it available on the address bus and low-
ering the CAS line. Here we can see how the two parts
of the address (more or less halves, nothing else makes
sense) can be transmitted over the same address bus.
Now the addressing is complete and the data can be trans-
mitted. The RAM chip needs some time to prepare for
this. The delay is usually called CAS Latency (CL). In
Figure 2.8 the CAS latency is 2. It can be higher or lower,
depending on the quality of the memory controller, moth-
erboard, and DRAM module. The latency can also have
half values. With CL=2.5 the first data would be avail-
able at the first falling flank in the blue area.
With all this preparation to get to the data it would be
wasteful to only transfer one data word. This is why
DRAM modules allow the memory controller to spec-
ify how much data is to be transmitted. Often the choice
is between 2, 4, or 8 words. This allows filling entire
lines in the caches without a new RAS/CAS sequence. It
is also possible for the memory controller to send a new
CAS signal without resetting the row selection. In this
way, consecutive memory addresses can be read from
or written to significantly faster because the RAS sig-
nal does not have to be sent and the row does not have
to be deactivated (see below). Keeping the row “open”
is something the memory controller has to decide. Spec-
ulatively leaving it open all the time has disadvantages
with real-world applications (see [3]). Sending new CAS
signals is only subject to the Command Rate of the RAM
module (usually specified as Tx, where x is a value like
1 or 2; it will be 1 for high-performance DRAM modules
which accept new commands every cycle).
In this example the SDRAM spits out one word per cy-
cle. This is what the first generation does. DDR is able
to transmit two words per cycle. This cuts down on the
transfer time but does not change the latency. In princi-
8
 Version 1.0
 What Every Programmer Should Know About Memory
ple, DDR2 works the same although in practice it looks
different. There is no need to go into the details here. It is
sufficient to note that DDR2 can be made faster, cheaper,
more reliable, and is more energy efficient (see [6] for
more information).
2.2.2
 Precharge and Activation
Figure 2.8 does not cover the whole cycle. It only shows
parts of the full cycle of accessing DRAM. Before a new
RAS signal can be sent the currently latched row must be
deactivated and the new row must be precharged. We can
concentrate here on the case where this is done with an
explicit command. There are improvements to the pro-
tocol which, in some situations, allows this extra step to
be avoided. The delays introduced by precharging still
affect the operation, though.
CLK
WE
RAS
CAS
tRP
Col
 Row
 Col
Address
Addr
 Addr
 Addr
CL
 tRCD
Data
 Data
DQ
Out
 Out
Figure 2.9: SDRAM Precharge and Activation
Figure 2.9 shows the activity starting from one CAS sig-
nal to the CAS signal for another row. The data requested
with the first CAS signal is available as before, after CL
cycles. In the example two words are requested which,
on a simple SDRAM, takes two cycles to transmit. Al-
ternatively, imagine four words on a DDR chip.
Even on DRAM modules with a command rate of one
the precharge command cannot be issued right away. It
is necessary to wait as long as it takes to transmit the
data. In this case it takes two cycles. This happens to be
the same as CL but that is just a coincidence. The pre-
charge signal has no dedicated line; instead, some imple-
mentations issue it by lowering the Write Enable (WE)
and RAS line simultaneously. This combination has no
useful meaning by itself (see [18] for encoding details).
Once the precharge command is issued it takes tRP (Row
Precharge time) cycles until the row can be selected. In
Figure 2.9 much of the time (indicated by the purplish
color) overlaps with the memory transfer (light blue).
This is good! But tRP is larger than the transfer time
and so the next RAS signal is stalled for one cycle.
If we were to continue the timeline in the diagram we
would find that the next data transfer happens 5 cycles
after the previous one stops. This means the data bus is
only in use two cycles out of seven. Multiply this with
the FSB speed and the theoretical 6.4GB/s for a 800MHz
bus become 1.8GB/s. That is bad and must be avoided.
The techniques described in section 6 help to raise this
number. But the programmer usually has to do her share.
There is one more timing value for a SDRAM module
which we have not discussed. In Figure 2.9 the precharge
command was only limited by the data transfer time. An-
other constraint is that an SDRAM module needs time
after a RAS signal before it can precharge another row
(denoted as tRAS ). This number is usually pretty high,
in the order of two or three times the tRP value. This is
a problem if, after a RAS signal, only one CAS signal
follows and the data transfer is finished in a few cycles.
Assume that in Figure 2.9 the initial CAS signal was pre-
ceded directly by a RAS signal and that tRAS is 8 cycles.
Then the precharge command would have to be delayed
by one additional cycle since the sum of tRCD, CL, and
tRP (since it is larger than the data transfer time) is only
7 cycles.
DDR modules are often described using a special nota-
tion: w-x-y-z-T. For instance: 2-3-2-8-T1. This means:
w
 2
 CAS Latency (CL)
x
y
z
T
3
2
8
T1
RAS-to-CAS delay (tRCD)
RAS Precharge (tRP )
Active to Precharge delay (tRAS)
Command Rate
There are numerous other timing constants which affect
the way commands can be issued and are handled. Those
five constants are in practice sufficient to determine the
performance of the module, though.
It is sometimes useful to know this information for the
computers in use to be able to interpret certain measure-
ments. It is definitely useful to know these details when
buying computers since they, along with the FSB and
SDRAM module speed, are among the most important
factors determining a computer’s speed.
The very adventurous reader could also try to tweak a
system. Sometimes the BIOS allows changing some or
all these values. SDRAM modules have programmable
registers where these values can be set. Usually the BIOS
picks the best default value. If the quality of the RAM
module is high it might be possible to reduce the one
or the other latency without affecting the stability of the
computer. Numerous overclocking websites all around
the Internet provide ample of documentation for doing
this. Do it at your own risk, though and do not say you
have not been warned.
2.2.3
 Recharging
A mostly-overlooked topic when it comes to DRAM ac-
cess is recharging. As explained in section 2.1.2, DRAM
cells must constantly be refreshed. This does not happen
Ulrich Drepper
 Version 1.0
 9
completely transparently for the rest of the system. At
times when a row10 is recharged no access is possible.
The study in [3] found that “[s]urprisingly, DRAM re-
fresh organization can affect performance dramatically”.
Each DRAM cell must be refreshed every 64ms accord-
ing to the JEDEC (Joint Electron Device Engineering
Council) specification. If a DRAM array has 8,192 rows
this means the memory controller has to issue a refresh
command on average every 7.8125μs (refresh commands
can be queued so in practice the maximum interval be-
tween two requests can be higher). It is the memory
controller’s responsibility to schedule the refresh com-
mands. The DRAM module keeps track of the address
of the last refreshed row and automatically increases the
address counter for each new request.
There is really not much the programmer can do about
the refresh and the points in time when the commands are
issued. But it is important to keep this part of the DRAM
life cycle in mind when interpreting measurements. If a
critical word has to be retrieved from a row which cur-
rently is being refreshed the processor could be stalled
for quite a long time. How long each refresh takes de-
pends on the DRAM module.
2.2.4
 Memory Types
It is worth spending some time on the current and soon-
to-be current memory types in use. We will start with
SDR (Single Data Rate) SDRAMs since they are the ba-
sis of the DDR (Double Data Rate) SDRAMs. SDRs
were pretty simple. The memory cells and the data trans-
fer rate were identical.
f
DRAM
Cell
Array
f
Figure 2.10: SDR SDRAM Operation
In Figure 2.10 the DRAM cell array can output the mem-
ory content at the same rate it can be transported over
the memory bus. If the DRAM cell array can operate at
100MHz, the data transfer rate of the bus of a single cell
is thus 100Mb/s. The frequency f for all components is
the same. Increasing the throughput of the DRAM chip
is expensive since the energy consumption rises with the
frequency. With a huge number of array cells this is
prohibitively expensive.11 In reality it is even more of
a problem since increasing the frequency usually also
requires increasing the voltage to maintain stability of
the system. DDR SDRAM (called DDR1 retroactively)
manages to improve the throughput without increasing
any of the involved frequencies.
10 Rows are the granularity this happens with despite what [3] and
other literature says (see [18]).
11 Power = Dynamic Capacity × Voltage2 × Frequency.
f
 f
 f
DRAM
I/O
Cell
Buffer
Array
Figure 2.11: DDR1 SDRAM Operation
The difference between SDR and DDR1 is, as can be
seen in Figure 2.11 and guessed from the name, that twice
the amount of data is transported per cycle. I.e., the
DDR1 chip transports data on the rising and falling edge.
This is sometimes called a “double-pumped” bus. To
make this possible without increasing the frequency of
the cell array a buffer has to be introduced. This buffer
holds two bits per data line. This in turn requires that,
in the cell array in Figure 2.7, the data bus consists of
two lines. Implementing this is trivial: one only has to
use the same column address for two DRAM cells and
access them in parallel. The changes to the cell array to
implement this are also minimal.
The SDR DRAMs were known simply by their frequency
(e.g., PC100 for 100MHz SDR). To make DDR1 DRAM
sound better the marketers had to come up with a new
scheme since the frequency did not change. They came
with a name which contains the transfer rate in bytes a
DDR module (they have 64-bit busses) can sustain:
100MHz × 64bit × 2 = 1, 600MB/s
Hence a DDR module with 100MHz frequency is called
PC1600. With 1600 > 100 all marketing requirements
are fulfilled; it sounds much better although the improve-
ment is really only a factor of two.12
f
 2f
 2f
DRAM
I/O
Cell
Buffer
Array
Figure 2.12: DDR2 SDRAM Operation
To get even more out of the memory technology DDR2
includes a bit more innovation. The most obvious change
that can be seen in Figure 2.12 is the doubling of the
frequency of the bus. Doubling the frequency means
doubling the bandwidth. Since this doubling of the fre-
quency is not economical for the cell array it is now re-
quired that the I/O buffer gets four bits in each clock cy-
cle which it then can send on the bus. This means the
changes to the DDR2 modules consist of making only the
I/O buffer component of the DIMM capable of running
at higher speeds. This is certainly possible and will not
require measurably more energy, it is just one tiny com-
ponent and not the whole module. The names the mar-
12I will take the factor of two but I do not have to like the inflated
numbers.
10
 Version 1.0
 What Every Programmer Should Know About Memory
keters came up with for DDR2 are similar to the DDR1
names only in the computation of the value the factor of
two is replaced by four (we now have a quad-pumped
bus). Table 2.1 shows the names of the modules in use
today.
Array
 Bus
 Data
 Name
 Name
Freq.
 Freq.
 Rate
 (Rate)
 (FSB)
133MHz
 266MHz
 4,256MB/s
 PC2-4200
 DDR2-533
166MHz
 333MHz
 5,312MB/s
 PC2-5300
 DDR2-667
200MHz
 400MHz
 6,400MB/s
 PC2-6400
 DDR2-800
250MHz
 500MHz
 8,000MB/s
 PC2-8000
 DDR2-1000
266MHz
 533MHz
 8,512MB/s
 PC2-8500
 DDR2-1066
Table 2.1: DDR2 Module Names
There is one more twist to the naming. The FSB speed
used by CPU, motherboard, and DRAM module is spec-
ified by using the effective frequency. I.e., it factors in
the transmission on both flanks of the clock cycle and
thereby inflates the number. So, a 133MHz module with
a 266MHz bus has an FSB “frequency” of 533MHz.
The specification for DDR3 (the real one, not the fake
GDDR3 used in graphics cards) calls for more changes
along the lines of the transition to DDR2. The voltage
will be reduced from 1.8V for DDR2 to 1.5V for DDR3.
Since the power consumption equation is calculated us-
ing the square of the voltage this alone brings a 30% im-
provement. Add to this a reduction in die size plus other
electrical advances and DDR3 can manage, at the same
frequency, to get by with half the power consumption.
Alternatively, with higher frequencies, the same power
envelope can be hit. Or with double the capacity the same
heat emission can be achieved.
The cell array of DDR3 modules will run at a quarter of
the speed of the external bus which requires an 8 bit I/O
buffer, up from 4 bits for DDR2. See Figure 2.13 for the
schematics.
f
 4f
 4f
DRAM
I/O
Cell
Buffer
Array
Figure 2.13: DDR3 SDRAM Operation
Initially DDR3 modules will likely have slightly higher
CAS latencies just because the DDR2 technology is more
mature. This would cause DDR3 to be useful only at
frequencies which are higher than those which can be
achieved with DDR2, and, even then, mostly when band-
width is more important than latency. There is already
talk about 1.3V modules which can achieve the same
CAS latency as DDR2. In any case, the possibility of
achieving higher speeds because of faster buses will out-
weigh the increased latency.
One possible problem with DDR3 is that, for 1,600Mb/s
transfer rate or higher, the number of modules per chan-
nel may be reduced to just one. In earlier versions this
requirement held for all frequencies, so one can hope
that the requirement will at some point be lifted for all
frequencies. Otherwise the capacity of systems will be
severely limited.
Table 2.2 shows the names of the DDR3 modules we are
likely to see. JEDEC agreed so far on the first four types.
Given that Intel’s 45nm processors have an FSB speed of
1,600Mb/s, the 1,866Mb/s is needed for the overclocking
market. We will likely see more of this towards the end
of the DDR3 lifecycle.
Array
 Bus
 Data
 Name
 Name
Freq.
 Freq.
 Rate
 (Rate)
 (FSB)
100MHz
 400MHz 6,400MB/s PC3-6400 DDR3-800
133MHz
 533MHz 8,512MB/s PC3-8500 DDR3-1066
166MHz
 667MHz 10,667MB/s PC3-10667 DDR3-1333
200MHz
 800MHz 12,800MB/s PC3-12800 DDR3-1600
233MHz
 933MHz 14,933MB/s PC3-14900 DDR3-1866
Table 2.2: DDR3 Module Names
All DDR memory has one problem: the increased bus
frequency makes it hard to create parallel data busses. A
DDR2 module has 240 pins. All connections to data and
address pins must be routed so that they have approxi-
mately the same length. Even more of a problem is that,
if more than one DDR module is to be daisy-chained on
the same bus, the signals get more and more distorted for
each additional module. The DDR2 specification allow
only two modules per bus (aka channel), the DDR3 spec-
ification only one module for high frequencies. With 240
pins per channel a single Northbridge cannot reasonably
drive more than two channels. The alternative is to have
external memory controllers (as in Figure 2.2) but this is
expensive.
What this means is that commodity motherboards are re-
stricted to hold at most four DDR2 or DDR3 modules.
This restriction severely limits the amount of memory
a system can have. Even old 32-bit IA-32 processors
can handle 64GB of RAM and memory demand even for
home use is growing, so something has to be done.
One answer is to add memory controllers into each pro-
cessor as explained in section 2. AMD does it with the
Opteron line and Intel will do it with their CSI technol-
ogy. This will help as long as the reasonable amount of
memory a processor is able to use can be connected to a
single processor. In some situations this is not the case
and this setup will introduce a NUMA architecture and
its negative effects. For some situations another solution
is needed.
Intel’s answer to this problem for big server machines, at
least at the moment, is called Fully Buffered DRAM (FB-
DRAM). The FB-DRAM modules use the same memory
chips as today’s DDR2 modules which makes them rela-
tively cheap to produce. The difference is in the connec-
tion with the memory controller. Instead of a parallel data
bus FB-DRAM utilizes a serial bus (Rambus DRAM had
Ulrich Drepper
 Version 1.0
 11
this back when, too, and SATA is the successor of PATA,
as is PCI Express for PCI/AGP). The serial bus can be
driven at a much higher frequency, reverting the negative
impact of the serialization and even increasing the band-
width. The main effects of using a serial bus are
1. more modules per channel can be used.
2. more channels per Northbridge/memory controller
can be used.
3. the serial bus is designed to be fully-duplex (two
lines).
4. it is cheap enough to implement a differential bus
(two lines in each direction) and so increase the
speed.
An FB-DRAM module has only 69 pins, compared with
the 240 for DDR2. Daisy chaining FB-DRAM modules
is much easier since the electrical effects of the bus can
be handled much better. The FB-DRAM specification
allows up to 8 DRAM modules per channel.
Compared with the connectivity requirements of a dual-
channel Northbridge it is now possible to drive 6 chan-
nels of FB-DRAM with fewer pins: 2 × 240 pins ver-
sus 6 × 69 pins. The routing for each channel is much
simpler which could also help reducing the cost of the
motherboards.
Fully duplex parallel busses are prohibitively expensive
for the traditional DRAM modules, duplicating all those
lines is too costly. With serial lines (even if they are dif-
ferential, as FB-DRAM requires) this is not the case and
so the serial bus is designed to be fully duplexed, which
means, in some situations, that the bandwidth is theoret-
ically doubled alone by this. But it is not the only place
where parallelism is used for bandwidth increase. Since
an FB-DRAM controller can run up to six channels at the
same time the bandwidth can be increased even for sys-
tems with smaller amounts of RAM by using FB-DRAM.
Where a DDR2 system with four modules has two chan-
nels, the same capacity can be handled via four chan-
nels using an ordinary FB-DRAM controller. The actual
bandwidth of the serial bus depends on the type of DDR2
(or DDR3) chips used on the FB-DRAM module.
We can summarize the advantages like this:
DDR2
 FB-DRAM
Pins
 240
 69
Channels
 2
 6
DIMMs/Channel
 2
 8
Max Memory13
 16GB14
 192GB
Throughput15
 ∼10GB/s
 ∼40GB/s
13 Assuming 4GB modules.
14 An Intel presentation, for some reason I do not see, says 8GB. . .
15 Assuming DDR2-800 modules.
There are a few drawbacks to FB-DRAMs if multiple
DIMMs on one channel are used. The signal is delayed–
albeit minimally–at each DIMM in the chain, thereby in-
creasing the latency. A second problem is that the chip
driving the serial bus requires significant amounts of en-
ergy because of the very high frequency and the need to
drive a bus. But for the same amount of memory with
the same frequency FB-DRAM can always be faster than
DDR2 and DDR3 since the up-to four DIMMS can each
get their own channel; for large memory systems DDR
simply has no answer using commodity components.
2.2.5
 Conclusions
This section should have shown that accessing DRAM is
not an arbitrarily fast process. At least not fast compared
with the speed the processor is running and with which it
can access registers and cache. It is important to keep in
mind the differences between CPU and memory frequen-
cies. An Intel Core 2 processor running at 2.933GHz and
a 1.066GHz FSB have a clock ratio of 11:1 (note: the
1.066GHz bus is quad-pumped). Each stall of one cycle
on the memory bus means a stall of 11 cycles for the pro-
cessor. For most machines the actual DRAMs used are
slower, thusly increasing the delay. Keep these numbers
in mind when we are talking about stalls in the upcoming
sections.
The timing charts for the read command have shown that
DRAM modules are capable of high sustained data rates.
Entire DRAM rows could be transported without a single
stall. The data bus could be kept occupied 100%. For
DDR modules this means two 64-bit words transferred
each cycle. With DDR2-800 modules and two channels
this means a rate of 12.8GB/s.
But, unless designed this way, DRAM access is not al-
ways sequential. Non-continuous memory regions are
used which means precharging and new RAS signals are
needed. This is when things slow down and when the
DRAM modules need help. The sooner the precharg-
ing can happen and the RAS signal sent the smaller the
penalty when the row is actually used.
Hardware and software prefetching (see section 6.3) can
be used to create more overlap in the timing and reduce
the stall. Prefetching also helps shift memory operations
in time so that there is less contention at later times, right
before the data is actually needed. This is a frequent
problem when the data produced in one round has to be
stored and the data required for the next round has to be
read. By shifting the read in time, the write and read op-
erations do not have to be issued at basically the same
time.
2.3
 Other Main Memory Users
Beside CPUs there are other system components which
can access the main memory. High-performance cards
such as network and mass-storage controllers cannot af-
12
 Version 1.0
 What Every Programmer Should Know About Memory
ford to pipe all the data they need or provide through the
CPU. Instead, they read or write the data directly from/to
the main memory (Direct Memory Access, DMA). In
Figure 2.1 we can see that the cards can talk through
the South- and Northbridge directly with the memory.
Other buses, like USB, also require FSB bandwidth–even
if they do not use DMA–since the Southbridge is con-
nected via the Northbridge to the processor through the
FSB, too.
While DMA is certainly beneficial, it means that there is
more competition for the FSB bandwidth. In times with
high DMA traffic the CPU might stall more than usual
while waiting for data from the main memory. There
are ways around this given the right hardware. With an
architecture as in Figure 2.3 one can make sure the com-
putation uses memory on nodes which are not affected
by DMA. It is also possible to attach a Southbridge to
each node, equally distributing the load on the FSB of
all the nodes. There are a myriad of possibilities. In
section 6 we will introduce techniques and programming
interfaces which help achieving the improvements which
are possible in software.
Finally it should be mentioned that some cheap systems
have graphics systems without separate, dedicated video
RAM. Those systems use parts of the main memory as
video RAM. Since access to the video RAM is frequent
(for a 1024x768 display with 16 bpp at 60Hz we are talk-
ing 94MB/s) and system memory, unlike RAM on graph-
ics cards, does not have two ports this can substantially
influence the systems performance and especially the la-
tency. It is best to ignore such systems when performance
is a priority. They are more trouble than they are worth.
People buying those machines know they will not get the
best performance.
3
 CPU Caches
CPUs are today much more sophisticated than they were
only 25 years ago. In those days, the frequency of the
CPU core was at a level equivalent to that of the mem-
ory bus. Memory access was only a bit slower than reg-
ister access. But this changed dramatically in the early
90s, when CPU designers increased the frequency of the
CPU core but the frequency of the memory bus and the
performance of RAM chips did not increase proportion-
ally. This is not due to the fact that faster RAM could
not be built, as explained in the previous section. It is
possible but it is not economical. RAM as fast as current
CPU cores is orders of magnitude more expensive than
any dynamic RAM.
If the choice is between a machine with very little, very
fast RAM and a machine with a lot of relatively fast
RAM, the second will always win given a working set
size which exceeds the small RAM size and the cost of
accessing secondary storage media such as hard drives.
The problem here is the speed of secondary storage, usu-
ally hard disks, which must be used to hold the swapped
out part of the working set. Accessing those disks is or-
ders of magnitude slower than even DRAM access.
Fortunately it does not have to be an all-or-nothing deci-
sion. A computer can have a small amount of high-speed
SRAM in addition to the large amount of DRAM. One
possible implementation would be to dedicate a certain
area of the address space of the processor as containing
the SRAM and the rest the DRAM. The task of the op-
erating system would then be to optimally distribute data
to make use of the SRAM. Basically, the SRAM serves
in this situation as an extension of the register set of the
processor.
While this is a possible implementation it is not viable.
Ignoring the problem of mapping the physical resources
of such SRAM-backed memory to the virtual address
spaces of the processes (which by itself is terribly hard)
this approach would require each process to administer
in software the allocation of this memory region. The
size of the memory region can vary from processor to
processor (i.e., processors have different amounts of the
expensive SRAM-backed memory). Each module which
makes up part of a program will claim its share of the
fast memory, which introduces additional costs through
synchronization requirements. In short, the gains of hav-
ing fast memory would be eaten up completely by the
overhead of administering the resources.
So, instead of putting the SRAM under the control of
the OS or user, it becomes a resource which is transpar-
ently used and administered by the processors. In this
mode, SRAM is used to make temporary copies of (to
cache, in other words) data in main memory which is
likely to be used soon by the processor. This is possible
because program code and data has temporal and spa-
tial locality. This means that, over short periods of time,
there is a good chance that the same code or data gets
Ulrich Drepper
 Version 1.0
 13
reused. For code this means that there are most likely
loops in the code so that the same code gets executed
over and over again (the perfect case for spatial locality).
Data accesses are also ideally limited to small regions.
Even if the memory used over short time periods is not
close together there is a high chance that the same data
will be reused before long (temporal locality). For code
this means, for instance, that in a loop a function call is
made and that function is located elsewhere in the ad-
dress space. The function may be distant in memory, but
calls to that function will be close in time. For data it
means that the total amount of memory used at one time
(the working set size) is ideally limited but the memory
used, as a result of the random access nature of RAM, is
not close together. Realizing that locality exists is key to
the concept of CPU caches as we use them today.
A simple computation can show how effective caches
can theoretically be. Assume access to main memory
takes 200 cycles and access to the cache memory take
15 cycles. Then code using 100 data elements 100 times
each will spend 2,000,000 cycles on memory operations
if there is no cache and only 168,500 cycles if all data
can be cached. That is an improvement of 91.5%.
The size of the SRAM used for caches is many times
smaller than the main memory. In the author’s experi-
ence with workstations with CPU caches the cache size
has always been around 1/1000th of the size of the main
memory (today: 4MB cache and 4GB main memory).
This alone does not constitute a problem. If the size of
the working set (the set of data currently worked on) is
smaller than the cache size it does not matter. But com-
puters do not have large main memories for no reason.
The working set is bound to be larger than the cache.
This is especially true for systems running multiple pro-
cesses where the size of the working set is the sum of the
sizes of all the individual processes and the kernel.
What is needed to deal with the limited size of the cache
is a set of good strategies to determine what should be
cached at any given time. Since not all data of the work-
ing set is used at exactly the same time we can use tech-
niques to temporarily replace some data in the cache with
others. And maybe this can be done before the data is
actually needed. This prefetching would remove some
of the costs of accessing main memory since it happens
asynchronously with respect to the execution of the pro-
gram. All these techniques and more can be used to make
the cache appear bigger than it actually is. We will dis-
cuss them in section 3.3. Once all these techniques are
exploited it is up to the programmer to help the processor.
How this can be done will be discussed in section 6.
3.1
 CPU Caches in the Big Picture
Before diving into technical details of the implementa-
tion of CPU caches some readers might find it useful to
first see in some more details how caches fit into the “big
picture” of a modern computer system.
Main Memory
Bus
Cache
 CPU Core
Figure 3.1: Minimum Cache Configuration
Figure 3.1 shows the minimum cache configuration. It
corresponds to the architecture which could be found in
early systems which deployed CPU caches. The CPU
core is no longer directly connected to the main mem-
ory.16 All loads and stores have to go through the cache.
The connection between the CPU core and the cache is
a special, fast connection. In a simplified representation,
the main memory and the cache are connected to the sys-
tem bus which can also be used for communication with
other components of the system. We introduced the sys-
tem bus as “FSB” which is the name in use today; see
section 2.2. In this section we ignore the Northbridge; it
is assumed to be present to facilitate the communication
of the CPU(s) with the main memory.
Even though most computers for the last several decades
have used the von Neumann architecture, experience has
shown that it is of advantage to separate the caches used
for code and for data. Intel has used separate code and
data caches since 1993 and never looked back. The mem-
ory regions needed for code and data are pretty much
independent of each other, which is why independent
caches work better. In recent years another advantage
emerged: the instruction decoding step for the most com-
mon processors is slow; caching decoded instructions
can speed up the execution, especially when the pipeline
is empty due to incorrectly predicted or impossible-to-
predict branches.
Soon after the introduction of the cache the system got
more complicated. The speed difference between the
cache and the main memory increased again, to a point
that another level of cache was added, bigger and slower
than the first-level cache. Only increasing the size of the
first-level cache was not an option for economical rea-
sons. Today, there are even machines with three levels
of cache in regular use. A system with such a processor
looks like Figure 3.2. With the increase on the number of
cores in a single CPU the number of cache levels might
increase in the future even more.
Figure 3.2 shows three levels of cache and introduces the
nomenclature we will use in the remainder of the docu-
ment. L1d is the level 1 data cache, L1i the level 1 in-
struction cache, etc. Note that this is a schematic; the
data flow in reality need not pass through any of the
higher-level caches on the way from the core to the main
16 In even earlier systems the cache was attached to the system bus
just like the CPU and the main memory. This was more a hack than a
real solution.
14
 Version 1.0
 What Every Programmer Should Know About Memory
Main Memory
Bus
L3 Cache
L2 Cache
 L1i Cache
L1d Cache
 CPU Core
Figure 3.2: Processor with Level 3 Cache
memory. CPU designers have a lot of freedom design-
ing the interfaces of the caches. For programmers these
design choices are invisible.
In addition we have processors which have multiple cores
and each core can have multiple “threads”. The differ-
ence between a core and a thread is that separate cores
have separate copies of (almost17 ) all the hardware re-
sources. The cores can run completely independently
unless they are using the same resources–e.g., the con-
nections to the outside–at the same time. Threads, on the
other hand, share almost all of the processor’s resources.
Intel’s implementation of threads has only separate reg-
isters for the threads and even that is limited, some regis-
ters are shared. The complete picture for a modern CPU
therefore looks like Figure 3.3.
Main Memory
Bus
Figure 3.3: Multi processor, multi-core, multi-thread
In this figure we have two processors, each with two
cores, each of which has two threads. The threads share
the Level 1 caches. The cores (shaded in the darker gray)
have individual Level 1 caches. All cores of the CPU
share the higher-level caches. The two processors (the
two big boxes shaded in the lighter gray) of course do
not share any caches. All this will be important, espe-
cially when we are discussing the cache effects on multi-
process and multi-thread applications.
17 Early multi-core processors even had separate 2nd level caches and
no 3rd level cache.
3.2
 Cache Operation at High Level
To understand the costs and savings of using a cache we
have to combine the knowledge about the machine ar-
chitecture and RAM technology from section 2 with the
structure of caches described in the previous section.
By default all data read or written by the CPU cores is
stored in the cache. There are memory regions which
cannot be cached but this is something only the OS im-
plementers have to be concerned about; it is not visible
to the application programmer. There are also instruc-
tions which allow the programmer to deliberately bypass
certain caches. This will be discussed in section 6.
If the CPU needs a data word the caches are searched
first. Obviously, the cache cannot contain the content
of the entire main memory (otherwise we would need
no cache), but since all memory addresses are cacheable,
each cache entry is tagged using the address of the data
word in the main memory. This way a request to read or
write to an address can search the caches for a matching
tag. The address in this context can be either the virtual
or physical address, varying based on the cache imple-
mentation.
Since for the tag, in addition to the actual memory, addi-
tional space is required, it is inefficient to chose a word as
the granularity of the cache. For a 32-bit word on an x86
machine the tag itself might need 32 bits or more. Fur-
thermore, since spatial locality is one of the principles on
which caches are based, it would be bad to not take this
into account. Since neighboring memory is likely to be
used together it should also be loaded into the cache to-
gether. Remember also what we learned in section 2.2.1:
RAM modules are much more effective if they can trans-
port many data words in a row without a new CAS or
even RAS signal. So the entries stored in the caches are
not single words but, instead, “lines” of several contigu-
ous words. In early caches these lines were 32 bytes
long; nowadays the norm is 64 bytes. If the memory
bus is 64 bits wide this means 8 transfers per cache line.
DDR supports this transport mode efficiently.
When memory content is needed by the processor the
entire cache line is loaded into the L1d. The memory
address for each cache line is computed by masking the
address value according to the cache line size. For a 64
byte cache line this means the low 6 bits are zeroed. The
discarded bits are used as the offset into the cache line.
The remaining bits are in some cases used to locate the
line in the cache and as the tag. In practice an address
value is split into three parts. For a 32-bit address it might
look as follows:
31
 0
Tag
 Cache Set
 Offset
T
 S
 O
With a cache line size of 2O the low O bits are used
Ulrich Drepper
 Version 1.0
 15
as the offset into the cache line. The next S bits select
the “cache set”. We will go into more detail soon on
why sets, and not single slots, are used for cache lines.
SFor now it is sufficient to understand there are 2 sets of
cache lines. This leaves the top 32−S−O = T bits which
form the tag. These T bits are the value associated with
each cache line to distinguish all the aliases18 which are
cached in the same cache set. The S bits used to address
the cache set do not have to be stored since they are the
same for all cache lines in the same set.
When an instruction modifies memory the processor still
has to load a cache line first because no instruction mod-
ifies an entire cache line at once (exception to the rule:
write-combining as explained in section 6.1). The con-
tent of the cache line before the write operation therefore
has to be loaded. It is not possible for a cache to hold
partial cache lines. A cache line which has been written
to and which has not been written back to main memory
is said to be “dirty”. Once it is written the dirty flag is
cleared.
To be able to load new data in a cache it is almost always
first necessary to make room in the cache. An eviction
from L1d pushes the cache line down into L2 (which
uses the same cache line size). This of course means
room has to be made in L2. This in turn might push the
content into L3 and ultimately into main memory. Each
eviction is progressively more expensive. What is de-
scribed here is the model for an exclusive cache as is
preferred by modern AMD and VIA processors. Intel
implements inclusive caches19 where each cache line in
L1d is also present in L2. Therefore evicting from L1d is
much faster. With enough L2 cache the disadvantage of
wasting memory for content held in two places is mini-
mal and it pays off when evicting. A possible advantage
of an exclusive cache is that loading a new cache line
only has to touch the L1d and not the L2, which could be
faster.
The CPUs are allowed to manage the caches as they like
as long as the memory model defined for the processor
architecture is not changed. It is, for instance, perfectly
fine for a processor to take advantage of little or no mem-
ory bus activity and proactively write dirty cache lines
back to main memory. The wide variety of cache archi-
tectures among the processors for the x86 and x86-64,
between manufacturers and even within the models of
the same manufacturer, are testament to the power of the
memory model abstraction.
In symmetric multi-processor (SMP) systems the caches
of the CPUs cannot work independently from each other.
All processors are supposed to see the same memory con-
tent at all times. The maintenance of this uniform view
of memory is called “cache coherency”. If a processor
were to look simply at its own caches and main mem-
18 All cache lines with the same S part of the address are known by
the same alias.
19This generalization is not completely correct. A few caches are
exclusive and some inclusive caches have exclusive cache properties.
ory it would not see the content of dirty cache lines in
other processors. Providing direct access to the caches
of one processor from another processor would be terri-
bly expensive and a huge bottleneck. Instead, processors
detect when another processor wants to read or write to a
certain cache line.
If a write access is detected and the processor has a clean
copy of the cache line in its cache, this cache line is
marked invalid. Future references will require the cache
line to be reloaded. Note that a read access on another
CPU does not necessitate an invalidation, multiple clean
copies can very well be kept around.
More sophisticated cache implementations allow another
possibility to happen. Assume a cache line is dirty in
one processor’s cache and a second processor wants to
read or write that cache line. In this case the main mem-
ory is out-of-date and the requesting processor must, in-
stead, get the cache line content from the first proces-
sor. Through snooping, the first processor notices this
situation and automatically sends the requesting proces-
sor the data. This action bypasses main memory, though
in some implementations the memory controller is sup-
posed to notice this direct transfer and store the updated
cache line content in main memory. If the access is for
writing the first processor then invalidates its copy of the
local cache line.
Over time a number of cache coherency protocols have
been developed. The most important is MESI, which we
will introduce in section 3.3.4. The outcome of all this
can be summarized in a few simple rules:
• A dirty cache line is not present in any other pro-
cessor’s cache.
• Clean copies of the same cache line can reside in
arbitrarily many caches.
If these rules can be maintained, processors can use their
caches efficiently even in multi-processor systems. All
the processors need to do is to monitor each others’ write
accesses and compare the addresses with those in their
local caches. In the next section we will go into a few
more details about the implementation and especially the
costs.
Finally, we should at least give an impression of the costs
associated with cache hits and misses. These are the
numbers Intel lists for a Pentium M:
To Where
Register
L1d
L2
Main Memory
Cycles
≤ 1
∼ 3
∼ 14
∼ 240
These are the actual access times measured in CPU cy-
cles. It is interesting to note that for the on-die L2 cache
16
 Version 1.0
 What Every Programmer Should Know About Memory
a large part (probably even the majority) of the access
time is caused by wire delays. This is a physical lim-
itation which can only get worse with increasing cache
sizes. Only process shrinking (for instance, going from
60nm for Merom to 45nm for Penryn in Intel’s lineup)
can improve those numbers.
The numbers in the table look high but, fortunately, the
entire cost does not have to be paid for each occurrence
of the cache load and miss. Some parts of the cost can
be hidden. Today’s processors all use internal pipelines
of different lengths where the instructions are decoded
and prepared for execution. Part of the preparation is
loading values from memory (or cache) if they are trans-
ferred to a register. If the memory load operation can
be started early enough in the pipeline, it may happen in
parallel with other operations and the entire cost of the
load might be hidden. This is often possible for L1d; for
some processors with long pipelines for L2 as well.
There are many obstacles to starting the memory read
early. It might be as simple as not having sufficient re-
sources for the memory access or it might be that the final
address of the load becomes available late as the result of
another instruction. In these cases the load costs cannot
be hidden (completely).
For write operations the CPU does not necessarily have
to wait until the value is safely stored in memory. As
long as the execution of the following instructions ap-
pears to have the same effect as if the value were stored
in memory there is nothing which prevents the CPU from
taking shortcuts. It can start executing the next instruc-
tion early. With the help of shadow registers which can
hold values no longer available in a regular register it is
even possible to change the value which is to be stored in
the incomplete write operation.
1000
500
n
 200
oi t 100
ar 50
epO 20
/s 10
el c 5
yC 2
1
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Figure 3.4: Access Times for Random Writes
For an illustration of the effects of cache behavior see
Figure 3.4. We will talk about the program which gener-
ated the data later; it is a simple simulation of a program
which accesses a configurable amount of memory repeat-
edly in a random fashion. Each data item has a fixed size.
The number of elements depends on the selected work-
ing set size. The Y–axis shows the average number of
CPU cycles it takes to process one element; note that the
scale for the Y–axis is logarithmic. The same applies in
all the diagrams of this kind to the X–axis. The size of
the working set is always shown in powers of two.
The graph shows three distinct plateaus. This is not sur-
prising: the specific processor has L1d and L2 caches,
but no L3. With some experience we can deduce that the
L1d is 213 bytes in size and that the L2 is 220 bytes in
size. If the entire working set fits into the L1d the cycles
per operation on each element is below 10. Once the L1d
size is exceeded the processor has to load data from L2
and the average time springs up to around 28. Once the
L2 is not sufficient anymore the times jump to 480 cycles
and more. This is when many or most operations have to
load data from main memory. And worse: since data is
being modified dirty cache lines have to be written back,
too.
This graph should give sufficient motivation to look into
coding improvements which help improve cache usage.
We are not talking about a difference of a few measly per-
cent here; we are talking about orders-of-magnitude im-
provements which are sometimes possible. In section 6
we will discuss techniques which allow writing more ef-
ficient code. The next section goes into more details of
CPU cache designs. The knowledge is good to have but
not necessary for the rest of the paper. So this section
could be skipped.
3.3
 CPU Cache Implementation Details
Cache implementers have the problem that each cell in
the huge main memory potentially has to be cached. If
the working set of a program is large enough this means
there are many main memory locations which fight for
each place in the cache. Previously it was noted that a
ratio of 1-to-1000 for cache versus main memory size is
not uncommon.
3.3.1
 Associativity
It would be possible to implement a cache where each
cache line can hold a copy of any memory location (see
Figure 3.5). This is called a fully associative cache. To
access a cache line the processor core would have to
compare the tags of each and every cache line with the
tag for the requested address. The tag would be com-
prised of the entire part of the address which is not the
offset into the cache line (that means, S in the figure on
page 15 is zero).
There are caches which are implemented like this but,
by looking at the numbers for an L2 in use today, will
show that this is impractical. Given a 4MB cache with
64B cache lines the cache would have 65,536 entries.
To achieve adequate performance the cache logic would
have to be able to pick from all these entries the one
matching a given tag in just a few cycles. The effort to
Ulrich Drepper
 Version 1.0
 17
Tag
 Offset
Tag
T T T T
T
 O
Comp
T
 O
Comp
T
 O
Comp
T
 O
Comp
O O O O
Data
Figure 3.5: Fully Associative Cache Schematics
implement this would be enormous.
For each cache line a comparator is needed to compare
the large tag (note, S is zero). The letter next to each
connection indicates the width in bits. If none is given
it is a single bit line. Each comparator has to compare
two T-bit-wide values. Then, based on the result, the ap-
propriate cache line content is selected and made avail-
able. This requires merging as many sets of O data lines
as there are cache buckets. The number of transistors
needed to implement a single comparator is large espe-
cially since it must work very fast. No iterative com-
parator is usable. The only way to save on the number
of comparators is to reduce the number of them by iter-
atively comparing the tags. This is not suitable for the
same reason that iterative comparators are not: it takes
too long.
Fully associative caches are practical for small caches
(for instance, the TLB caches on some Intel processors
are fully associative) but those caches are small, really
small. We are talking about a few dozen entries at most.
For L1i, L1d, and higher level caches a different ap-
proach is needed. What can be done is to restrict the
search. In the most extreme restriction each tag maps
to exactly one cache entry. The computation is simple:
given the 4MB/64B cache with 65,536 entries we can
directly address each entry by using bits 6 to 21 of the
address (16 bits). The low 6 bits are the index into the
cache line.
Tag
 Set
 Offset
S
 S
T
 O
T
 T
 O
X
 X
T
 O
U Comp
 UT
 M M O
T
 O
O
Tag
 Data
Figure 3.6: Direct-Mapped Cache Schematics
Such a direct-mapped cache is fast and relatively easy
to implement as can be seen in Figure 3.6. It requires
exactly one comparator, one multiplexer (two in this di-
agram where tag and data are separated, but this is not
a hard requirement on the design), and some logic to
select only valid cache line content. The comparator is
complex due to the speed requirements but there is only
one of them now; as a result more effort can be spent
on making it fast. The real complexity in this approach
lies in the multiplexers. The number of transistors in a
simple multiplexer grows with O(log N), where N is the
number of cache lines. This is tolerable but might get
slow, in which case speed can be increased by spending
more real estate on transistors in the multiplexers to par-
allelize some of the work and to increase the speed. The
total number of transistors can grow slowly with a grow-
ing cache size which makes this solution very attractive.
But it has a drawback: it only works well if the addresses
used by the program are evenly distributed with respect
to the bits used for the direct mapping. If they are not,
and this is usually the case, some cache entries are heav-
ily used and therefore repeated evicted while others are
hardly used at all or remain empty.
Tag
 Set
 Offset
S
 S
MUX
 MUX
···
 T T T T
 ···
···
 T
 Comp
 O
 ···
···
 T
 Comp
 O
 ···
···
 T
 Comp
 O
 ···
···
 T
 Comp
 O
 ···
Tag
 Data
O O O O
Figure 3.7: Set-Associative Cache Schematics
This problem can be solved by making the cache set as-
sociative. A set-associative cache combines the good
features of the full associative and direct-mapped caches
to largely avoid the weaknesses of those designs. Fig-
ure 3.7 shows the design of a set-associative cache. The
tag and data storage are divided into sets, one of which is
selected by the address of a cache line. This is similar to
the direct-mapped cache. But instead of only having one
element for each set value in the cache a small number
of values is cached for the same set value. The tags for
all the set members are compared in parallel, which is
similar to the functioning of the fully associative cache.
The result is a cache which is not easily defeated by
unfortunate–or deliberate–selection of addresses with the
same set numbers and at the same time the size of the
cache is not limited by the number of comparators which
can be implemented economically. If the cache grows it
is (in this figure) only the number of columns which in-
creases, not the number of rows. The number of rows
(and therefore comparators) only increases if the asso-
ciativity of the cache is increased. Today processors are
using associativity levels of up to 24 for L2 caches or
higher. L1 caches usually get by with 8 sets.
18
 Version 1.0
 What Every Programmer Should Know About Memory
L2
 Associativity
Cache
 Direct
 2
 4
 8
Size
CL=32
 CL=64
 CL=32
 CL=64
 CL=32
 CL=64
 CL=32
 CL=64
512k
 27,794,595
 20,422,527
 25,222,611
 18,303,581
 24,096,510
 17,356,121
 23,666,929
 17,029,334
1M
 19,007,315
 13,903,854
 16,566,738
 12,127,174
 15,537,500
 11,436,705
 15,162,895
 11,233,896
2M
 12,230,962
 8,801,403
 9,081,881
 6,491,011
 7,878,601
 5,675,181
 7,391,389
 5,382,064
4M
 7,749,986
 5,427,836
 4,736,187
 3,159,507
 3,788,122
 2,418,898
 3,430,713
 2,125,103
8M
 4,731,904
 3,209,693
 2,690,498
 1,602,957
 2,207,655
 1,228,190
 2,111,075
 1,155,847
16M
 2,620,587
 1,528,592
 1,958,293
 1,089,580
 1,704,878
 883,530
 1,671,541
 862,324
Table 3.1: Effects of Cache Size, Associativity, and Line Size
Given our 4MB/64B cache and 8-way set associativity
the cache we are left with has 8,192 sets and only 13
bits of the tag are used in addressing the cache set. To
determine which (if any) of the entries in the cache set
contains the addressed cache line 8 tags have to be com-
pared. That is feasible to do in very short time. With an
experiment we can see that this makes sense.
Table 3.1 shows the number of L2 cache misses for a
program (gcc in this case, the most important benchmark
of them all, according to the Linux kernel people) for
changing cache size, cache line size, and associativity set
size. In section 7.2 we will introduce the tool to simulate
the caches as required for this test.
Just in case this is not yet obvious, the relationship of all
these values is that the cache size is
cache line size × associativity × number of sets
The addresses are mapped into the cache by using
O
S
=
=
log2 cache line size
log2 number of sets
in the way the figure on page 15 shows.
Figure 3.8 makes the data of the table more comprehen-
sible. It shows the data for a fixed cache line size of
32 bytes. Looking at the numbers for a given cache size
we can see that associativity can indeed help to reduce
the number of cache misses significantly. For an 8MB
cache going from direct mapping to 2-way set associative
cache saves almost 44% of the cache misses. The proces-
sor can keep more of the working set in the cache with
a set associative cache compared with a direct mapped
cache.
In the literature one can occasionally read that introduc-
ing associativity has the same effect as doubling cache
size. This is true in some extreme cases as can be seen
in the jump from the 4MB to the 8MB cache. But it
certainly is not true for further doubling of the associa-
tivity. As we can see in the data, the successive gains are
28
)
s n 24
oil l i 20
Mn i 16
(se s 12
siM 8
ehc 4
aC0
512k
 1M
 2M
 4M
 8M
 16M
Cache Size
Direct
 2-way
 4-way
 8-way
Figure 3.8: Cache Size vs Associativity (CL=32)
much smaller. We should not completely discount the ef-
fects, though. In the example program the peak memory
use is 5.6M. So with a 8MB cache there are unlikely to
be many (more than two) uses for the same cache set.
With a larger working set the savings can be higher as
we can see from the larger benefits of associativity for
the smaller cache sizes.
In general, increasing the associativity of a cache above
8 seems to have little effects for a single-threaded work-
load. With the introduction of hyper-threaded proces-
sors where the first level cache is shared and multi-core
processors which use a shared L2 cache the situation
changes. Now you basically have two programs hitting
on the same cache which causes the associativity in prac-
tice to be halved (or quartered for quad-core processors).
So it can be expected that, with increasing numbers of
cores, the associativity of the shared caches should grow.
Once this is not possible anymore (16-way set associa-
tivity is already hard) processor designers have to start
using shared L3 caches and beyond, while L2 caches are
potentially shared by a subset of the cores.
Another effect we can study in Figure 3.8 is how the in-
crease in cache size helps with performance. This data
cannot be interpreted without knowing about the working
Ulrich Drepper
 Version 1.0
 19
set size. Obviously, a cache as large as the main mem-
ory would lead to better results than a smaller cache, so
there is in general no limit to the largest cache size with
measurable benefits.
As already mentioned above, the size of the working set
at its peak is 5.6M. This does not give us any absolute
number of the maximum beneficial cache size but it al-
lows us to estimate the number. The problem is that
not all the memory used is contiguous and, therefore,
we have, even with a 16M cache and a 5.6M working
set, conflicts (see the benefit of the 2-way set associa-
tive 16MB cache over the direct mapped version). But
it is a safe bet that with the same workload the benefits
of a 32MB cache would be negligible. But who says the
working set has to stay the same? Workloads are grow-
ing over time and so should the cache size. When buying
machines, and one has to choose the cache size one is
willing to pay for, it is worthwhile to measure the work-
ing set size. Why this is important can be seen in the
figures on page 21.
Random
Sequential
Figure 3.9: Test Memory Layouts
Two types of tests are run. In the first test the elements
are processed sequentially. The test program follows the
pointer n but the array elements are chained so that they
are traversed in the order in which they are found in mem-
ory. This can be seen in the lower part of Figure 3.9.
There is one back reference from the last element. In the
second test (upper part of the figure) the array elements
are traversed in a random order. In both cases the array
elements form a circular single-linked list.
3.3.2
 Measurements of Cache Effects
All the figures are created by measuring a program which
can simulate working sets of arbitrary size, read and write
access, and sequential or random access. We have al-
ready seen some results in Figure 3.4. The program cre-
ates an array corresponding to the working set size of
elements of this type:
struct l {
struct l *n;
long int pad[NPAD];
};
All entries are chained in a circular list using the n el-
ement, either in sequential or random order. Advancing
from one entry to the next always uses the pointer, even if
the elements are laid out sequentially. The pad element
is the payload and it can grow arbitrarily large. In some
tests the data is modified, in others the program only per-
forms read operations.
In the performance measurements we are talking about
working set sizes. The working set is made up of an ar-
ray of struct l elements. A working set of 2N bytes
contains
2N /sizeof(struct l)
elements. Obviously sizeof(struct l) depends on
the value of NPAD. For 32-bit systems, NPAD=7 means the
size of each array element is 32 bytes, for 64-bit systems
the size is 64 bytes.
Single Threaded Sequential Access
 The simplest
case is a simple walk over all the entries in the list. The
list elements are laid out sequentially, densely packed.
Whether the order of processing is forward or backward
does not matter, the processor can deal with both direc-
tions equally well. What we measure here–and in all the
following tests–is how long it takes to handle a single list
element. The time unit is a processor cycle. Figure 3.10
shows the result. Unless otherwise specified, all mea-
surements are made on a Pentium 4 machine in 64-bit
mode which means the structure l with NPAD=0 is eight
bytes in size.
The first two measurements are polluted by noise. The
measured workload is simply too small to filter the ef-
fects of the rest of the system out. We can safely assume
that the values are all at the 4 cycles level. With this in
mind we can see three distinct levels:
• Up to a working set size of 214 bytes.
• From 215 bytes to 220 bytes.
• From 221 bytes and up.
These steps can be easily explained: the processor has a
16kB L1d and 1MB L2. We do not see sharp edges in the
transition from one level to the other because the caches
are used by other parts of the system as well and so the
cache is not exclusively available for the program data.
Specifically the L2 cache is a unified cache and also used
for the instructions (NB: Intel uses inclusive caches).
What is perhaps not quite expected are the actual times
for the different working set sizes. The times for the L1d
hits are expected: load times after an L1d hit are around
4 cycles on the P4. But what about the L2 accesses?
Once the L1d is not sufficient to hold the data one might
expect it would take 14 cycles or more per element since
this is the access time for the L2. But the results show
that only about 9 cycles are required. This discrepancy
20
 Version 1.0
 What Every Programmer Should Know About Memory
10
9
t
nemelEtsiL/selcyC8
7
6
5
4
3
2
1
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Figure 3.10: Sequential Read Access, NPAD=0
350
300
t
nemelEtsiL/selcyC250
200
150
100
50
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
NPAD=0
 NPAD=7
 NPAD=15
 NPAD=31
Figure 3.11: Sequential Read for Several Sizes
400
350
t
nemelEtsiL/selcyC300
250
200
150
100
50
0
210
 212 214 216 218 220 222
 224
Working Set Size (Bytes)
On Cache Line
 On Page
Figure 3.12: TLB Influence for Sequential Read
Ulrich Drepper
 Versioncan be explained by the advanced logic in the processors.
In anticipation of using consecutive memory regions, the
processor prefetches the next cache line. This means that
when the next line is actually used it is already halfway
loaded. The delay required to wait for the next cache line
to be loaded is therefore much less than the L2 access
time.
The effect of prefetching is even more visible once the
working set size grows beyond the L2 size. Before we
said that a main memory access takes 200+ cycles. Only
with effective prefetching is it possible for the processor
to keep the access times as low as 9 cycles. As we can
see from the difference between 200 and 9, this works
out nicely.
We can observe the processor while prefetching, at least
indirectly. In Figure 3.11 we see the times for the same
working set sizes but this time we see the graphs for dif-
ferent sizes of the structure l. This means we have fewer
but larger elements in the list. The different sizes have
the effect that the distance between the n elements in the
(still consecutive) list grows. In the four cases of the
graph the distance is 0, 56, 120, and 248 bytes respec-
tively.
At the bottom we can see the line from Figure 3.10, but
this time it appears more or less as a flat line. The times
for the other cases are simply so much worse. We can see
in this graph, too, the three different levels and we see the
large errors in the tests with the small working set sizes
(ignore them again). The lines more or less all match
each other as long as only the L1d is involved. There is
no prefetching necessary so all element sizes just hit the
L1d for each access.
For the L2 cache hits we see that the three new lines
all pretty much match each other but that they are at a
higher level (about 28). This is the level of the access
time for the L2. This means prefetching from L2 into
L1d is basically disabled. Even with NPAD=7 we need a
new cache line for each iteration of the loop; for NPAD=0,
instead, the loop has to iterate eight times before the next
cache line is needed. The prefetch logic cannot load a
new cache line every cycle. Therefore we see a stall to
load from L2 in every iteration.
It gets even more interesting once the working set size
exceeds the L2 capacity. Now all four lines vary widely.
The different element sizes play obviously a big role in
the difference in performance. The processor should rec-
ognize the size of the strides and not fetch unnecessary
cache lines for NPAD=15 and 31 since the element size
is smaller than the prefetch window (see section 6.3.1).
Where the element size is hampering the prefetching ef-
forts is a result of a limitation of hardware prefetching:
it cannot cross page boundaries. We are reducing the ef-
fectiveness of the hardware scheduler by 50% for each
size increase. If the hardware prefetcher were allowed to
cross page boundaries and the next page is not resident
or valid the OS would have to get involved in locating the
1.0
 21
page. That means the program would experience a page
fault it did not initiate itself. This is completely unaccept-
able since the processor does not know whether a page is
not present or does not exist. In the latter case the OS
would have to abort the process. In any case, given that,
for NPAD=7 and higher, we need one cache line per list
element the hardware prefetcher cannot do much. There
simply is no time to load the data from memory since
all the processor does is read one word and then load the
next element.
Another big reason for the slowdown are the misses of
the TLB cache. This is a cache where the results of the
translation of a virtual address to a physical address are
stored, as is explained in more detail in section 4. The
TLB cache is quite small since it has to be extremely
fast. If more pages are accessed repeatedly than the TLB
cache has entries for the translation from virtual to phys-
ical address has to be constantly repeated. This is a very
costly operation. With larger element sizes the cost of
a TLB lookup is amortized over fewer elements. That
means the total number of TLB entries which have to be
computed per list element is higher.
To observe the TLB effects we can run a different test.
For one measurement we lay out the elements sequen-
tially as usual. We use NPAD=7 for elements which oc-
cupy one entire cache line. For the second measurement
we place each list element on a separate page. The rest
of each page is left untouched and we do not count it in
the total for the working set size.20 The consequence is
that, for the first measurement, each list iteration requires
a new cache line and, for every 64 elements, a new page.
For the second measurement each iteration requires load-
ing a new cache line which is on a new page.
The result can be seen in Figure 3.12. The measurements
were performed on the same machine as Figure 3.11.
Due to limitations of the available RAM the working set
size had to be restricted to 224 bytes which requires 1GB
to place the objects on separate pages. The lower, red
curve corresponds exactly to the NPAD=7 curve in Fig-
ure 3.11. We see the distinct steps showing the sizes of
the L1d and L2 caches. The second curve looks radically
different. The important feature is the huge spike start-
ing when the working set size reaches 213 bytes. This
is when the TLB cache overflows. With an element size
of 64 bytes we can compute that the TLB cache has 64
entries. There are no page faults affecting the cost since
the program locks the memory to prevent it from being
swapped out.
As can be seen the number of cycles it takes to compute
the physical address and store it in the TLB is very high.
The graph in Figure 3.12 shows the extreme case, but it
should now be clear that a significant factor in the slow-
20 Yes, this is a bit inconsistent because in the other tests we count
the unused part of the struct in the element size and we could define
NPAD so that each element fills a page. In that case the working set
sizes would be very different. This is not the point of this test, though,
and since prefetching is ineffective anyway this makes little difference.
down for larger NPAD values is the reduced efficiency of
the TLB cache. Since the physical address has to be com-
puted before a cache line can be read for either L2 or
main memory the address translation penalties are addi-
tive to the memory access times. This in part explains
why the total cost per list element for NPAD=31 is higher
than the theoretical access time for the RAM.
We can glimpse a few more details of the prefetch im-
plementation by looking at the data of test runs where
the list elements are modified. Figure 3.13 shows three
lines. The element width is in all cases 16 bytes. The first
line is the now familiar list walk which serves as a base-
line. The second line, labeled “Inc”, simply increments
the pad[0] member of the current element before going
on to the next. The third line, labeled “Addnext0”, takes
the pad[0] list element of the next element and adds it
to the pad[0] member of the current list element.
The naı̈ve assumption would be that the “Addnext0” test
runs slower because it has more work to do. Before ad-
vancing to the next list element a value from that element
has to be loaded. This is why it is surprising to see that
this test actually runs, for some working set sizes, faster
than the “Inc” test. The explanation for this is that the
load from the next list element is basically a forced pre-
fetch. Whenever the program advances to the next list
element we know for sure that element is already in the
L1d cache. As a result we see that the “Addnext0” per-
forms as well as the simple “Follow” test as long as the
working set size fits into the L2 cache.
The “Addnext0” test runs out of L2 faster than the “Inc”
test, though. It needs more data loaded from main mem-
ory. This is why the “Addnext0” test reaches the 28 cy-
cles level for a working set size of 221 bytes. The 28 cy-
cles level is twice as high as the 14 cycles level the “Fol-
low” test reaches. This is easy to explain, too. Since the
other two tests modify memory an L2 cache eviction to
make room for new cache lines cannot simply discard the
data. Instead it has to be written to memory. This means
the available bandwidth on the FSB is cut in half, hence
doubling the time it takes to transfer the data from main
memory to L2.
One last aspect of the sequential, efficient cache han-
dling is the size of the cache. This should be obvious
but it still should be pointed out. Figure 3.14 shows the
timing for the Increment benchmark with 128-byte ele-
ments (NPAD=15 on 64-bit machines). This time we see
the measurement from three different machines. The first
two machines are P4s, the last one a Core2 processor.
The first two differentiate themselves by having different
cache sizes. The first processor has a 32k L1d and an 1M
L2. The second one has 16k L1d, 512k L2, and 2M L3.
The Core2 processor has 32k L1d and 4M L2.
The interesting part of the graph is not necessarily how
well the Core2 processor performs relative to the other
two (although it is impressive). The main point of in-
terest here is the region where the working set size is
22
 Version 1.0
 What Every Programmer Should Know About Memory
30
t
nemelEtsiL/selcyC25
20
15
10
5
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Follow
 Inc
 Addnext0
Figure 3.13: Sequential Read and Write, NPAD=1
700
600
t
nemelEtsiL/selcyC500
400
300
200
100
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
P4/32k/1M
 P4/16k/512k/2M
 Core2/32k/4M
Figure 3.14: Advantage of Larger L2/L3 Caches
500
450
t
nemelEtsiL/selcyC400
350
300
250
200
150
100
50
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Sequential
 Random
Figure 3.15: Sequential vs Random Read, NPAD=0
Ulrich Drepper
 Versiontoo large for the respective last level cache and the main
memory gets heavily involved.
As expected, the larger the last level cache is the longer
the curve stays at the low level corresponding to the L2
access costs. The important part to notice is the perfor-
mance advantage this provides. The second processor
(which is slightly older) can perform the work on the
working set of 220 bytes twice as fast as the first proces-
sor. All thanks to the increased last level cache size. The
Core2 processor with its 4M L2 performs even better.
For a random workload this might not mean that much.
But if the workload can be tailored to the size of the last
level cache the program performance can be increased
quite dramatically. This is why it sometimes is worth-
while to spend the extra money for a processor with a
larger cache.
Single Threaded Random Access We have seen that
the processor is able to hide most of the main memory
and even L2 access latency by prefetching cache lines
into L2 and L1d. This can work well only when the mem-
ory access is predictable, though.
If the access pattern is unpredictable or random the situa-
tion is quite different. Figure 3.15 compares the per-list-
element times for the sequential access (same as in Fig-
ure 3.10) with the times when the list elements are ran-
domly distributed in the working set. The order is deter-
mined by the linked list which is randomized. There is no
way for the processor to reliably prefetch data. This can
only work by chance if elements which are used shortly
after one another are also close to each other in memory.
There are two important points to note in Figure 3.15.
The first is the large number of cycles needed for grow-
ing working set sizes. The machine makes it possible
to access the main memory in 200-300 cycles but here
we reach 450 cycles and more. We have seen this phe-
nomenon before (compare Figure 3.11). The automatic
prefetching is actually working to a disadvantage here.
The second interesting point is that the curve is not flat-
tening at various plateaus as it has been for the sequen-
tial access cases. The curve keeps on rising. To explain
this we can measure the L2 access of the program for
the various working set sizes. The result can be seen in
Figure 3.16 and Table 3.2.
The figure shows that, when the working set size is larger
than the L2 size, the cache miss ratio (L2 accesses / L2
misses) starts to grow. The curve has a similar form to
the one in Figure 3.15: it rises quickly, declines slightly,
and starts to rise again. There is a strong correlation with
the cycles per list element graph. The L2 miss rate will
grow until it eventually reaches close to 100%. Given a
large enough working set (and RAM) the probability that
any of the randomly picked cache lines is in L2 or is in
the process of being loaded can be reduced arbitrarily.
1.0
 23
Sequential
 Random
Set
 Ratio
 Ł2 Accesses
 Ratio
 L2 Accesses
Size
 L2 Hit
 L2 Miss
 #Iter
 Miss/Hit
 per Iteration
 L2 Hit
 L2 Miss
 #Iter
 Miss/Hit
 per Iteration
220
 88,636
 843
 16,384
 0.94%
 5.5
 30,462
 4721
 1,024
 13.42%
 34.4
21
2
 88,105
 1,584
 8,192
 1.77%
 10.9
 21,817
 15,151
 512
 40.98%
 72.2
2
22
 88,106
 1,600
 4,096
 1.78%
 21.9
 22,258
 22,285
 256
 50.03%
 174.0
23
2
 88,104
 1,614
 2,048
 1.80%
 43.8
 27,521
 26,274
 128
 48.84%
 420.3
224
 88,114
 1,655
 1,024
 1.84%
 87.7
 33,166
 29,115
 64
 46.75%
 973.1
2
25
 88,112
 1,730
 512
 1.93%
 175.5
 39,858
 32,360
 32
 44.81%
 2,256.8
26
2
 88,112
 1,906
 256
 2.12%
 351.6
 48,539
 38,151
 16
 44.01%
 5,418.1
27
2
 88,114
 2,244
 128
 2.48%
 705.9
 62,423
 52,049
 8
 45.47%
 14,309.0
228
 88,120
 2,939
 64
 3.23%
 1,422.8
 81,906
 87,167
 4
 51.56%
 42,268.3
29
2
 88,137
 4,318
 32
 4.67%
 2,889.2
 119,079
 163,398
 2
 57.84%
 141,238.5
Table 3.2: L2 Hits and Misses for SequentialThe increasing cache miss rate alone explains some of
the costs. But there is another factor. Looking at Ta-
ble 3.2 we can see in the L2/#Iter columns that the total
number of L2 uses per iteration of the program is grow-
ing. Each working set is twice as large as the one be-
fore. So, without caching we would expect double the
main memory accesses. With caches and (almost) per-
fect predictability we see the modest increase in the L2
use shown in the data for sequential access. The increase
is due to the increase of the working set size and nothing
else.
For random access the per-element access time more than
doubles for each doubling of the working set size. This
means the average access time per list element increases
since the working set size only doubles. The reason be-
hind this is a rising rate of TLB misses. In Figure 3.17 we
see the cost for random accesses for NPAD=7. Only this
time the randomization is modified. While in the normal
case the entire list of randomized as one block (indicated
by the label ∞) the other 11 curves show randomizations
which are performed in smaller blocks. For the curve
labeled ‘60’ each set of 60 pages (245.760 bytes) is ran-
domized individually. That means all list elements in the
block are traversed before going over to an element in
the next block. This has the effect that number of TLB
entries which are used at any one time is limited.
The element size for NPAD=7 is 64 bytes, which corre-
sponds to the cache line size. Due to the randomized or-
der of the list elements it is unlikely that the hardware
prefetcher has any effect, most certainly not for more
than a handful of elements. This means the L2 cache
miss rate does not differ significantly from the random-
ization of the entire list in one block. The performance
of the test with increasing block size approaches asymp-
totically the curve for the one-block randomization. This
means the performance of this latter test case is signifi-
cantly influenced by the TLB misses. If the TLB misses
can be lowered the performance increases significantly
and Random Walks, NPAD=0
60%
50%
s
essiM2L40%
30%
20%
10%
0%
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Sequential
 Random
Figure 3.16: L2d Miss Ratio
550
500
450
t
n e 400
m 350
elE 300
ts i 250
L/ s 200
el c 150
yC 100
50
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
60 120 240
 480 960 1920
 3840
 7680
15360 30720
 61440 ∞
Figure 3.17: Page-Wise Randomization, NPAD=7
24
 Version 1.0
 What Every Programmer Should Know About Memory
(in one test we will see later up to 38%).
3.3.3
 Write Behavior
Before we start looking at the cache behavior when mul-
tiple execution contexts (threads or processes) use the
same memory we have to explore a detail of cache im-
plementations. Caches are supposed to be coherent and
this coherency is supposed to be completely transparent
for the userlevel code. Kernel code is a different story; it
occasionally requires cache flushes.
This specifically means that, if a cache line is modified,
the result for the system after this point in time is the
same as if there were no cache at all and the main mem-
ory location itself had been modified. This can be imple-
mented in two ways or policies:
• write-through cache implementation;
• write-back cache implementation.
The write-through cache is the simplest way to imple-
ment cache coherency. If the cache line is written to,
the processor immediately also writes the cache line into
main memory. This ensures that, at all times, the main
memory and cache are in sync. The cache content could
simply be discarded whenever a cache line is replaced.
This cache policy is simple but not very fast. A pro-
gram which, for instance, modifies a local variable over
and over again would create a lot of traffic on the FSB
even though the data is likely not used anywhere else and
might be short-lived.
The write-back policy is more sophisticated. Here the
processor does not immediately write the modified cache
line back to main memory. Instead, the cache line is only
marked as dirty. When the cache line is dropped from the
cache at some point in the future the dirty bit will instruct
the processor to write the data back at that time instead
of just discarding the content.
Write-back caches have the chance to be significantly
better performing, which is why most memory in a sys-
tem with a decent processor is cached this way. The pro-
cessor can even take advantage of free capacity on the
FSB to store the content of a cache line before the line
has to be evacuated. This allows the dirty bit to be cleared
and the processor can just drop the cache line when the
room in the cache is needed.
But there is a significant problem with the write-back im-
plementation. When more than one processor (or core or
hyper-thread) is available and accessing the same mem-
ory it must still be assured that both processors see the
same memory content at all times. If a cache line is dirty
on one processor (i.e., it has not been written back yet)
and a second processor tries to read the same memory lo-
cation, the read operation cannot just go out to the main
memory. Instead the content of the first processor’s cache
line is needed. In the next section we will see how this is
currently implemented.
Before we get to this there are two more cache policies
to mention:
• write-combining; and
• uncacheable.
Both these policies are used for special regions of the
address space which are not backed by real RAM. The
kernel sets up these policies for the address ranges (on
x86 processors using the Memory Type Range Regis-
ters, MTRRs) and the rest happens automatically. The
MTRRs are also usable to select between write-through
and write-back policies.
Write-combining is a limited caching optimization more
often used for RAM on devices such as graphics cards.
Since the transfer costs to the devices are much higher
than the local RAM access it is even more important
to avoid doing too many transfers. Transferring an en-
tire cache line just because a word in the line has been
written is wasteful if the next operation modifies the next
word. One can easily imagine that this is a common oc-
currence, the memory for horizontal neighboring pixels
on a screen are in most cases neighbors, too. As the name
suggests, write-combining combines multiple write ac-
cesses before the cache line is written out. In ideal cases
the entire cache line is modified word by word and, only
after the last word is written, the cache line is written to
the device. This can speed up access to RAM on devices
significantly.
Finally there is uncacheable memory. This usually means
the memory location is not backed by RAM at all. It
might be a special address which is hardcoded to have
some functionality implemented outside the CPU. For
commodity hardware this most often is the case for mem-
ory mapped address ranges which translate to accesses
to cards and devices attached to a bus (PCIe etc). On
embedded boards one sometimes finds such a memory
address which can be used to turn an LED on and off.
Caching such an address would obviously be a bad idea.
LEDs in this context are used for debugging or status re-
ports and one wants to see this as soon as possible. The
memory on PCIe cards can change without the CPU’s
interaction, so this memory should not be cached.
3.3.4
 Multi-Processor Support
In the previous section we have already pointed out the
problem we have when multiple processors come into
play. Even multi-core processors have the problem for
those cache levels which are not shared (at least the L1d).
It is completely impractical to provide direct access from
one processor to the cache of another processor. The con-
nection is simply not fast enough, for a start. The prac-
tical alternative is to transfer the cache content over to
Ulrich Drepper
 Version 1.0
 25
the other processor in case it is needed. Note that this
also applies to caches which are not shared on the same
processor.
The question now is when does this cache line transfer
have to happen? This question is pretty easy to answer:
when one processor needs a cache line which is dirty in
another processor’s cache for reading or writing. But
how can a processor determine whether a cache line is
dirty in another processor’s cache? Assuming it just be-
cause a cache line is loaded by another processor would
be suboptimal (at best). Usually the majority of mem-
ory accesses are read accesses and the resulting cache
lines are not dirty. Processor operations on cache lines
are frequent (of course, why else would we have this
paper?) which means broadcasting information about
changed cache lines after each write access would be im-
practical.
What developed over the years is the MESI cache co-
herency protocol (Modified, Exclusive, Shared, Invalid).
The protocol is named after the four states a cache line
can be in when using the MESI protocol:
Modified: The local processor has modified the cache
line. This also implies it is the only copy in any
cache.
Exclusive: The cache line is not modified but known to
not be loaded into any other processor’s cache.
Shared: The cache line is not modified and might exist
in another processor’s cache.
Invalid: The cache line is invalid, i.e., unused.
This protocol developed over the years from simpler ver-
sions which were less complicated but also less efficient.
With these four states it is possible to efficiently imple-
ment write-back caches while also supporting concurrent
use of read-only data on different processors.
M
S
E
I
local read
local write
remote read
remote write
Figure 3.18: MESI Protocol Transitions
The state changes are accomplished without too much
effort by the processors listening, or snooping, on the
other processors’ work. Certain operations a processor
performs are announced on external pins and thus make
the processor’s cache handling visible to the outside. The
address of the cache line in question is visible on the ad-
dress bus. In the following description of the states and
their transitions (shown in Figure 3.18) we will point out
when the bus is involved.
Initially all cache lines are empty and hence also Invalid.
If data is loaded into the cache for writing the cache
changes to Modified. If the data is loaded for reading
the new state depends on whether another processor has
the cache line loaded as well. If this is the case then the
new state is Shared, otherwise Exclusive.
If a Modified cache line is read from or written to on
the local processor, the instruction can use the current
cache content and the state does not change. If a sec-
ond processor wants to read from the cache line the first
processor has to send the content of its cache to the sec-
ond processor and then it can change the state to Shared.
The data sent to the second processor is also received
and processed by the memory controller which stores the
content in memory. If this did not happen the cache line
could not be marked as Shared. If the second processor
wants to write to the cache line the first processor sends
the cache line content and marks the cache line locally
as Invalid. This is the infamous “Request For Owner-
ship” (RFO) operation. Performing this operation in the
last level cache, just like the I→M transition is compara-
tively expensive. For write-through caches we also have
to add the time it takes to write the new cache line con-
tent to the next higher-level cache or the main memory,
further increasing the cost.
If a cache line is in the Shared state and the local pro-
cessor reads from it no state change is necessary and the
read request can be fulfilled from the cache. If the cache
line is locally written to the cache line can be used as well
but the state changes to Modified. It also requires that all
other possible copies of the cache line in other proces-
sors are marked as Invalid. Therefore the write operation
has to be announced to the other processors via an RFO
message. If the cache line is requested for reading by a
second processor nothing has to happen. The main mem-
ory contains the current data and the local state is already
Shared. In case a second processor wants to write to the
cache line (RFO) the cache line is simply marked Invalid.
No bus operation is needed.
The Exclusive state is mostly identical to the Shared state
with one crucial difference: a local write operation does
not have to be announced on the bus. The local cache
is known to be the only one holding this specific cache
line. This can be a huge advantage so the processor will
try to keep as many cache lines as possible in the Exclu-
sive state instead of the Shared state. The latter is the
fallback in case the information is not available at that
moment. The Exclusive state can also be left out com-
pletely without causing functional problems. It is only
the performance that will suffer since the E→M transi-
tion is much faster than the S→M transition.
26
 Version 1.0
 What Every Programmer Should Know About Memory
From this description of the state transitions it should be
clear where the costs specific to multi-processor opera-
tions are. Yes, filling caches is still expensive but now
we also have to look out for RFO messages. Whenever
such a message has to be sent things are going to be slow.
There are two situations when RFO messages are neces-
sary:
• A thread is migrated from one processor to another
and all the cache lines have to be moved over to the
new processor once.
• A cache line is truly needed in two different pro-
cessors. 21
In multi-thread or multi-process programs there is always
some need for synchronization; this synchronization is
implemented using memory. So there are some valid
RFO messages. They still have to be kept as infrequent
as possible. There are other sources of RFO messages,
though. In section 6 we will explain these scenarios. The
Cache coherency protocol messages must be distributed
among the processors of the system. A MESI transition
cannot happen until it is clear that all the processors in
the system have had a chance to reply to the message.
That means that the longest possible time a reply can
take determines the speed of the coherency protocol.22
Collisions on the bus are possible, latency can be high in
NUMA systems, and of course sheer traffic volume can
slow things down. All good reasons to focus on avoiding
unnecessary traffic.
There is one more problem related to having more than
one processor in play. The effects are highly machine
specific but in principle the problem always exists: the
FSB is a shared resource. In most machines all proces-
sors are connected via one single bus to the memory con-
troller (see Figure 2.1). If a single processor can saturate
the bus (as is usually the case) then two or four processors
sharing the same bus will restrict the bandwidth available
to each processor even more.
Even if each processor has its own bus to the memory
controller as in Figure 2.2 there is still the bus to the
memory modules. Usually this is one bus but, even in
the extended model in Figure 2.2, concurrent accesses to
the same memory module will limit the bandwidth.
The same is true with the AMD model where each pro-
cessor can have local memory. All processors can indeed
concurrently access their local memory quickly, espe-
cially with the integrated memory controller. But multi-
thread and multi-process programs–at least from time to
21 At a smaller level the same is true for two cores on the same pro-
cessor. The costs are just a bit smaller. The RFO message is likely to
be sent many times.
22 Which is why we see nowadays, for instance, AMD Opteron sys-
tems with three sockets. Each processor is exactly one hop away given
that the processors only have three hyperlinks and one is needed for the
Northbridge connection.
time–have to access the same memory regions to syn-
chronize.
Concurrency is severely limited by the finite bandwidth
available for the implementation of the necessary syn-
chronization. Programs need to be carefully designed to
minimize accesses from different processors and cores
to the same memory locations. The following measure-
ments will show this and the other cache effects related
to multi-threaded code.
Multi Threaded Access To ensure that the gravity of
the problems introduced by concurrently using the same
cache lines on different processors is understood, we will
look here at some more performance graphs for the same
program we used before. This time, though, more than
one thread is running at the same time. What is measured
is the fastest runtime of any of the threads. This means
the time for a complete run when all threads are done is
even higher. The machine used has four processors; the
tests use up to four threads. All processors share one bus
to the memory controller and there is only one bus to the
memory modules.
Figure 3.19 shows the performance for sequential read-
only access for 128 bytes entries (NPAD=15 on 64-bit ma-
chines). For the curve for one thread we can expect a
curve similar to Figure 3.11. The measurements are for a
different machine so the actual numbers vary.
The important part in this figure is of course the behavior
when running multiple threads. Note that no memory is
modified and no attempts are made to keep the threads
in sync when walking the linked list. Even though no
RFO messages are necessary and all the cache lines can
be shared, we see up to an 18% performance decrease
for the fastest thread when two threads are used and up
to 34% when four threads are used. Since no cache lines
have to be transported between the processors this slow-
down is solely caused by the one or both of the two bot-
tlenecks: the shared bus from the processor to the mem-
ory controller and bus from the memory controller to the
memory modules. Once the working set size is larger
than the L3 cache in this machine all three threads will
be prefetching new list elements. Even with two threads
the available bandwidth is not sufficient to scale linearly
(i.e., have no penalty from running multiple threads).
When we modify memory things get even uglier. Fig-
ure 3.20 shows the results for the sequential Increment
test. This graph is using a logarithmic scale for the Y
axis. So, do not be fooled by the apparently small dif-
ferences. We still have about a 18% penalty for run-
ning two threads and now an amazing 93% penalty for
running four threads. This means the prefetch traffic to-
gether with the write-back traffic is pretty much saturat-
ing the bus when four threads are used.
We use the logarithmic scale to show the results for the
L1d range. What can be seen is that, as soon as more
Ulrich Drepper
 Version 1.0
 27
500
450
t
nemelEtsiL/selcyC400
350
300
250
200
150
100
50
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
#Threads=1
 #Threads=2
 #Threads=4
Figure 3.19: Sequential Read Access, Multiple Threads
1000
500
t
nemelEtsiL/selcyC200
100
50
20
10
5
2
1
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
#Threads=1
 #Threads=2
 #Threads=4
Figure 3.20: Sequential Increment, Multiple Threads
1600
1400
t
nemelEtsiL/selcyC1200
1000
800
600
400
200
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
#Threads=1
 #Threads=2
 #Threads=4
Figure 3.21: Random Addnextlast, Multiple Threads
28
 Version 1.0
than one thread is running, the L1d is basically ineffec-
tive. The single-thread access times exceed 20 cycles
only when the L1d is not sufficient to hold the work-
ing set. When multiple threads are running, those access
times are hit immediately, even with the smallest work-
ing set sizes.
One aspect of the problem is not shown here. It is hard to
measure with this specific test program. Even though the
test modifies memory and we therefore must expect RFO
messages we do not see higher costs for the L2 range
when more than one thread is used. The program would
have to use a large amount of memory and all threads
must access the same memory in parallel. This is hard
to achieve without a lot of synchronization which would
then dominate the execution time.
Finally in Figure 3.21 we have the numbers for the Add-
nextlast test with random access of memory. This figure
is provided mainly to show the appallingly high numbers.
It now takes around 1,500 cycles to process a single list
element in the extreme case. The use of more threads
is even more questionable. We can summarize the effi-
ciency of multiple thread use in a table.
#Threads
 Seq Read
 Seq Inc
 Rand Add
2
 1.69
 1.69
 1.54
4
 2.98
 2.07
 1.65
Table 3.3: Efficiency for Multiple Threads
The table shows the efficiency for the multi-thread run
with the largest working set size in the three figures on
page 28. The number shows the best possible speed-up
the test program incurs for the largest working set size by
using two or four threads. For two threads the theoretical
limits for the speed-up are 2 and, for four threads, 4. The
numbers for two threads are not that bad. But for four
threads the numbers for the last test show that it is almost
not worth it to scale beyond two threads. The additional
benefit is minuscule. We can see this more easily if we
represent the data in Figure 3.21 a bit differently.
The curves in Figure 3.22 show the speed-up factors, i.e.,
relative performance compared to the code executed by
a single thread. We have to ignore the smallest sizes, the
measurements are not accurate enough. For the range of
the L2 and L3 cache we can see that we indeed achieve
almost linear acceleration. We almost reach factors of
2 and 4 respectively. But as soon as the L3 cache is
not sufficient to hold the working set the numbers crash.
They crash to the point that the speed-up of two and four
threads is identical (see the fourth column in Table 3.3).
This is one of the reasons why one can hardly find moth-
erboard with sockets for more than four CPUs all using
the same memory controller. Machines with more pro-
cessors have to be built differently (see section 5).
These numbers are not universal. In some cases even
working sets which fit into the last level cache will not
What Every Programmer Should Know About Memory
4.5
4
3.5
3
p
U 2.5
-de 2
ep 1.5
S1
0.5
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
#Threads=1
 #Threads=2
 #Threads=4
Figure 3.22: Speed-Up Through Parallelism
allow linear speed-ups. In fact, this is the norm since
threads are usually not as decoupled as is the case in this
test program. On the other hand it is possible to work
with large working sets and still take advantage of more
than two threads. Doing this requires thought, though.
We will talk about some approaches in section 6.
Special Case: Hyper-Threads Hyper-Threads (some-
times called Symmetric Multi-Threading, SMT) are im-
plemented by the CPU and are a special case since the
individual threads cannot really run concurrently. They
all share almost all the processing resources except for
the register set. Individual cores and CPUs still work
in parallel but the threads implemented on each core are
limited by this restriction. In theory there can be many
threads per core but, so far, Intel’s CPUs at most have
two threads per core. The CPU is responsible for time-
multiplexing the threads. This alone would not make
much sense, though. The real advantage is that the CPU
can schedule another hyper-thread and take advantage of
available resources such as arithmetic logic units (ALUs)
when the currently running hyper-thread is delayed. In
most cases this is a delay caused by memory accesses.
If two threads are running on one hyper-threaded core the
program is only more efficient than the single-threaded
code if the combined runtime of both threads is lower
than the runtime of the single-threaded code. This is pos-
sible by overlapping the wait times for different memory
accesses which usually would happen sequentially. A
simple calculation shows the minimum requirement on
the cache hit rate to achieve a certain speed-up.
The execution time for a program can be approximated
with a simple model with only one level of cache as fol-
lows (see [16]):
Texe
 =
 N 
(1 − Fmem )Tproc
+Fmem (Ghit Tcache + (1 − Ghit )Tmiss )

Ulrich Drepper
 VersionThe meaning of the variables is as follows:
N
Fmem
Ghit
Tproc
Tcache
Tmiss
Texe
=======Number of instructions.
Fraction of N that access memory.
Fraction of loads that hit the cache.
Number of cycles per instruction.
Number of cycles for cache hit.
Number of cycles for cache miss.
Execution time for program.
For it to make any sense to use two threads the execution
time of each of the two threads must be at most half of
that of the single-threaded code. The only variable on
either side is the number of cache hits. If we solve the
equation for the minimum cache hit rate required to not
slow down the thread execution by 50% or more we get
the graph in Figure 3.23.
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
60%
 70%
 80%
 90%
 100%
Figure 3.23: Minimum Cache Hit Rate For Speed-Up
The input, measured on the X–axis, is the cache hit rate
Ghit of the single-thread code. The Y–axis shows the
cache hit rate for the multi-threaded code. This value can
never be higher than the single-threaded hit rate since,
otherwise, the single-threaded code would use that im-
proved code, too. For single-threaded hit rates–in this
specific case–below 55% the program can in all cases
benefit from using threads. The CPU is more or less idle
enough due to cache misses to enable running a second
hyper-thread.
The green area is the target. If the slowdown for the
thread is less than 50% and the workload of each thread
is halved the combined runtime might be less than the
single-thread runtime. For the modeled processor (num-
bers for a P4 with hyper-threads were used) a program
1.0
 29
with a hit rate of 60% for the single-threaded code re-
quires a hit rate of at least 10% for the dual-threaded pro-
gram. That is usually doable. But if the single-threaded
code has a hit rate of 95% then the multi-threaded code
needs a hit rate of at least 80%. That is harder. Espe-
cially, and this is the problem with hyper-threads, be-
cause now the effective cache size (L1d here, in practice
also L2 and so on) available to each hyper-thread is cut
in half. Both hyper-threads use the same cache to load
their data. If the working set of the two threads is non-
overlapping the original 95% hit rate could also be cut in
half and is therefore much lower than the required 80%.
Hyper-threads are therefore only useful in a limited range
of situations. The cache hit rate of the single-threaded
code must be low enough that given the equations above
and reduced cache size the new hit rate still meets the
goal. Then and only then can it make any sense at all to
use hyper-threads. Whether the result is faster in prac-
tice depends on whether the processor is sufficiently able
to overlap the wait times in one thread with execution
times in the other threads. The overhead of parallelizing
the code must be added to the new total runtime and this
additional cost often cannot be neglected.
In section 6.3.4 we will see a technique where threads
collaborate closely and the tight coupling through the
common cache is actually an advantage. This technique
can be applicable to many situations if only the program-
mers are willing to put in the time and energy to extend
their code.
What should be clear is that if the two hyper-threads ex-
ecute completely different code (i.e., the two threads are
treated like separate processors by the OS to execute sep-
arate processes) the cache size is indeed cut in half which
means a significant increase in cache misses. Such OS
scheduling practices are questionable unless the caches
are sufficiently large. Unless the workload for the ma-
chine consists of processes which, through their design,
can indeed benefit from hyper-threads it might be best to
turn off hyper-threads in the computer’s BIOS.23
3.3.5
 Other Details
So far we talked about the address as consisting of three
parts, tag, set index, and cache line offset. But what ad-
dress is actually used? All relevant processors today pro-
vide virtual address spaces to processes, which means
that there are two different kinds of addresses: virtual
and physical.
The problem with virtual addresses is that they are not
unique. A virtual address can, over time, refer to dif-
ferent physical memory addresses. The same address in
different processes also likely refers to different physi-
cal addresses. So it is always better to use the physical
memory address, right?
23Another reason to keep hyper-threads enabled is debugging. SMT
is astonishingly good at finding some sets of problems in parallel code.
The problem here are the virtual addresses used during
execution which must to be translated with the help of
the Memory Management Unit (MMU) into physical ad-
dresses. This is a non-trivial operation. In the pipeline to
execute an instruction the physical address might only be
available at a later stage. This means that the cache logic
has to be very quick in determining whether the memory
location is cached. If virtual addresses could be used the
cache lookup can happen much earlier in the pipeline and
in case of a cache hit the memory content can be made
available. The result is that more of the memory access
costs could be hidden by the pipeline.
Processor designers are currently using virtual address
tagging for the first level caches. These caches are rather
small and can be cleared without too much pain. At
least partial clearing the cache is necessary if the page
table tree of a process changes. It might be possible to
avoid a complete flush if the processor has an instruc-
tion which allows to specify the virtual address range
which has changed. Given the low latency of L1i and
L1d caches (∼ 3 cycles) using virtual addresses is almost
mandatory.
For larger caches including L2, L3, . . . caches physical
address tagging is needed. These caches have a higher
latency and the virtual→physical address translation can
finish in time. Because these caches are larger (i.e., a lot
of information is lost when they are flushed) and refilling
them takes a long time due to the main memory access
latency, flushing them often would be costly.
It should, in general, not be necessary to know about the
details of the address handling in those caches. They can-
not be changed and all the factors which would influence
the performance are normally something which should
be avoided or is associated with high cost. Overflowing
the cache capacity is bad and all caches run into prob-
lems early if the majority of the used cache lines fall into
the same set. The latter can be avoided with virtually ad-
dressed caches but is impossible for user-level processes
to avoid for caches addressed using physical addresses.
The only detail one might want to keep in mind is to not
map the same physical memory location to two or more
virtual addresses in the same process, if at all possible.
Another detail of the caches which is rather uninterest-
ing to programmers is the cache replacement strategy.
Most caches evict the Least Recently Used (LRU) ele-
ment first. This is always a good default strategy. With
larger associativity (and associativity might indeed grow
further in the coming years due to the addition of more
cores) maintaining the LRU list becomes more and more
expensive and we might see different strategies adopted.
As for the cache replacement there is not much a pro-
grammer can do. If the cache is using physical address
tags there is no way to find out how the virtual addresses
correlate with the cache sets. It might be that cache lines
in all logical pages are mapped to the same cache sets,
30
 Version 1.0
 What Every Programmer Should Know About Memory
leaving much of the cache unused. If anything, it is the
job of the OS to arrange that this does not happen too
often.
With the advent of virtualization things get even more
complicated. Now not even the OS has control over the
assignment of physical memory. The Virtual Machine
Monitor (VMM, aka Hypervisor) is responsible for the
physical memory assignment.
The best a programmer can do is to a) use logical mem-
ory pages completely and b) use page sizes as large as
meaningful to diversify the physical addresses as much
as possible. Larger page sizes have other benefits, too,
but this is another topic (see section 4).
3.4
 Instruction Cache
Not just the data used by the processor is cached; the
instructions executed by the processor are also cached.
However, this cache is much less problematic than the
data cache. There are several reasons:
• The quantity of code which is executed depends on
the size of the code that is needed. The size of the
code in general depends on the complexity of the
problem. The complexity of the problem is fixed.
• While the program’s data handling is designed by
the programmer the program’s instructions are usu-
ally generated by a compiler. The compiler writers
know about the rules for good code generation.
• Program flow is much more predictable than data
access patterns. Today’s CPUs are very good at
detecting patterns. This helps with prefetching.
• Code always has quite good spatial and temporal
locality.
There are a few rules programmers should follow but
these mainly consist of rules on how to use the tools. We
will discuss them in section 6. Here we talk only about
the technical details of the instruction cache.
Ever since the core clock of CPUs increased dramati-
cally and the difference in speed between cache (even
first level cache) and core grew, CPUs have been de-
signed with pipelines. That means the execution of an
instruction happens in stages. First an instruction is de-
coded, then the parameters are prepared, and finally it
is executed. Such a pipeline can be quite long (> 20
stages for Intel’s Netburst architecture). A long pipeline
means that if the pipeline stalls (i.e., the instruction flow
through it is interrupted) it takes a while to get up to
speed again. Pipeline stalls happen, for instance, if the
location of the next instruction cannot be correctly pre-
dicted or if it takes too long to load the next instruction
(e.g., when it has to be read from memory).
As a result CPU designers spend a lot of time and chip
real estate on branch prediction so that pipeline stalls
happen as infrequently as possible.
On CISC processors the decoding stage can also take
some time. The x86 and x86-64 processors are espe-
cially affected. In recent years these processors therefore
do not cache the raw byte sequence of the instructions in
L1i but instead they cache the decoded instructions. L1i
in this case is called the “trace cache”. Trace caching
allows the processor to skip over the first steps of the
pipeline in case of a cache hit which is especially good if
the pipeline stalled.
As said before, the caches from L2 on are unified caches
which contain both code and data. Obviously here the
code is cached in the byte sequence form and not de-
coded.
To achieve the best performance there are only a few
rules related to the instruction cache:
1. Generate code which is as small as possible. There
are exceptions when software pipelining for the
sake of using pipelines requires creating more code
or where the overhead of using small code is too
high.
2. Help the processor making good prefetching de-
cisions. This can be done through code layout or
with explicit prefetching.
These rules are usually enforced by the code generation
of a compiler. There are a few things the programmer
can do and we will talk about them in section 6.
3.4.1
 Self Modifying Code
In early computer days memory was a premium. People
went to great lengths to reduce the size of the program
to make more room for program data. One trick fre-
quently deployed was to change the program itself over
time. Such Self Modifying Code (SMC) is occasionally
still found, these days mostly for performance reasons or
in security exploits.
SMC should in general be avoided. Though it is gener-
ally correctly executed there are boundary cases which
are not and it creates performance problems if not done
correctly. Obviously, code which is changed cannot be
kept in the trace cache which contains the decoded in-
structions. But even if the trace cache is not used because
the code has not been executed at all (or for some time)
the processor might have problems. If an upcoming in-
struction is changed while it already entered the pipeline
the processor has to throw away a lot of work and start
all over again. There are even situations where most of
the state of the processor has to be tossed away.
Finally, since the processor assumes–for simplicity rea-
sons and because it is true in 99.9999999% of all cases–
Ulrich Drepper
 Version 1.0
 31
that the code pages are immutable, the L1i implementa-
tion does not use the MESI protocol but instead a simpli-
fied SI protocol. This means if modifications are detected
a lot of pessimistic assumptions have to be made.
It is highly advised to avoid SMC whenever possible.
Memory is not such a scarce resource anymore. It is
better to write separate functions instead of modifying
one function according to specific needs. Maybe one day
SMC support can be made optional and we can detect
exploit code trying to modify code this way. If SMC ab-
solutely has to be used, the write operations should by-
pass the cache as to not create problems with data in L1d
needed in L1i. See section 6.1 for more information on
these instructions.
On Linux it is normally quite easy to recognize programs
which contain SMC. All program code is write-protected
when built with the regular toolchain. The programmer
has to perform significant magic at link time to create
an executable where the code pages are writable. When
this happens, modern Intel x86 and x86-64 processors
have dedicated performance counters which count uses
of self-modifying code. With the help of these counters it
is quite easily possible to recognize programs with SMC
even if the program will succeed due to relaxed permis-
sions.
3.5
 Cache Miss Factors
We have already seen that when memory accesses miss
the caches the costs skyrocket. Sometimes this is not
avoidable and it is important to understand the actual
costs and what can be done to mitigate the problem.
3.5.1
 Cache and Memory Bandwidth
To get a better understanding of the capabilities of the
processors we measure the bandwidth available in opti-
mal circumstances. This measurement is especially in-
teresting since different processor versions vary widely.
This is why this section is filled with the data of sev-
eral different machines. The program to measure perfor-
mance uses the SSE instructions of the x86 and x86-64
processors to load or store 16 bytes at once. The working
set is increased from 1kB to 512MB just as in our other
tests and it is measured how many bytes per cycle can be
loaded or stored.
Figure 3.24 shows the performance on a 64-bit Intel Net-
burst processor. For working set sizes which fit into L1d
the processor is able to read the full 16 bytes per cy-
cle, i.e., one load instruction is performed per cycle (the
movaps instruction moves 16 bytes at once). The test
does not do anything with the read data, we test only the
read instructions themselves. As soon as the L1d is not
sufficient anymore the performance goes down dramati-
cally to less than 6 bytes per cycle. The step at 218 bytes
is due to the exhaustion of the DTLB cache which means
additional work for each new page. Since the reading
32
 Version 1.0
16
14
12
s
 e 10
lcy 8
C/s e 6
tyB 4
2
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Read
 Write
 Copy
Figure 3.24: Pentium 4 Bandwidth
16
14
12
s
elcyC/setyB10
8
6
4
2
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Read
 Write
 Copy
Figure 3.25: P4 Bandwidth with 2 Hyper-Threads
16
14
12
s
elcyC/setyB10
8
6
4
2
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Read
 Write
 Copy
Figure 3.26: Core 2 Bandwidth
What Every Programmer Should Know About Memory
is sequential prefetching can predict the accesses per-
fectly and the FSB can stream the memory content at
about 5.3 bytes per cycle for all sizes of the working set.
The prefetched data is not propagated into L1d, though.
These are of course numbers which will never be achiev-
able in a real program. Think of them as practical limits.
What is more astonishing than the read performance is
the write and copy performance. The write performance,
even for small working set sizes, does not ever rise above
4 bytes per cycle. This indicates that, in these Netburst
processors, Intel elected to use a Write-Through mode
for L1d where the performance is obviously limited by
the L2 speed. This also means that the performance of
the copy test, which copies from one memory region into
a second, non-overlapping memory region, is not signifi-
cantly worse. The necessary read operations are so much
faster and can partially overlap with the write operations.
The most noteworthy detail of the write and copy mea-
surements is the low performance once the L2 cache is
not sufficient anymore. The performance drops to 0.5
bytes per cycle! That means write operations are by a
factor of ten slower than the read operations. This means
optimizing those operations is even more important for
the performance of the program.
In Figure 3.25 we see the results on the same processor
but with two threads running, one pinned to each of the
two hyper-threads of the processor. The graph is shown
at the same scale as the previous one to illustrate the dif-
ferences and the curves are a bit jittery simply because of
the problem of measuring two concurrent threads. The
results are as expected. Since the hyper-threads share all
the resources except the registers each thread has only
half the cache and bandwidth available. That means even
though each thread has to wait a lot and could award
the other thread with execution time this does not make
any difference since the other thread also has to wait for
the memory. This truly shows the worst possible use of
hyper-threads.
Compared to Figure 3.24 and 3.25 the results in Fig-
ure 3.26 and 3.27 look quite different for an Intel Core 2
processor. This is a dual-core processor with shared L2
which is four times as big as the L2 on the P4 machine.
This only explains the delayed drop-off of the write and
copy performance, though.
There are other, bigger differences. The read perfor-
mance throughout the working set range hovers around
the optimal 16 bytes per cycle. The drop-off in the read
performance after 220 bytes is again due to the working
set being too big for the DTLB. Achieving these high
numbers means the processor is not only able to prefetch
the data and transport the data in time. It also means the
data is prefetched into L1d.
The write and copy performance is dramatically differ-
ent, too. The processor does not have a Write-Through
policy; written data is stored in L1d and only evicted
when necessary. This allows for write speeds close to the
optimal 16 bytes per cycle. Once L1d is not sufficient
anymore the performance drops significantly. As with
the Netburst processor, the write performance is signifi-
cantly lower. Due to the high read performance the dif-
ference is even higher here. In fact, when even the L2 is
not sufficient anymore the speed difference increases to
a factor of 20! This does not mean the Core 2 proces-
sors perform poorly. To the contrary, their performance
is always better than the Netburst core’s.
In Figure 3.27 the test runs two threads, one on each of
the two cores of the Core 2 processor. Both threads ac-
cess the same memory, not necessarily perfectly in sync,
though. The results for the read performance are not dif-
ferent from the single-threaded case. A few more jitters
are visible which is to be expected in any multi-threaded
test case.
The interesting point is the write and copy performance
for working set sizes which would fit into L1d. As can be
seen in the figure, the performance is the same as if the
data had to be read from the main memory. Both threads
compete for the same memory location and RFO mes-
sages for the cache lines have to be sent. The problematic
point is that these requests are not handled at the speed
of the L2 cache, even though both cores share the cache.
Once the L1d cache is not sufficient anymore modified
entries are flushed from each core’s L1d into the shared
L2. At that point the performance increases significantly
since now the L1d misses are satisfied by the L2 cache
and RFO messages are only needed when the data has
not yet been flushed. This is why we see a 50% reduction
in speed for these sizes of the working set. The asymp-
totic behavior is as expected: since both cores share the
same FSB each core gets half the FSB bandwidth which
means for large working sets each thread’s performance
is about half that of the single threaded case.
Because there are significant differences even between
the processor versions of one vendor it is certainly worth-
while looking at the performance of other vendors’ pro-
cessors, too. Figure 3.28 shows the performance of an
AMD family 10h Opteron processor. This processor has
64kB L1d, 512kB L2, and 2MB of L3. The L3 cache is
shared between all cores of the processor. The results of
the performance test can be seen in Figure 3.28.
The first detail one notices about the numbers is that the
processor is capable of handling two instructions per cy-
cle if the L1d cache is sufficient. The read performance
exceeds 32 bytes per cycle and even the write perfor-
mance is, with 18.7 bytes per cycle, high. The read curve
flattens quickly, though, and is, with 2.3 bytes per cycle,
pretty low. The processor for this test does not prefetch
any data, at least not efficiently.
The write curve on the other hand performs according to
the sizes of the various caches. The peak performance
is achieved for the full size of the L1d, going down to 6
bytes per cycle for L2, to 2.8 bytes per cycle for L3, and
finally .5 bytes per cycle if not even L3 can hold all the
Ulrich Drepper
 Version 1.0
 33
16
14
12
s
elcyC/setyB10
8
6
4
2
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Read
 Write
 Copy
Figure 3.27: Core 2 Bandwidth with 2 Threads
36
32
28
s
 24
el c 20
yC / 16
se t 12
yB8
4
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Read
 Write
 Copy
Figure 3.28: AMD Family 10h Opteron Bandwidth
28
24
s
elcyC/setyB20
16
12
8
4
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Read
 Write
 Copy
Figure 3.29: AMD Fam 10h Bandwidth with 2 Threads
34
 Version 1.0
data. The performance for the L1d cache exceeds that of
the (older) Core 2 processor, the L2 access is equally fast
(with the Core 2 having a larger cache), and the L3 and
main memory access is slower.
The copy performance cannot be better than either the
read or write performance. This is why we see the curve
initially dominated by the read performance and later by
the write performance.
The multi-thread performance of the Opteron processor
is shown in Figure 3.29. The read performance is largely
unaffected. Each thread’s L1d and L2 works as before
and the L3 cache is in this case not prefetched very well
either. The two threads do not unduly stress the L3 for
their purpose. The big problem in this test is the write
performance. All data the threads share has to go through
the L3 cache. This sharing seems to be quite inefficient
since even if the L3 cache size is sufficient to hold the
entire working set the cost is significantly higher than an
L3 access. Comparing this graph with Figure 3.27 we see
that the two threads of the Core 2 processor operate at the
speed of the shared L2 cache for the appropriate range of
working set sizes. This level of performance is achieved
for the Opteron processor only for a very small range of
the working set sizes and even here it approaches only
the speed of the L3 which is slower than the Core 2’s L2.
3.5.2
 Critical Word Load
Memory is transferred from the main memory into the
caches in blocks which are smaller than the cache line
size. Today 64 bits are transferred at once and the cache
line size is 64 or 128 bytes. This means 8 or 16 transfers
per cache line are needed.
The DRAM chips can transfer those 64-byte blocks in
burst mode. This can fill the cache line without any fur-
ther commands from the memory controller and the pos-
sibly associated delays. If the processor prefetches cache
lines this is probably the best way to operate.
If a program’s cache access of the data or instruction
caches misses (that means, it is a compulsory cache miss,
because the data is used for the first time, or a capacity
cache miss, because the limited cache size requires evic-
tion of the cache line) the situation is different. The word
inside the cache line which is required for the program
to continue might not be the first word in the cache line.
Even in burst mode and with double data rate transfer
the individual 64-bit blocks arrive at noticeably different
times. Each block arrives 4 CPU cycles or more later
than the previous one. If the word the program needs to
continue is the eighth of the cache line the program has to
wait an additional 30 cycles or more after the first word
arrives.
Things do not necessarily have to be like this. The mem-
ory controller is free to request the words of the cache
line in a different order. The processor can communicate
which word the program is waiting on, the critical word,
What Every Programmer Should Know About Memory
and the memory controller can request this word first.
Once the word arrives the program can continue while
the rest of the cache line arrives and the cache is not yet in
a consistent state. This technique is called Critical Word
First & Early Restart.
Processors nowadays implement this technique but there
are situations when that is not possible. If the processor
prefetches data the critical word is not known. Should
the processor request the cache line during the time the
prefetch operation is in flight it will have to wait until the
critical word arrives without being able to influence the
order.
t
rat 0%
Sta−0.2%
droW −0.4%
lac i t −0.6%
irC−0.8%
sVn −1%
wod w −1.2%
ol S 210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Sequential
 Random
Figure 3.30: Critical Word at End of Cache Line
Even with these optimizations in place the position of the
critical word on a cache line matters. Figure 3.30 shows
the Follow test for sequential and random access. Shown
is the slowdown of running the test with the pointer used
in the chase in the first word versus the case when the
pointer is in the last word. The element size is 64 bytes,
corresponding the cache line size. The numbers are quite
noisy but it can be seen that, as soon as the L2 is not suffi-
cient to hold the working set size, the performance of the
case where the critical word is at the end is about 0.7%
slower. The sequential access appears to be affected a bit
more. This would be consistent with the aforementioned
problem when prefetching the next cache line.
3.5.3
 Cache Placement
Where the caches are placed in relationship to the hyper-
threads, cores, and processors is not under control of the
programmer. But programmers can determine where the
threads are executed and then it becomes important how
the caches relate to the used CPUs.
Here we will not go into details of when to select what
cores to run the threads. We will only describe architec-
ture details which the programmer has to take into ac-
count when setting the affinity of the threads.
Hyper-threads, by definition share everything but the reg-
ister set. This includes the L1 caches. There is not much
more to say here. The fun starts with the individual cores
of a processor. Each core has at least its own L1 caches.
Aside from this there are today not many details in com-
mon:
• Early multi-core processors had no shared caches
at all.
• Later Intel models have shared L2 caches for dual-
core processors. For quad-core processors we have
to deal with separate L2 caches for each pair of two
cores. There are no higher level caches.
• AMD’s family 10h processors have separate L2
caches and a unified L3 cache.
A lot has been written in the propaganda material of the
processor vendors about the advantage of their respec-
tive models. Having no shared cache has an advantage
if the working sets handled by the cores do not over-
lap. This works well for single-threaded programs. Since
this is still often the reality today this approach does not
perform too badly. But there is always some overlap.
The caches all contain the most actively used parts of
the common runtime libraries which means some cache
space is wasted.
Completely sharing all caches beside L1 as Intel’s dual-
core processors do can have a big advantage. If the work-
ing set of the threads working on the two cores over-
laps significantly the total available cache memory is in-
creased and working sets can be bigger without perfor-
mance degradation. If the working sets do not overlap In-
tel’s Advanced Smart Cache management is supposed to
prevent any one core from monopolizing the entire cache.
If both cores use about half the cache for their respective
working sets there is some friction, though. The cache
constantly has to weigh the two cores’ cache use and the
evictions performed as part of this rebalancing might be
chosen poorly. To see the problems we look at the results
of yet another test program.
The test program has one process constantly reading or
writing, using SSE instructions, a 2MB block of mem-
ory. 2MB was chosen because this is half the size of the
L2 cache of this Core 2 processor. The process is pinned
to one core while a second process is pinned to the other
core. This second process reads and writes a memory
region of variable size. The graph shows the number of
bytes per cycle which are read or written. Four different
graphs are shown, one for each combination of the pro-
cesses reading and writing. The read/write graph is for
the background process, which always uses a 2MB work-
ing set to write, and the measured process with variable
working set to read.
The interesting part of the graph is the part between 220
and 223 bytes. If the L2 cache of the two cores were com-
pletely separate we could expect that the performance of
Ulrich Drepper
 Version 1.0
 35
16
14
12
e
 l 10
cyC 8
/se t 6
yB 4
2
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Read/Read
 Write/Read
 Read/Write
 Write/Write
Figure 3.31: Bandwidth with two Processes
all four tests would drop between 221 and 222 bytes, that
means, once the L2 cache is exhausted. As we can see
in Figure 3.31 this is not the case. For the cases where
the background process is writing this is most visible.
The performance starts to deteriorate before the working
set size reaches 1MB. The two processes do not share
memory and therefore the processes do not cause RFO
messages to be generated. These are pure cache eviction
problems. The smart cache handling has its problems
with the effect that the experienced cache size per core is
closer to 1MB than the 2MB per core which are available.
One can only hope that, if caches shared between cores
remain a feature of upcoming processors, the algorithm
used for the smart cache handling will be fixed.
Having a quad-core processor with two L2 caches was
just a stop-gap solution before higher-level caches could
be introduced. This design provides no significant per-
formance advantage over separate sockets and dual-core
processors. The two cores communicate via the same bus
which is, at the outside, visible as the FSB. There is no
special fast-track data exchange.
The future of cache design for multi-core processors will
lie in more layers. AMD’s 10h processor family makes
the start. Whether we will continue to see lower level
caches be shared by a subset of the cores of a proces-
sor remains to be seen (in the 2008 generation of proces-
sors L2 caches are not shared). The extra levels of cache
are necessary since the high-speed and frequently used
caches cannot be shared among many cores. The per-
formance would be impacted. It would also require very
large caches with high associativity. Both numbers, the
cache size and the associativity, must scale with the num-
ber of cores sharing the cache. Using a large L3 cache
and reasonably-sized L2 caches is a reasonable trade-off.
The L3 cache is slower but it is ideally not as frequently
used as the L2 cache.
For programmers all these different designs mean com-
plexity when making scheduling decisions. One has to
know the workloads and the details of the machine archi-
tecture to achieve the best performance. Fortunately we
have support to determine the machine architecture. The
interfaces will be introduced in later sections.
3.5.4
 FSB Influence
The FSB plays a central role in the performance of the
machine. Cache content can only be stored and loaded
as quickly as the connection to the memory allows. We
can show how much so by running a program on two
machines which only differ in the speed of their memory
modules. Figure 3.32 shows the results of the Addnext0
test (adding the content of the next elements pad[0] el-
ement to the own pad[0] element) for NPAD=7 on a 64-
bit machine. Both machines have Intel Core 2 proces-
sors, the first uses 667MHz DDR2 modules, the second
800MHz modules (a 20% increase).
175
t
nemelEtsiL/selcyC150
125
100
75
50
25
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Core2/667
 Core2/800
Figure 3.32: Influence of FSB Speed
The numbers show that, when the FSB is really stressed
for large working set sizes, we indeed see a large bene-
fit. The maximum performance increase measured in this
test is 18.2%, close to the theoretical maximum. What
this shows is that a faster FSB indeed can pay off big
time. It is not critical when the working set fits into
the caches (and these processors have a 4MB L2). It
must be kept in mind that we are measuring one program
here. The working set of a system comprises the memory
needed by all concurrently running processes. This way
it is easily possible to exceed 4MB memory or more with
much smaller programs.
Today some of Intel’s processors support FSB speeds
up to 1,333MHz which would mean another 60% in-
crease. The future is going to see even higher speeds.
If speed is important and the working set sizes are larger,
fast RAM and high FSB speeds are certainly worth the
money. One has to be careful, though, since even though
the processor might support higher FSB speeds the moth-
erboard/Northbridge might not. It is critical to check the
specifications.
36
 Version 1.0
 What Every Programmer Should Know About Memory
4
 Virtual Memory
The virtual memory (VM) subsystem of a processor im-
plements the virtual address spaces provided to each pro-
cess. This makes each process think it is alone in the
system. The list of advantages of virtual memory are de-
scribed in detail elsewhere so they will not be repeated
here. Instead this section concentrates on the actual im-
plementation details of the virtual memory subsystem
and the associated costs.
A virtual address space is implemented by the Memory
Management Unit (MMU) of the CPU. The OS has to
fill out the page table data structures, but most CPUs do
the rest of the work themselves. This is actually a pretty
complicated mechanism; the best way to understand it
is to introduce the data structures used to describe the
virtual address space.
The input to the address translation performed by the
MMU is a virtual address. There are usually few–if any–
restrictions on its value. Virtual addresses are 32-bit val-
ues on 32-bit systems, and 64-bit values on 64-bit sys-
tems. On some systems, for instance x86 and x86-64,
the addresses used actually involve another level of indi-
rection: these architectures use segments which simply
cause an offset to be added to every logical address. We
can ignore this part of address generation, it is trivial and
not something that programmers have to care about with
respect to performance of memory handling.24
4.1
 Simplest Address Translation
The interesting part is the translation of the virtual ad-
dress to a physical address. The MMU can remap ad-
dresses on a page-by-page basis. Just as when addressing
cache lines, the virtual address is split into distinct parts.
These parts are used to index into various tables which
are used in the construction of the final physical address.
For the simplest model we have only one level of tables.
Directory
PageVirtual Address
Offset
Directory
Physical Page
Physical Address
Directory Entry
Figure 4.1: 1-Level Address Translation
Figure 4.1 shows how the different parts of the virtual
address are used. A top part is used to select an entry
in a Page Directory; each entry in that directory can be
24Segment limits on x86 are performance-relevant but that is another
story.
individually set by the OS. The page directory entry de-
termines the address of a physical memory page; more
than one entry in the page directory can point to the same
physical address. The complete physical address of the
memory cell is determined by combining the page ad-
dress from the page directory with the low bits from the
virtual address. The page directory entry also contains
some additional information about the page such as ac-
cess permissions.
The data structure for the page directory is stored in main
memory. The OS has to allocate contiguous physical
memory and store the base address of this memory re-
gion in a special register. The appropriate bits of the
virtual address are then used as an index into the page
directory, which is actually an array of directory entries.
For a concrete example, this is the layout used for 4MB
pages on x86 machines. The Offset part of the virtual
address is 22 bits in size, enough to address every byte in
a 4MB page. The remaining 10 bits of the virtual address
select one of the 1024 entries in the page directory. Each
entry contains a 10 bit base address of a 4MB page which
is combined with the offset to form a complete 32 bit
address.
4.2
 Multi-Level Page Tables
4MB pages are not the norm, they would waste a lot of
memory since many operations an OS has to perform re-
quire alignment to memory pages. With 4kB pages (the
norm on 32-bit machines and, still, often on 64-bit ma-
chines), the Offset part of the virtual address is only 12
bits in size. This leaves 20 bits as the selector of the
page directory. A table with 220 entries is not practical.
Even if each entry would be only 4 bytes the table would
be 4MB in size. With each process potentially having its
own distinct page directory much of the physical memory
of the system would be tied up for these page directories.
The solution is to use multiple levels of page tables. The
level then form a huge, sparse page directory; address
space regions which are not actually used do not require
allocated memory. The representation is therefore much
more compact, making it possible to have the page tables
for many processes in memory without impacting perfor-
mance too much.
Today the most complicated page table structures com-
prise four levels. Figure 4.2 shows the schematics of such
an implementation. The virtual address is, in this exam-
ple, split into at least five parts. Four of these parts are
indexes into the various directories. The level 4 directory
is referenced using a special-purpose register in the CPU.
The content of the level 4 to level 2 directories is a ref-
erence to next lower level directory. If a directory entry
is marked empty it obviously need not point to any lower
directory. This way the page table tree can be sparse and
compact. The entries of the level 1 directory are, just like
in Figure 4.1, partial physical addresses, plus auxiliary
data like access permissions.
Ulrich Drepper
 Version 1.0
 37
Level 4 Index
Level 3 Index
Virtual Address
Level 2 Index
Level 1 Index
Offset
Level 4 Directory
Level 4 Entry
Level 3 Directory
Level 3 Entry
Level 2 Directory
Level 2 Entry
Level 1 Directory
Level 1 Entry
Physical Page
Physical Address
Figure 4.2: 4-Level Address Translation
To determine the physical address corresponding to a vir-
tual address the processor first determines the address of
the highest level directory. This address is usually stored
in a register. Then the CPU takes the index part of the
virtual address corresponding to this directory and uses
that index to pick the appropriate entry. This entry is the
address of the next directory, which is indexed using the
next part of the virtual address. This process continues
until it reaches the level 1 directory, at which point the
value of the directory entry is the high part of the physi-
cal address. The physical address is completed by adding
the page offset bits from the virtual address. This process
is called page tree walking. Some processors (like x86
and x86-64) perform this operation in hardware, others
need assistance from the OS.
Each process running on the system might need its own
page table tree. It is possible to partially share trees but
this is rather the exception. It is therefore good for per-
formance and scalability if the memory needed by the
page table trees is as small as possible. The ideal case
for this is to place the used memory close together in the
virtual address space; the actual physical addresses used
do not matter. A small program might get by with using
just one directory at each of levels 2, 3, and 4 and a few
level 1 directories. On x86-64 with 4kB pages and 512
entries per directory this allows the addressing of 2MB
with a total of 4 directories (one for each level). 1GB of
contiguous memory can be addressed with one directory
for levels 2 to 4 and 512 directories for level 1.
Assuming all memory can be allocated contiguously is
too simplistic, though. For flexibility reasons the stack
and the heap area of a process are, in most cases, allo-
cated at pretty much opposite ends of the address space.
This allows either area to grow as much as possible if
needed. This means that there are most likely two level 2
directories needed and correspondingly more lower level
directories.
But even this does not always match current practice. For
security reasons the various parts of an executable (code,
data, heap, stack, Dynamic Shared Objects (DSOs), aka
shared libraries) are mapped at randomized addresses [9].
The randomization extends to the relative position of the
various parts; that implies that the various memory re-
gions in use in a process are widespread throughout the
virtual address space. By applying some limits to the
number of bits of the address which are randomized the
range can be restricted, but it certainly, in most cases, will
not allow a process to run with just one or two directories
for levels 2 and 3.
If performance is really much more important than se-
curity, randomization can be turned off. The OS will
then usually at least load all DSOs contiguously in vir-
tual memory.
4.3
 Optimizing Page Table Access
All the data structures for the page tables are kept in the
main memory; this is where the OS constructs and up-
dates the tables. Upon creation of a process or a change
of a page table the CPU is notified. The page tables are
used to resolve every virtual address into a physical ad-
dress using the page table walk described above. More to
the point: at least one directory for each level is used in
the process of resolving a virtual address. This requires
up to four memory accesses (for a single access by the
running process) which is slow. It is possible to treat
these directory table entries as normal data and cache
them in L1d, L2, etc., but this would still be far too slow.
From the earliest days of virtual memory, CPU designers
have used a different optimization. A simple computa-
tion can show that only keeping the directory table en-
tries in the L1d and higher cache would lead to horrible
performance. Each absolute address computation would
require a number of L1d accesses corresponding to the
page table depth. These accesses cannot be parallelized
since they depend on the previous lookup’s result. This
alone would, on a machine with four page table levels,
require at the very least 12 cycles. Add to that the proba-
bility of an L1d miss and the result is nothing the instruc-
tion pipeline can hide. The additional L1d accesses also
steal precious bandwidth to the cache.
38
 Version 1.0
 What Every Programmer Should Know About Memory
So, instead of just caching the directory table entries,
the complete computation of the address of the physi-
cal page is cached. For the same reason that code and
data caches work, such a cached address computation is
effective. Since the page offset part of the virtual address
does not play any part in the computation of the physi-
cal page address, only the rest of the virtual address is
used as the tag for the cache. Depending on the page size
this means hundreds or thousands of instructions or data
objects share the same tag and therefore same physical
address prefix.
The cache into which the computed values are stored is
called the Translation Look-Aside Buffer (TLB). It is
usually a small cache since it has to be extremely fast.
Modern CPUs provide multi-level TLB caches, just as
for the other caches; the higher-level caches are larger
and slower. The small size of the L1TLB is often made
up for by making the cache fully associative, with an
LRU eviction policy. Recently, this cache has been grow-
ing in size and, in the process, was changed to be set as-
sociative. As a result, it might not be the oldest entry
which gets evicted and replaced whenever a new entry
has to be added.
As noted above, the tag used to access the TLB is a part
of the virtual address. If the tag has a match in the cache,
the final physical address is computed by adding the page
offset from the virtual address to the cached value. This
is a very fast process; it has to be since the physical ad-
dress must be available for every instruction using abso-
lute addresses and, in some cases, for L2 look-ups which
use the physical address as the key. If the TLB lookup
misses the processor has to perform a page table walk;
this can be quite costly.
Prefetching code or data through software or hardware
could implicitly prefetch entries for the TLB if the ad-
dress is on another page. This cannot be allowed for
hardware prefetching because the hardware could initiate
page table walks that are invalid. Programmers therefore
cannot rely on hardware prefetching to prefetch TLB en-
tries. It has to be done explicitly using prefetch instruc-
tions. TLBs, just like data and instruction caches, can
appear in multiple levels. Just as for the data cache, the
TLB usually appears in two flavors: an instruction TLB
(ITLB) and a data TLB (DTLB). Higher-level TLBs such
as the L2TLB are usually unified, as is the case with the
other caches.
4.3.1
 Caveats Of Using A TLB
The TLB is a processor-core global resource. All threads
and processes executed on the processor core use the
same TLB. Since the translation of virtual to physical ad-
dresses depends on which page table tree is installed, the
CPU cannot blindly reuse the cached entries if the page
table is changed. Each process has a different page ta-
ble tree (but not the threads in the same process) as does
the kernel and the VMM (hypervisor) if present. It is
also possible that the address space layout of a process
changes. There are two ways to deal with this problem:
• The TLB is flushed whenever the page table tree is
changed.
• The tags for the TLB entries are extended to ad-
ditionally and uniquely identify the page table tree
they refer to.
In the first case the TLB is flushed whenever a context
switch is performed. Since, in most OSes, a switch from
one thread/process to another requires executing some
kernel code, TLB flushes are restricted to leaving (and
sometimes entering) the kernel address space. On vir-
tualized systems it also happens when the kernel has to
call the VMM and on the way back. If the kernel and/or
VMM does not have to use virtual addresses, or can reuse
the same virtual addresses as the process or kernel which
made the system/VMM call (i.e., the address spaces are
overlaid), the TLB only has to be flushed if, upon leaving
the kernel or VMM, the processor resumes execution of
a different process or kernel.
Flushing the TLB is effective but expensive. When exe-
cuting a system call, for instance, the kernel code might
be restricted to a few thousand instructions which touch,
perhaps, a handful of new pages (or one huge page, as
is the case for Linux on some architectures). This work
would replace only as many TLB entries as pages are
touched. For Intel’s Core2 architecture with its 128 ITLB
and 256 DTLB entries, a full flush would mean that more
than 100 and 200 entries (respectively) would be flushed
unnecessarily. When the system call returns to the same
process, all those flushed TLB entries can be used again,
but they will be gone. The same is true for often-used
code in the kernel or VMM. On each entry into the ker-
nel the TLB has to be filled from scratch even though
the page tables for the kernel and VMM usually do not
change and, therefore, TLB entries could, in theory, be
preserved for a very long time. This also explains why
the TLB caches in today’s processors are not bigger: pro-
grams most likely will not run long enough to fill all these
entries.
This fact, of course, did not escape the CPU architects.
One possibility to optimize the cache flushes is to indi-
vidually invalidate TLB entries. For instance, if the ker-
nel code and data falls into a specific address range, only
the pages falling into this address range have to evicted
from the TLB. This only requires comparing tags and,
therefore, is not very expensive. This method is also use-
ful in case a part of the address space is changed, for
instance, through a call to munmap.
A much better solution is to extend the tag used for the
TLB access. If, in addition to the part of the virtual ad-
dress, a unique identifier for each page table tree (i.e., a
process’s address space) is added, the TLB does not have
to be completely flushed at all. The kernel, VMM, and
Ulrich Drepper
 Version 1.0
 39
the individual processes all can have unique identifiers.
The only issue with this scheme is that the number of
bits available for the TLB tag is severely limited, while
the number of address spaces is not. This means some
identifier reuse is necessary. When this happens the TLB
has to be partially flushed (if this is possible). All en-
tries with the reused identifier must be flushed but this is,
hopefully, a much smaller set.
This extended TLB tagging is of advantage outside the
realm of virtualization when multiple processes are run-
ning on the system. If the memory use (and hence TLB
entry use) of each of the runnable processes is limited,
there is a good chance the most recently used TLB entries
for a process are still in the TLB when it gets scheduled
again. But there are two additional advantages:
1. Special address spaces, such as those used by the
kernel and VMM, are often only entered for a short
time; afterward control is often returned to the ad-
dress space which initiated the entry. Without tags,
one or two TLB flushes are performed. With tags
the calling address space’s cached translations are
preserved and, since the kernel and VMM address
space do not often change TLB entries at all, the
translations from previous system calls, etc. can
still be used.
2. When switching between two threads of the same
process no TLB flush is necessary at all. With-
out extended TLB tags the entry into the kernel
destroys the first thread’s TLB entries, though.
Some processors have, for some time, implemented these
extended tags. AMD introduced a 1-bit tag extension
with the Pacifica virtualization extensions. This 1-bit Ad-
dress Space ID (ASID) is, in the context of virtualization,
used to distinguish the VMM’s address space from that of
the guest domains. This allows the OS to avoid flushing
the guest’s TLB entries every time the VMM is entered
(for instance, to handle a page fault) or the VMM’s TLB
entries when control returns to the guest. The architec-
ture will allow the use of more bits in the future. Other
mainstream processors will likely follow suit and support
this feature.
4.3.2
 Influencing TLB Performance
There are a couple of factors which influence TLB per-
formance. The first is the size of the pages. Obviously,
the larger a page is, the more instructions or data objects
will fit into it. So a larger page size reduces the overall
number of address translations which are needed, mean-
ing that fewer entries in the TLB cache are needed. Most
architectures nowadays allow the use of multiple differ-
ent page sizes; some sizes can be used concurrently. For
instance, the x86/x86-64 processors have a normal page
size of 4kB but they can also use 4MB and 2MB pages
respectively. IA-64 and PowerPC allow sizes like 64kB
as the base page size.
The use of large page sizes brings some problems with
it, though. The memory regions used for the large pages
must be contiguous in physical memory. If the unit size
for the administration of physical memory is raised to the
size of the virtual memory pages, the amount of wasted
memory will grow. All kinds of memory operations (like
loading executables) require alignment to page bound-
aries. This means, on average, that each mapping wastes
half the page size in physical memory for each mapping.
This waste can easily add up; it thus puts an upper limit
on the reasonable unit size for physical memory alloca-
tion.
It is certainly not practical to increase the unit size to
2MB to accommodate large pages on x86-64. This is
just too large a size. But this in turn means that each
large page has to be comprised of many smaller pages.
And these small pages have to be contiguous in physical
memory. Allocating 2MB of contiguous physical mem-
ory with a unit page size of 4kB can be challenging. It
requires finding a free area with 512 contiguous pages.
This can be extremely difficult (or impossible) after the
system runs for a while and physical memory becomes
fragmented.
On Linux it is therefore necessary to allocate these big
pages at system start time using the special hugetlbfs
filesystem. A fixed number of physical pages are re-
served for exclusive use as big virtual pages. This ties
down resources which might not always be used. It also
is a limited pool; increasing it normally means restart-
ing the system. Still, huge pages are the way to go in
situations where performance is a premium, resources
are plenty, and cumbersome setup is not a big deterrent.
Database servers are an example.
Increasing the minimum virtual page size (as opposed to
optional big pages) has its problems, too. Memory map-
ping operations (loading applications, for example) must
conform to these page sizes. No smaller mappings are
possible. The location of the various parts of an exe-
cutable have, for most architectures, a fixed relationship.
If the page size is increased beyond what has been taken
into account when the executable or DSO was built, the
load operation cannot be performed. It is important to
keep this limitation in mind. Figure 4.3 shows how the
alignment requirements of an ELF binary can be deter-
mined. It is encoded in the ELF program header. In
this example, an x86-64 binary, the value is 20000016 =
2, 097, 152 = 2MB which corresponds to the maximum
page size supported by the processor.
There is a second effect of using larger page sizes: the
number of levels of the page table tree is reduced. Since
the part of the virtual address corresponding to the page
offset increases, there are not that many bits left which
need to be handled through page directories. This means
that, in case of a TLB miss, the amount of work which
has to be done is reduced.
Beyond using large page sizes, it is possible to reduce the
40
 Version 1.0
 What Every Programmer Should Know About Memory
$ eu-readelf -l /bin/ls
Program Headers:
Type
 Offset
 VirtAddr
 PhysAddr
 FileSiz MemSiz
 Flg Align
...
LOAD
 0x000000 0x0000000000400000 0x0000000000400000 0x0132ac 0x0132ac R E 0x200000
LOAD
 0x0132b0 0x00000000006132b0 0x00000000006132b0 0x001a71 0x001a71 RW 0x200000
...
Figure 4.3: ELF Program Header Indicating Alignment Requirements
number of TLB entries needed by moving data which is
used at the same time to fewer pages. This is similar to
some optimizations for cache use we talked about above.
Only now the alignment required is large. Given that
the number of TLB entries is quite small this can be an
important optimization.
4.4
 Impact Of Virtualization
Virtualization of OS images will become more and more
prevalent; this means another layer of memory handling
is added to the picture. Virtualization of processes (basi-
cally jails) or OS containers do not fall into this category
since only one OS is involved. Technologies like Xen or
KVM enable–with or without help from the processor–
the execution of independent OS images. In these situa-
tions there is one piece of software alone which directly
controls access to the physical memory.
Xen I/OSupportXen I/O
Support
Dom0 Kernel
 DomU Kernel
 DomU Kernel
Xen VMM
Figure 4.4: Xen Virtualization Model
In the case of Xen (see Figure 4.4) the Xen VMM is
that piece of software. The VMM does not implement
many of the other hardware controls itself, though. Un-
like VMMs on other, earlier systems (and the first re-
lease of the Xen VMM) the hardware outside of memory
and processors is controlled by the privileged Dom0 do-
main. Currently, this is basically the same kernel as the
unprivileged DomU kernels and, as far as memory han-
dling is concerned, they do not differ. Important here is
that the VMM hands out physical memory to the Dom0
and DomU kernels which, themselves, then implement
the usual memory handling as if they were running di-
rectly on a processor.
To implement the separation of the domains which is re-
quired for the virtualization to be complete, the mem-
ory handling in the Dom0 and DomU kernels does not
have unrestricted access to physical memory. The VMM
does not hand out memory by giving out individual phys-
ical pages and letting the guest OSes handle the address-
ing; this would not provide any protection against faulty
or rogue guest domains. Instead, the VMM creates its
own page table tree for each guest domain and hands out
memory using these data structures. The good thing is
that access to the administrative information of the page
table tree can be controlled. If the code does not have
appropriate privileges it cannot do anything.
This access control is exploited in the virtualization Xen
provides, regardless of whether para- or hardware (aka
full) virtualization is used. The guest domains construct
their page table trees for each process in a way which is
intentionally quite similar for para- and hardware virtu-
alization. Whenever the guest OS modifies its page ta-
bles the VMM is invoked. The VMM then uses the up-
dated information in the guest domain to update its own
shadow page tables. These are the page tables which are
actually used by the hardware. Obviously, this process
is quite expensive: each modification of the page table
tree requires an invocation of the VMM. While changes
to the memory mapping are not cheap without virtualiza-
tion they become even more expensive now.
The additional costs can be really large, considering that
the changes from the guest OS to the VMM and back
themselves are already quite expensive. This is why the
processors are starting to have additional functionality to
avoid the creation of shadow page tables. This is good
not only because of speed concerns but it also reduces
memory consumption by the VMM. Intel has Extended
Page Tables (EPTs) and AMD calls it Nested Page Ta-
bles (NPTs). Basically both technologies have the page
tables of the guest OSes produce “host virtual addresses”
from the “guest virtual address”. The host virtual ad-
dresses must then be further translated, using the per-
domain EPT/NPT trees, into actual physical addresses.
This will allow memory handling at almost the speed of
the no-virtualization case since most VMM entries for
memory handling are removed. It also reduces the mem-
ory use of the VMM since now only one page table tree
for each domain (as opposed to process) has to be main-
tained.
The results of the additional address translation steps are
also stored in the TLB. That means the TLB does not
store the virtual physical address but, instead, the com-
plete result of the lookup. It was already explained that
Ulrich Drepper
 Version 1.0
 41
AMD’s Pacifica extension introduced the ASID to avoid
TLB flushes on each entry. The number of bits for the
ASID is one in the initial release of the processor exten-
sions; this is just enough to differentiate VMM and guest
OS. Intel has virtual processor IDs (VPIDs) which serve
the same purpose, only there are more of them. But the
VPID is fixed for each guest domain and therefore it can-
not be used to mark separate processes and avoid TLB
flushes at that level, too.
The amount of work needed for each address space mod-
ification is one problem with virtualized OSes. There
is another problem inherent in VMM-based virtualiza-
tion, though: there is no way around having two layers
of memory handling. But memory handling is hard (es-
pecially when taking complications like NUMA into ac-
count, see section 5). The Xen approach of using a sep-
arate VMM makes optimal (or even good) handling hard
since all the complications of a memory management im-
plementation, including “trivial” things like discovery of
memory regions, must be duplicated in the VMM. The
OSes have fully-fledged and optimized implementations;
one really wants to avoid duplicating them.
leads to less work, fewer bugs, and, perhaps, less friction
where the two memory handlers touch since the memory
handler in a Linux guest makes the same assumptions as
the memory handler in the outer Linux kernel which runs
on the bare hardware.
Overall, programmers must be aware that, with virtual-
ization used, the cost of cache misses (instruction, data,
or TLB) is even higher than without virtualization. Any
optimization which reduces this work will pay off even
more in virtualized environments. Processor designers
will, over time, reduce the difference more and more
through technologies like EPT and NPT but it will never
completely go away.
Userlevel
Process
Guest Kernel
KVM VMM
Guest Kernel
KVM VMM
Linux Kernel
Figure 4.5: KVM Virtualization Model
This is why carrying the VMM/Dom0 model to its con-
clusion is such an attractive alternative. Figure 4.5 shows
how the KVM Linux kernel extensions try to solve the
problem. There is no separate VMM running directly
on the hardware and controlling all the guests; instead, a
normal Linux kernel takes over this functionality. This
means the complete and sophisticated memory handling
functionality in the Linux kernel is used to manage the
memory of the system. Guest domains run alongside
the normal user-level processes in what the creators call
“guest mode”. The virtualization functionality, para- or
full virtualization, is controlled by the KVM VMM. This
is just another userlevel process which happens to control
a guest domain using the special KVM device the kernel
implements.
The benefit of this model over the separate VMM of the
Xen model is that, even though there are still two mem-
ory handlers at work when guest OSes are used, there
only needs to be one implementation, that in the Linux
kernel. It is not necessary to duplicate the same function-
ality in another piece of code like the Xen VMM. This
42
 Version 1.0
 What Every Programmer Should Know About Memory
5
 NUMA Support
In section 2 we saw that, on some machines, the cost
of access to specific regions of physical memory differs
depending on where the access originated. This type of
hardware requires special care from the OS and the ap-
plications. We will start with a few details of NUMA
hardware, then we will cover some of the support the
Linux kernel provides for NUMA.
5.1
 NUMA Hardware
Non-uniform memory architectures are becoming more
and more common. In the simplest form of NUMA, a
processor can have local memory (see Figure 2.3) which
is cheaper to access than memory local to other proces-
sors. The difference in cost for this type of NUMA sys-
tem is not high, i.e., the NUMA factor is low.
NUMA is also–and especially–used in big machines. We
have described the problems of having many processors
access the same memory. For commodity hardware all
processors would share the same Northbridge (ignoring
the AMD Opteron NUMA nodes for now, they have their
own problems). This makes the Northbridge a severe
bottleneck since all memory traffic is routed through it.
Big machines can, of course, use custom hardware in
place of the Northbridge but, unless the memory chips
used have multiple ports–i.e. they can be used from mul-
tiple busses–there still is a bottleneck. Multiport RAM
is complicated and expensive to build and support and,
therefore, it is hardly ever used.
The next step up in complexity is the model AMD uses
where an interconnect mechanism (Hyper Transport in
AMD’s case, technology they licensed from Digital) pro-
vides access for processors which are not directly con-
nected to the RAM. The size of the structures which can
be formed this way is limited unless one wants to in-
crease the diameter (i.e., the maximum distance between
any two nodes) arbitrarily.
6
 8
2
 2
 4
 7
5
2
 4
C1
=1
1
C3
=2
1
C3
=3
Figure 5.1: Hypercubes
An efficient topology for connecting the nodes is the hy-
percube, which limits the number of nodes to 2C where
C is the number of interconnect interfaces each node has.
Hypercubes have the smallest diameter for all systems
with 2n CPUs and n interconnects. Figure 5.1 shows
the first three hypercubes. Each hypercube has a diam-
eter of C which is the absolute minimum. AMD’s first-
generation Opteron processors have three hypertransport
links per processor. At least one of the processors has to
have a Southbridge attached to one link, meaning, cur-
rently, that a hypercube with C = 2 can be implemented
directly and efficiently. The next generation will at some
point have four links, at which point C = 3 hypercubes
will be possible.
This does not mean, though, that larger accumulations
of processors cannot be supported. There are companies
which have developed crossbars allowing larger sets of
processors to be used (e.g., Newisys’s Horus). But these
crossbars increase the NUMA factor and they stop being
effective at a certain number of processors.
The next step up means connecting groups of CPUs and
implementing a shared memory for all of them. All such
systems need specialized hardware and are by no means
commodity systems. Such designs exist at several levels
of complexity. A system which is still quite close to a
commodity machine is IBM x445 and similar machines.
They can be bought as ordinary 4U, 8-way machines with
x86 and x86-64 processors. Two (at some point up to
four) of these machines can then be connected to work
as a single machine with shared memory. The intercon-
nect used introduces a significant NUMA factor which
the OS, as well as applications, must take into account.
At the other end of the spectrum, machines like SGI’s Al-
tix are designed specifically to be interconnected. SGI’s
NUMAlink interconnect fabric is very fast and has low
latency at the same time; both properties are require-
ments for high-performance computing (HPC), specifi-
cally when Message Passing Interfaces (MPI) are used.
The drawback is, of course, that such sophistication and
specialization is very expensive. They make a reason-
ably low NUMA factor possible but with the number of
CPUs these machines can have (several thousands) and
the limited capacity of the interconnects, the NUMA fac-
tor is actually dynamic and can reach unacceptable levels
depending on the workload.
More commonly used are solutions where many com-
modity machines are connected using high-speed net-
working to form a cluster. These are no NUMA ma-
chines, though; they do not implement a shared address
space and therefore do not fall into any category which is
discussed here.
5.2
 OS Support for NUMA
To support NUMA machines, the OS has to take the dis-
tributed nature of the memory into account. For instance,
if a process is run on a given processor, the physical
RAM assigned to the process’s address space should ide-
ally come from local memory. Otherwise each instruc-
tion has to access remote memory for code and data.
There are special cases to be taken into account which
are only present in NUMA machines. The text segment
of DSOs is normally present exactly once in a machine’s
Ulrich Drepper
 Version 1.0
 43
physical RAM. But if the DSO is used by processes and
threads on all CPUs (for instance, the basic runtime li-
braries like libc) this means that all but a few proces-
sors have to have remote accesses. The OS ideally would
“mirror” such DSOs into each processor’s physical RAM
and use local copies. This is an optimization, not a re-
quirement, and generally hard to implement. It might
not be supported or only in a limited fashion.
To avoid making the situation worse, the OS should not
migrate a process or thread from one node to another.
The OS should already try to avoid migrating processes
on normal multi-processor machines because migrating
from one processor to another means the cache content is
lost. If load distribution requires migrating a process or
thread off of a processor, the OS can usually pick an ar-
bitrary new processor which has sufficient capacity left.
In NUMA environments the selection of the new proces-
sor is a bit more limited. The newly selected processor
should not have higher access costs to the memory the
process is using than the old processor; this restricts the
list of targets. If there is no free processor matching that
criteria available, the OS has no choice but to migrate to
a processor where memory access is more expensive.
In this situation there are two possible ways forward.
First, one can hope the situation is temporary and the
process can be migrated back to a better-suited proces-
sor. Alternatively, the OS can also migrate the process’s
memory to physical pages which are closer to the newly-
used processor. This is quite an expensive operation.
Possibly huge amounts of memory have to be copied, al-
beit not necessarily in one step. While this is happening
the process, at least briefly, has to be stopped so that mod-
ifications to the old pages are correctly migrated. There
are a whole list of other requirements for page migration
to be efficient and fast. In short, the OS should avoid it
unless it is really necessary.
Generally, it cannot be assumed that all processes on a
NUMA machine use the same amount of memory such
that, with the distribution of processes across the pro-
cessors, memory usage is also equally distributed. In
fact, unless the applications running on the machines are
very specific (common in the HPC world, but not out-
side) the memory use will be very unequal. Some appli-
cations will use vast amounts of memory, others hardly
any. This will, sooner or later, lead to problems if mem-
ory is always allocated local to the processor where the
request is originated. The system will eventually run out
of memory local to nodes running large processes.
In response to these severe problems, memory is, by de-
fault, not allocated exclusively on the local node. To uti-
lize all the system’s memory the default strategy is to
stripe the memory. This guarantees equal use of all the
memory of the system. As a side effect, it becomes possi-
ble to freely migrate processes between processors since,
on average, the access cost to all the memory used does
not change. For small NUMA factors, striping is accept-
able but still not optimal (see data in section 5.4).
This is a pessimization which helps the system avoid se-
vere problems and makes it more predictable under nor-
mal operation. But it does decrease overall system per-
formance, in some situations significantly. This is why
Linux allows the memory allocation rules to be selected
by each process. A process can select a different strategy
for itself and its children. We will introduce the inter-
faces which can be used for this in section 6.
5.3
 Published Information
The kernel publishes, through the sys pseudo file system
(sysfs), information about the processor caches below
/sys/devices/system/cpu/cpu*/cache
In section 6.2.1 we will see interfaces which can be used
to query the size of the various caches. What is important
here is the topology of the caches. The directories above
contain subdirectories (named index*) which list infor-
mation about the various caches the CPU possesses. The
files type, level, and shared_cpu_map are the im-
portant files in these directories as far as the topology is
concerned. For an Intel Core 2 QX6700 the information
looks as in Table 5.1.
type
 level
 shared_cpu_map
index0
 Data
 1
 00000001
cpu0
 index1
 Instruction
 1
 00000001
index2
 Unified
 2
 00000003
index0
 Data
 1
 00000002
cpu1
 index1
 Instruction
 1
 00000002
index2
 Unified
 2
 00000003
index0
 Data
 1
 00000004
cpu2
 index1
 Instruction
 1
 00000004
index2
 Unified
 2
 0000000c
index0
 Data
 1
 00000008
cpu3
 index1
 Instruction
 1
 00000008
index2
 Unified
 2
 0000000c
Table 5.1: sysfs Information for Core 2 CPU Caches
What this data means is as follows:
• Each core25 has three caches: L1i, L1d, L2.
• The L1d and L1i caches are not shared with any
other core–each core has its own set of caches.
This is indicated by the bitmap in shared_cpu_-
map having only one set bit.
• The L2 cache on cpu0 and cpu1 is shared, as is
the L2 on cpu2 and cpu3.
If the CPU had more cache levels, there would be more
index* directories.
For a four-socket, dual-core Opteron machine the cache
information looks like Table 5.2. As can be seen these
25The knowledge that cpu0 to cpu3 are cores comes from another
place that will be explained shortly.
44
 Version 1.0
 What Every Programmer Should Know About Memory
type
 level
index0
 Data
cpu0
 index1
 Instruction
index2
 Unified
index0
 Data
cpu1
 index1
 Instruction
index2
 Unified
index0
 Data
cpu2
 index1
 Instruction
index2
 Unified
index0
 Data
cpu3
 index1
 Instruction
index2
 Unified
index0
 Data
cpu4
 index1
 Instruction
index2
 Unified
index0
 Data
cpu5
 index1
 Instruction
index2
 Unified
index0
 Data
cpu6
 index1
 Instruction
index2
 Unified
index0
 Data
cpu7
 index1
 Instruction
index2
 Unified
1
1
2
1
1
2
1
1
2
1
1
2
1
1
2
1
1
2
1
1
2
1
1
2
shared_cpu_map
00000001
00000001
00000001
00000002
00000002
00000002
00000004
00000004
00000004
00000008
00000008
00000008
00000010
00000010
00000010
00000020
00000020
00000020
00000040
00000040
00000040
00000080
00000080
00000080
Table 5.2: sysfs Information for Opteron CPU Caches
processors also have three caches: L1i, L1d, L2. None
of the cores shares any level of cache. The interesting
part for this system is the processor topology. Without
this additional information one cannot make sense of the
cache data. The sys file system exposes this information
in the files below
/sys/devices/system/cpu/cpu*/topology
Table 5.3 shows the interesting files in this hierarchy for
the SMP Opteron machine.
physical_
 core_
 thread_
package_id
 core_id
 siblings
 siblings
cpu0
 0
 00000003
 00000001
0
cpu1
 1
 00000003
 00000002
cpu2
 0
 0000000c
 00000004
1
cpu3
 1
 0000000c
 00000008
cpu4
 0
 00000030
 00000010
2
cpu5
 1
 00000030
 00000020
cpu6
 0
 000000c0
 00000040
3
cpu7
 1
 000000c0
 00000080
Table 5.3: sysfs Information for Opteron CPU Topol-
ogy
Taking Table 5.2 and Table 5.3 together we can see that
a) none of the CPU has hyper-threads (the thread_-
siblings bitmaps have one bit set),
b) the system in fact has a total of four processors
(physical_package_id 0 to 3),
c) each processor has two cores, and
d) none of the cores share any cache.
This is exactly what corresponds to earlier Opterons.
What is completely missing in the data provided so far is
information about the nature of NUMA on this machine.
Any SMP Opteron machine is a NUMA machine. For
this data we have to look at yet another part of the sys
file system which exists on NUMA machines, namely in
the hierarchy below
/sys/devices/system/node
This directory contains a subdirectory for every NUMA
node on the system. In the node-specific directories there
are a number of files. The important files and their con-
tent for the Opteron machine described in the previous
two tables are shown in Table 5.4.
node0
node1
node2
node3
cpumap
000000030000000c00000030000000c0distance
10 20 20 20
20 10 20 20
20 20 10 20
20 20 20 10
Table 5.4: sysfs Information for Opteron Nodes
This information ties all the rest together; now we have
a complete picture of the architecture of the machine.
We already know that the machine has four processors.
Each processor constitutes its own node as can be seen
by the bits set in the value in cpumap file in the node*
directories. The distance files in those directories con-
tains a set of values, one for each node, which represent
a cost of memory accesses at the respective nodes. In
this example all local memory accesses have the cost 10,
all remote access to any other node has the cost 20.26
This means that, even though the processors are orga-
nized as a two-dimensional hypercube (see Figure 5.1),
accesses between processors which are not directly con-
nected is not more expensive. The relative values of the
costs should be usable as an estimate of the actual differ-
ence of the access times. The accuracy of all this infor-
mation is another question.
5.4
 Remote Access Costs
The distance is relevant, though. In [1] AMD documents
the NUMA cost of a four socket machine. For write op-
erations the numbers are shown in Figure 5.3. Writes
26 This is, by the way, incorrect.
 The ACPI information is appar-
ently wrong since, although the processors used have three coherent
HyperTransport links, at least one processor must be connected to a
Southbridge. At least one pair of nodes must therefore have a larger
distance.
Ulrich Drepper
 Version 1.0
 45
00400000 default file=/bin/cat mapped=3 N3=3
00504000 default file=/bin/cat anon=1 dirty=1 mapped=2 N3=2
00506000 default heap anon=3 dirty=3 active=0 N3=3
38a9000000 default file=/lib64/ld-2.4.so mapped=22 mapmax=47 N1=22
38a9119000 default file=/lib64/ld-2.4.so anon=1 dirty=1 N3=1
38a911a000 default file=/lib64/ld-2.4.so anon=1 dirty=1 N3=1
38a9200000 default file=/lib64/libc-2.4.so mapped=53 mapmax=52 N1=51 N2=2
38a933f000 default file=/lib64/libc-2.4.so
38a943f000 default file=/lib64/libc-2.4.so anon=1 dirty=1 mapped=3 mapmax=32 N1=2 N3=1
38a9443000 default file=/lib64/libc-2.4.so anon=1 dirty=1 N3=1
38a9444000 default anon=4 dirty=4 active=0 N3=4
2b2bbcdce000 default anon=1 dirty=1 N3=1
2b2bbcde4000 default anon=2 dirty=2 N3=2
2b2bbcde6000 default file=/usr/lib/locale/locale-archive mapped=11 mapmax=8 N0=11
7fffedcc7000 default stack anon=2 dirty=2 N3=2
Figure 5.2: Content of /proc/PID/numa_maps
are slower than reads, this is no surprise. The interest-
ing parts are the costs of the 1- and 2-hop cases. The
two 1-hop cases actually have slightly different costs.
See [1] for the details. The fact we need to remem-
ber from this chart is that 2-hop reads and writes are
30% and 49% (respectively) slower than 0-hop reads. 2-
hop writes are 32% slower than 0-hop writes, and 17%
slower than 1-hop writes. The relative position of pro-
cessor and memory nodes can make a big difference.
The next generation of processors from AMD will fea-
ture four coherent HyperTransport links per processor. In
that case a four socket machine would have diameter of
one. With eight sockets the same problem returns, with a
vengeance, since the diameter of a hypercube with eight
nodes is three.
160%
d
aeRpoH-0sVnwodwolS140%
120%
100%
80%
60%
40%
20%
0%
0 Hop
 1 Hop
 1 Hop
Number of Hops
Reads
 Writes
2 Hop
Figure 5.3:
 Read/Write Performance with Multiple
Nodes
All this information is available but it is cumbersome to
use. In section 6.5 we will see an interface which helps
accessing and using this information easier.
The last piece of information the system provides is in the
status of a process itself. It is possible to determine how
the memory-mapped files, the Copy-On-Write (COW)27
pages and anonymous memory are distributed over the
nodes in the system. For each process the kernel pro-
vides a pseudo-file /proc/PID/numa_maps, where PID
is the ID of the process, as shown in Figure 5.2. The
important information in the file is the values for N0 to
N3, which indicate the number of pages allocated for the
memory area on nodes 0 to 3. It is a good guess that the
program was executed on a core on node 3. The pro-
gram itself and the dirtied pages are allocated on that
node. Read-only mappings, such as the first mapping for
ld-2.4.so and libc-2.4.so as well as the shared file
locale-archive are allocated on other nodes.
As we have seen in Figure 5.3, when performed across
nodes the read performance falls by 9% and 30% respec-
tively for 1- and 2-hop reads. For execution, such reads
are needed and, if the L2 cache is missed, each cache line
incurs these additional costs. All the costs measured for
large workloads beyond the size of the cache would have
to be increased by 9%/30% if the memory is remote to
the processor.
To see the effects in the real world we can measure the
bandwidth as in section 3.5.1 but this time with the mem-
ory being on a remote node, one hop away. The result
of this test when compared with the data for using local
memory can be seen in Figure 5.4. The numbers have a
few big spikes in both directions which are the result of
a problem of measuring multi-threaded code and can be
ignored. The important information in this graph is that
read operations are always 20% slower. This is signifi-
cantly slower than the 9% in Figure 5.3, which is, most
likely, not a number for uninterrupted read/write opera-
tions and might refer to older processor revisions. Only
27 Copy-On-Write is a method often used in OS implementations
when a memory page has one user at first and then has to be copied
to allow independent users. In many situations the copying is unneces-
sary, at all or at first, in which case it makes sense to only copy when
either user modifies the memory. The operating system intercepts the
write operation, duplicates the memory page, and then allows the write
instruction to proceed.
46
 Version 1.0
 What Every Programmer Should Know About Memory
15%
y
 10%
ro 5%
me 0%
Ml −5%
ac −10%
oL −15%
sV −20%
n −25%
wo −30%
dw −35%
ol S −40%
2
10
 2
13
 2
16
 2
19
 2
22
 2
25
 2
28
Working Set Size (Bytes)
Read
 Write
 Copy
Figure 5.4: Operating on Remote Memory
AMD knows.
For working set sizes which fit into the caches, the perfor-
mance of write and copy operations is also 20% slower.
For working sets exceeding the size of the caches, the
write performance is not measurably slower than the op-
eration on the local node. The speed of the interconnect
is fast enough to keep up with the memory. The dominat-
ing factor is the time spent waiting on the main memory.
6
 What Programmers Can Do
After the descriptions in the previous sections it is clear
that there are many, many opportunities for programmers
to influence a program’s performance, positively or neg-
atively. And this is for memory-related operations only.
We will proceed in covering the opportunities from the
ground up, starting with the lowest levels of physical
RAM access and L1 caches, up to and including OS func-
tionality which influences memory handling.
6.1
 Bypassing the Cache
When data is produced and not (immediately) consumed
again, the fact that memory store operations read a full
cache line first and then modify the cached data is detri-
mental to performance. This operation pushes data out
of the caches which might be needed again in favor of
data which will not be used soon. This is especially true
for large data structures, like matrices, which are filled
and then used later. Before the last element of the matrix
is filled the sheer size evicts the first elements, making
caching of the writes ineffective.
For this and similar situations, processors provide sup-
port for non-temporal write operations. Non-temporal in
this context means the data will not be reused soon, so
there is no reason to cache it. These non-temporal write
operations do not read a cache line and then modify it;
instead, the new content is directly written to memory.
This might sound expensive but it does not have to be.
The processor will try to use write-combining (see sec-
tion 3.3.3) to fill entire cache lines. If this succeeds no
memory read operation is needed at all. For the x86 and
x86-64 architectures a number of intrinsics are provided
by gcc:
#include <emmintrin.h>
void _mm_stream_si32(int *p, int a);
void _mm_stream_si128(int *p, __m128i a);
void _mm_stream_pd(double *p, __m128d a);
#include <xmmintrin.h>
void _mm_stream_pi(__m64 *p, __m64 a);
void _mm_stream_ps(float *p, __m128 a);
#include <ammintrin.h>
void _mm_stream_sd(double *p, __m128d a);
void _mm_stream_ss(float *p, __m128 a);
Ulrich Drepper
These instructions are used most efficiently if they pro-
cess large amounts of data in one go. Data is loaded from
memory, processed in one or more steps, and then written
back to memory. The data “streams” through the proces-
sor, hence the names of the intrinsics.
The memory address must be aligned to 8 or 16 bytes re-
spectively. In code using the multimedia extensions it is
possible to replace the normal _mm_store_* intrinsics
Version 1.0
 47
with these non-temporal versions. In the matrix multi-
plication code in section A.1 we do not do this since the
written values are reused in a short order of time. This
is an example where using the stream instructions is not
useful. More on this code in section 6.2.1.
The processor’s write-combining buffer can hold requests
for partial writing to a cache line for only so long. It
is generally necessary to issue all the instructions which
modify a single cache line one after another so that the
write-combining can actually take place. An example for
how to do this is as follows:
#include <emmintrin.h>
void setbytes(char *p, int c)
{
__m128i i = _mm_set_epi8(c, c, c, c,
c, c, c, c,
c, c, c, c,
c, c, c, c);
_mm_stream_si128((__m128i *)&p[0], i);
_mm_stream_si128((__m128i *)&p[16], i);
_mm_stream_si128((__m128i *)&p[32], i);
_mm_stream_si128((__m128i *)&p[48], i);
}
Assuming the pointer p is appropriately aligned, a call to
this function will set all bytes of the addressed cache line
to c. The write-combining logic will see the four gener-
ated movntdq instructions and only issue the write com-
mand for the memory once the last instruction has been
executed. To summarize, this code sequence not only
avoids reading the cache line before it is written, it also
avoids polluting the cache with data which might not be
needed soon. This can have huge benefits in certain situa-
tions. An example of everyday code using this technique
is the memset function in the C runtime, which should
use a code sequence like the above for large blocks.
Some architectures provide specialized solutions. The
PowerPC architecture defines the dcbz instruction which
can be used to clear an entire cache line. The instruction
does not really bypass the cache since a cache line is al-
located for the result, but no data is read from memory. It
is more limited than the non-temporal store instructions
since a cache line can only be set to all-zeros and it pol-
lutes the cache (in case the data is non-temporal), but no
write-combining logic is needed to achieve the results.
To see the non-temporal instructions in action we will
look at a new test which is used to measure writing to a
matrix, organized as a two-dimensional array. The com-
piler lays out the matrix in memory so that the leftmost
(first) index addresses the row which has all elements laid
out sequentially in memory. The right (second) index ad-
dresses the elements in a row. The test program iterates
over the matrix in two ways: first by increasing the col-
umn number in the inner loop and then by increasing the
row index in the inner loop. This means we get the be-
i
 j
j
 i
Figure 6.1: Matrix Access Pattern
havior shown in Figure 6.1.
We measure the time it takes to initialize a 3000 × 3000
matrix. To see how memory behaves, we use store in-
structions which do not use the cache. On IA-32 proces-
sors the “non-temporal hint” is used for this. For com-
parison we also measure ordinary store operations. The
results can be seen in Table 6.1.
Inner Loop Increment
Row
 Column
Normal
 0.048s
 0.127s
Non-Temporal
 0.048s
 0.160s
Table 6.1: Timing Matrix Initialization
For the normal writes which do use the cache we see
the expected result: if memory is used sequentially we
get a much better result, 0.048s for the whole operation
translating to about 750MB/s, compared to the more-or-
less random access which takes 0.127s (about 280MB/s).
The matrix is large enough that the caches are essentially
ineffective.
The part we are mainly interested in here are the writes
bypassing the cache. It might be surprising that the se-
quential access is just as fast here as in the case where the
cache is used. The reason for this result is that the proces-
sor is performing write-combining as explained above.
In addition, the memory ordering rules for non-temporal
writes are relaxed: the program needs to explicitly in-
sert memory barriers (sfence instructions for the x86
and x86-64 processors). This means the processor has
more freedom to write back the data and thereby using
the available bandwidth as well as possible.
In the case of column-wise access in the inner loop the
situation is different. The results for uncached accesses
are significantly slower than in the case of cached ac-
cesses (0.16s, about 225MB/s). Here we can see that no
write combining is possible and each memory cell must
be addressed individually. This requires constantly se-
lecting new rows in the RAM chips with all the associ-
ated delays. The result is a 25% worse result than the
cached run.
48
 Version 1.0
 What Every Programmer Should Know About Memory
On the read side, processors, until recently, lacked sup-
port aside from weak hints using non-temporal access
(NTA) prefetch instructions. There is no equivalent to
write-combining for reads, which is especially bad for
uncacheable memory such as memory-mapped I/O. In-
tel, with the SSE4.1 extensions, introduced NTA loads.
They are implemented using a small number of streaming
load buffers; each buffer contains a cache line. The first
movntdqa instruction for a given cache line will load a
cache line into a buffer, possibly replacing another cache
line. Subsequent 16-byte aligned accesses to the same
cache line will be serviced from the load buffer at little
cost. Unless there are other reasons to do so, the cache
line will not be loaded into a cache, thus enabling the
loading of large amounts of memory without polluting
the caches. The compiler provides an intrinsic for this
instruction:
#include <smmintrin.h>
__m128i _mm_stream_load_si128 (__m128i *p);
This intrinsic should be used multiple times, with ad-
dresses of 16-byte blocks passed as the parameter, un-
til each cache line is read. Only then should the next
cache line be started. Since there are a few streaming
read buffers it might be possible to read from two mem-
ory locations at once.
What we should take away from this experiment is that
modern CPUs very nicely optimize uncached write and
more recently even read accesses as long as they are se-
quential. This knowledge can come in very handy when
handling large data structures which are used only once.
Second, caches can help to cover up some–but not all–of
the costs of random memory access. Random access in
this example is 70% slower due to the implementation of
RAM access. Until the implementation changes, random
accesses should be avoided whenever possible.
In the section about prefetching we will again take a look
at the non-temporal flag.
6.2
 Cache Access
mance. In this section we will show what kinds of code
changes can help to improve that performance. Contin-
uing from the previous section, we first concentrate on
optimizations to access memory sequentially. As seen in
the numbers of section 3.3, the processor automatically
prefetches data when memory is accessed sequentially.
The example code used is a matrix multiplication. We
use two square matrices of 1000 × 1000 double ele-
ments. For those who have forgotten the math, given
two matrices A and B with elements aij and bij with
0 ≤ i, j < N the product is
N
−1
X
(AB)ij =
 aik bkj = ai1b1j + ai2 b2j + · · · + ai(N −1) b(N −1)j
k=0
A straight-forward C implementation of this can look like
this:
for (i = 0; i < N; ++i)
for (j = 0; j < N; ++j)
for (k = 0; k < N; ++k)
res[i][j] += mul1[i][k] * mul2[k][j];
The two input matrices are mul1 and mul2. The result
matrix res is assumed to be initialized to all zeroes. It
is a nice and simple implementation. But it should be
obvious that we have exactly the problem explained in
Figure 6.1. While mul1 is accessed sequentially, the in-
ner loop advances the row number of mul2. That means
that mul1 is handled like the left matrix in Figure 6.1
while mul2 is handled like the right matrix. This cannot
be good.
There is one possible remedy one can easily try. Since
each element in the matrices is accessed multiple times it
might be worthwhile to rearrange (“transpose,” in math-
ematical terms) the second matrix mul2 before using it.
N
−1
X
 T
 T
 T
 T
(AB)ij =
 aik bjk = ai1bj1 + ai2 bj2 + · · · + ai(N −1) bj(N −1)
k=0
Programmers wishing to improve their programs’ perfor-
mance will find it best to focus on changes affected the
level 1 cache since those will likely yield the best results.
We will discuss it first before extending the discussion to
the other levels. Obviously, all the optimizations for the
level 1 cache also affect the other caches. The theme for
all memory access is the same: improve locality (spatial
and temporal) and align the code and data.
6.2.1
 Optimizing Level 1 Data Cache Access
In section section 3.3 we have already seen how much
the effective use of the L1d cache can improve perfor-
After the transposition (traditionally indicated by a su-
perscript ‘T’) we now iterate over both matrices sequen-
tially. As far as the C code is concerned, it now looks
like this:
double tmp[N][N];
for (i = 0; i < N; ++i)
for (j = 0; j < N; ++j)
tmp[i][j] = mul2[j][i];
for (i = 0; i < N; ++i)
for (j = 0; j < N; ++j)
for (k = 0; k < N; ++k)
res[i][j] += mul1[i][k] * tmp[j][k];
Ulrich Drepper
 Version 1.0
 49
We create a temporary variable to contain the transposed
matrix. This requires touching additional memory, but
this cost is, hopefully, recovered since the 1000 non-
sequential accesses per column are more expensive (at
least on modern hardware). Time for some performance
tests. The results on a Intel Core 2 with 2666MHz clock
speed are (in clock cycles):
Original
 Transposed
Cycles
 16,765,297,870
 3,922,373,010
Relative
 100%
 23.4%
Through the simple transformation of the matrix we can
achieve a 76.6% speed-up! The copy operation is more
than made up. The 1000 non-sequential accesses really
hurt.
The next question is whether this is the best we can do.
We certainly need an alternative method anyway which
does not require the additional copy. We will not always
have the luxury to be able to perform the copy: the matrix
can be too large or the available memory too small.
The search for an alternative implementation should start
with a close examination of the math involved and the op-
erations performed by the original implementation. Triv-
ial math knowledge allows us to see that the order in
which the additions for each element of the result ma-
trix are performed is irrelevant as long as each addend
appears exactly once.28 This understanding allows us to
look for solutions which reorder the additions performed
in the inner loop of the original code.
Now let us examine the actual problem in the execution
of the original code. The order in which the elements
of mul2 are accessed is: (0,0), (1,0), . . . , (N -1,0), (0,1),
(1,1), . . . . The elements (0,0) and (0,1) are in the same
cache line but, by the time the inner loop completes one
round, this cache line has long been evicted. For this
example, each round of the inner loop requires, for each
of the three matrices, 1000 cache lines (with 64 bytes for
the Core 2 processor). This adds up to much more than
the 32k of L1d available.
But what if we handle two iterations of the middle loop
together while executing the inner loop? In this case
we use two double values from the cache line which
is guaranteed to be in L1d. We cut the L1d miss rate in
half. That is certainly an improvement, but, depending
on the cache line size, it still might not be as good as we
can get it. The Core 2 processor has a L1d cache line size
of 64 bytes. The actual value can be queried using
sysconf (_SC_LEVEL1_DCACHE_LINESIZE)
at runtime or using the getconf utility from the com-
mand line so that the program can be compiled for a spe-
cific cache line size. With sizeof(double) being 8
28We ignore arithmetic effects here which might change the occur-
rence of overflows, underflows, or rounding.
this means that, to fully utilize the cache line, we should
unroll the middle loop 8 times. Continuing this thought,
to effectively use the res matrix as well, i.e., to write 8
results at the same time, we should unroll the outer loop 8
times as well. We assume here cache lines of size 64 but
the code works also well on systems with 32 byte cache
lines since both cache lines are also 100% utilized. In
general it is best to hardcode cache line sizes at compile
time by using the getconf utility as in:
gcc -DCLS=$(getconf LEVEL1_DCACHE_LINESIZE) ...
If the binaries are supposed to be generic, the largest
cache line size should be used. With very small L1ds
this might mean that not all the data fits into the cache
but such processors are not suitable for high-performance
programs anyway. The code we arrive at looks some-
thing like this:
#define SM (CLS / sizeof (double))
for (i = 0; i < N; i += SM)
for (j = 0; j < N; j += SM)
for (k = 0; k < N; k += SM)
for (i2 = 0, rres = &res[i][j],
rmul1 = &mul1[i][k]; i2 < SM;
++i2, rres += N, rmul1 += N)
for (k2 = 0, rmul2 = &mul2[k][j];
k2 < SM; ++k2, rmul2 += N)
for (j2 = 0; j2 < SM; ++j2)
rres[j2] += rmul1[k2] * rmul2[j2];
This looks quite scary. To some extent it is but only
because it incorporates some tricks. The most visible
change is that we now have six nested loops. The outer
loops iterate with intervals of SM (the cache line size di-
vided by sizeof(double)). This divides the multipli-
cation in several smaller problems which can be handled
with more cache locality. The inner loops iterate over
the missing indexes of the outer loops. There are, once
again, three loops. The only tricky part here is that the k2
and j2 loops are in a different order. This is done since,
in the actual computation, only one expression depends
on k2 but two depend on j2.
The rest of the complication here results from the fact
that gcc is not very smart when it comes to optimizing ar-
ray indexing. The introduction of the additional variables
rres, rmul1, and rmul2 optimizes the code by pulling
common expressions out of the inner loops, as far down
as possible. The default aliasing rules of the C and C++
languages do not help the compiler making these deci-
sions (unless restrict is used, all pointer accesses are
potential sources of aliasing). This is why Fortran is still
a preferred language for numeric programming: it makes
writing fast code easier.29
29 In theory the restrict keyword introduced into the C lan-
guage in the 1999 revision should solve the problem. Compilers have
not caught up yet, though. The reason is mainly that too much incorrect
code exists which would mislead the compiler and cause it to generate
incorrect object code.
50
 Version 1.0
 What Every Programmer Should Know About Memory
Original
 Transposed
 Sub-Matrix
 Vectorized
Cycles
 16,765,297,870
 3,922,373,010
 2,895,041,480
 1,588,711,750
Relative
 100%
 23.4%
 17.3%
 9.47%
Table 6.2: Matrix Multiplication Timing
How all this work pays off can be seen in Table 6.2. By
avoiding the copying we gain another 6.1% of perfor-
mance. Plus, we do not need any additional memory.
The input matrices can be arbitrarily large as long as the
result matrix fits into memory as well. This is a require-
ment for a general solution which we have now achieved.
There is one more column in Table 6.2 which has not
been explained. Most modern processors nowadays in-
clude special support for vectorization. Often branded
as multi-media extensions, these special instructions al-
low processing of 2, 4, 8, or more values at the same
time. These are often SIMD (Single Instruction, Mul-
tiple Data) operations, augmented by others to get the
data in the right form. The SSE2 instructions provided
by Intel processors can handle two double values in one
operation. The instruction reference manual lists the in-
trinsic functions which provide access to these SSE2 in-
structions. If these intrinsics are used the program runs
another 7.3% (relative to the original) faster. The result is
a program which runs in 10% of the time of the original
code. Translated into numbers which people recognize,
we went from 318 MFLOPS to 3.35 GFLOPS. Since we
are only interested in memory effects here, the program
code is pushed out into section A.1.
It should be noted that, in the last version of the code,
we still have some cache problems with mul2; prefetch-
ing still will not work. But this cannot be solved with-
out transposing the matrix. Maybe the cache prefetching
units will get smarter to recognize the patterns, then no
additional change would be needed. 3.19 GFLOPS on a
2.66 GHz processor with single-threaded code is not bad,
though.
What we optimized in the example of the matrix multi-
plication is the use of the loaded cache lines. All bytes of
a cache line are always used. We just made sure they are
used before the cache line is evacuated. This is certainly
a special case.
It is much more common to have data structures which
fill one or more cache lines where the program uses only
a few members at any one time. In Figure 3.11 we have
already seen the effects of large structure sizes if only
few members are used.
Figure 6.2 shows the results of yet another set of bench-
marks performed using the by now well-known program.
This time two values of the same list element are added.
In one case, both elements are in the same cache line; in
another case, one element is in the first cache line of the
list element and the second is in the last cache line. The
graph shows the slowdown we are experiencing.
35%
e
niLehcaC1sVnwodwolS30%
25%
20%
15%
10%
5%
0%
−5%
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Sequential 4 CLs
 Random 2 CLs
 Random 4 CLs
Figure 6.2: Spreading Over Multiple Cache Lines
Unsurprisingly, in all cases there are no negative effects
if the working set fits into L1d. Once L1d is no longer
sufficient, penalties are paid by using two cache lines in
the process instead of one. The red line shows the data
when the list is laid out sequentially in memory. We see
the usual two step patterns: about 17% penalty when the
L2 cache is sufficient and about 27% penalty when the
main memory has to be used.
In the case of random memory accesses the relative data
looks a bit different. The slowdown for working sets
which fit into L2 is between 25% and 35%. Beyond that
it goes down to about 10%. This is not because the penal-
ties get smaller but, instead, because the actual memory
accesses get disproportionally more costly. The data also
shows that, in some cases, the distance between the el-
ements does matter. The Random 4 CLs curve shows
higher penalties because the first and fourth cache lines
are used.
An easy way to see the layout of a data structure com-
pared to cache lines is to use the pahole program (see
[4]). This program examines the data structures defined
in a binary. Take a program containing this definition:
struct foo {
int a;
long fill[7];
int b;
};
Ulrich Drepper
 Version 1.0
 51
struct foo {
int
 a;
/* XXX 4 bytes hole, try to pack */
/*
 0
 4 */
long int
 fill[7];
 /*
 8
 56 */
/* --- cacheline 1 boundary (64 bytes) --- */
int
 b;
 /*
 64
 4 */
}; /* size: 72, cachelines: 2 */
/* sum members: 64, holes: 1, sum holes: 4 */
/* padding: 4 */
/* last cacheline: 8 bytes */
Figure 6.3: Output of pahole Run
When compiled on a 64-bit machine, the output of pa-
hole contains (among other things) the output shown in
Figure 6.3. This output tells us a lot. First, it shows that
the data structure uses up more than one cache line. The
tool assumes the currently used processor’s cache line
size, but this value can be overridden using a command
line parameter. Especially in cases where the size of the
structure is barely over the limit of a cache line, and many
objects of this type are allocated, it makes sense to seek
a way to compress that structure. Maybe a few elements
can have a smaller type, or maybe some fields are actu-
ally flags which can be represented using individual bits.
In the case of the example the compression is easy and it
is hinted at by the program. The output shows that there
is a hole of four bytes after the first element. This hole is
caused by the alignment requirement of the structure and
the fill element. It is easy to see that the element b,
which has a size of four bytes (indicated by the 4 at the
end of the line), fits perfectly into the gap. The result in
this case is that the gap no longer exists and that the data
structure fits onto one cache line. The pahole tool can
perform this optimization itself. If the --reorganize
parameter is used and the structure name is added at the
end of the command line the output of the tool is the op-
timized structure and the cache line use. Besides moving
elements to fill gaps, the tool can also optimize bit fields
and combine padding and holes. For more details see [4].
Having a hole which is just large enough for the trailing
element is, of course, the ideal situation. For this opti-
mization to be useful it is required that the object itself is
aligned to a cache line. We get to that in a bit.
The pahole output also allows to see easily whether ele-
ments have to be reordered so that those elements which
are used together are also stored together. Using the pa-
hole tool, it is easily possible to determine which ele-
ments are on the same cache line and when, instead, the
elements have to be reshuffled to achieve that. This is not
an automatic process but the tool can help quite a bit.
The position of the individual structure elements and the
way they are used is important, too. As we have seen
in section 3.5.2 the performance of code with the critical
word late in the cache line is worse. This means a pro-
grammer should always follow the following two rules:
1. Always move the structure element which is most
likely to be the critical word to the beginning of
the structure.
2. When accessing the data structures, and the order
of access is not dictated by the situation, access the
elements in the order in which they are defined in
the structure.
For small structures, this means that the elements should
be arranged in the order in which they are likely accessed.
This must be handled in a flexible way to allow the other
optimizations, such as filling holes, to be applied as well.
For bigger data structures each cache line-sized block
should be arranged to follow the rules.
If the object itself is not aligned as expected, reorder-
ing elements is not worth the time it takes, though. The
alignment of an object is determined by the alignment
requirement of the data type. Each fundamental type has
its own alignment requirement. For structured types the
largest alignment requirement of any of its elements de-
termines the alignment of the structure. This is almost
always smaller than the cache line size. This means even
if the members of a structure are lined up to fit into the
same cache line an allocated object might not have an
alignment matching the cache line size. There are two
ways to ensure that the object has the alignment which
was used when designing the layout of the structure:
• the object can be allocated with an explicit align-
ment requirement. For dynamic allocation a call
to malloc would only allocate the object with an
alignment matching that of the most demanding
standard type (usually long double). It is pos-
sible to use posix_memalign, though, to request
higher alignments.
#include <stdlib.h>
int posix_memalign(void **memptr,
size_t align,
size_t size);
52
 Version 1.0
 What Every Programmer Should Know About Memory
The function stores a pointer pointing to the newly-
allocated memory in the pointer variable pointed to
by memptr. The memory block is size bytes in
size and is aligned on a align-byte boundary.
For objects allocated by the compiler (in .data,
.bss, etc, and on the stack) a variable attribute can
be used:
struct strtype variable
__attribute((aligned(64)));
In this case the variable is aligned at a 64 byte
boundary regardless of the alignment requirement
of the strtype structure. This works for global
variables as well as automatic variables.
For arrays this method does not work as one might
expect. Only the first element of the array would
be aligned unless the size of each array element is
a multiple of the alignment value. It also means
that every single variable must be annotated ap-
propriately. The use of posix_memalign is also
not entirely free since the alignment requirements
usually lead to fragmentation and/or higher mem-
ory consumption.
• the alignment requirement of a user-defined type
can be changed by using a type attribute:
struct strtype {
...members...
} __attribute((aligned(64)));
This will cause the compiler to allocate all objects
with the appropriate alignment, including arrays.
The programmer has to take care of requesting the
appropriate alignment for dynamically allocated ob-
jects, though. Here once again posix_memalign
must be used. It is easy enough to use the alignof
operator gcc provides and pass the value as the sec-
ond parameter to posix_memalign.
The multimedia extensions previously mentioned in this
section almost always require that the memory accesses
are aligned. I.e., for 16 byte memory accesses the address
is supposed to be 16 byte aligned. The x86 and x86-64
processors have special variants of the memory opera-
tions which can handle unaligned accesses but these are
slower. This hard alignment requirement is nothing new
for most RISC architectures which require full alignment
for all memory accesses. Even if an architecture sup-
ports unaligned accesses this is sometimes slower than
using appropriate alignment, especially if the misalign-
ment causes a load or store to use two cache lines instead
of one.
400%
s
 s 350%
ecc 300%
Ad e 250%
ngi l 200%
As V 150%
nw 100%
odw 50%
olS0%
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Sequential Inc
 Random Inc
Figure 6.4: Overhead of Unaligned Accesses
Figure 6.4 shows the effects of unaligned memory ac-
cesses. The now well-known tests which increment a
data element while visiting memory (sequentially or ran-
domly) are measured, once with aligned list elements and
once with deliberately misaligned elements. The graph
shows the slowdown the program incurs because of the
unaligned accesses. The effects are more dramatic for the
sequential access case than for the random case because,
in the latter case, the costs of unaligned accesses are par-
tially hidden by the generally higher costs of the mem-
ory access. In the sequential case, for working set sizes
which do fit into the L2 cache, the slowdown is about
300%. This can be explained by the reduced effective-
ness of the L1 cache. Some increment operations now
touch two cache lines, and beginning work on a list ele-
ment now often requires reading of two cache lines. The
connection between L1 and L2 is simply too congested.
For very large working set sizes, the effects of the un-
aligned access are still 20% to 30%–which is a lot given
that the aligned access time for those sizes is long. This
graph should show that alignment must be taken seri-
ously. Even if the architecture supports unaligned ac-
cesses, this must not be taken as “they are as good as
aligned accesses”.
There is some fallout from these alignment requirements,
though. If an automatic variable has an alignment re-
quirement, the compiler has to ensure that it is met in all
situations. This is not trivial since the compiler has no
control over the call sites and the way they handle the
stack. This problem can be handled in two ways:
1. The generated code actively aligns the stack, in-
serting gaps if necessary. This requires code to
check for alignment, create alignment, and later
undo the alignment.
2. Require that all callers have the stack aligned.
Ulrich Drepper
 Version 1.0
 53
All of the commonly used application binary interfaces
(ABIs) follow the second route. Programs will likely fail
if a caller violates the rule and alignment is needed in the
callee. Keeping alignment intact does not come for free,
though.
The size of a stack frame used in a function is not neces-
sarily a multiple of the alignment. This means padding is
needed if other functions are called from this stack frame.
The big difference is that the stack frame size is, in most
cases, known to the compiler and, therefore, it knows
how to adjust the stack pointer to ensure alignment for
any function which is called from that stack frame. In
fact, most compilers will simply round the stack frame
size up and be done with it.
This simple way to handle alignment is not possible if
variable length arrays (VLAs) or alloca are used. In
that case, the total size of the stack frame is only known
at runtime. Active alignment control might be needed in
this case, making the generated code (slightly) slower.
On some architectures, only the multimedia extensions
require strict alignment; stacks on those architectures are
always minimally aligned for the normal data types, usu-
ally 4 or 8 byte alignment for 32- and 64-bit architectures
respectively. On these systems, enforcing the alignment
incurs unnecessary costs. That means that, in this case,
we might want to get rid of the strict alignment require-
ment if we know that it is never depended upon. Tail
functions (those which call no other functions) which do
no multimedia operations do not need alignment. Nei-
ther do functions which only call functions which need
no alignment. If a large enough set of functions can be
identified, a program might want to relax the alignment
requirement. For x86 binaries gcc has support for relaxed
stack alignment requirements:
-mpreferred-stack-boundary=2
If this option is given a value of N , the stack alignment
requirement will be set to 2N bytes. So, if a value of 2
is used, the stack alignment requirement is reduced from
the default (which is 16 bytes) to just 4 bytes. In most
cases this means no additional alignment operation is
needed since normal stack push and pop operations work
on four-byte boundaries anyway. This machine-specific
option can help to reduce code size and also improve ex-
ecution speed. But it cannot be applied for many other
architectures. Even for x86-64 it is generally not appli-
cable since the x86-64 ABI requires that floating-point
parameters are passed in an SSE register and the SSE in-
structions require full 16 byte alignment. Nevertheless,
whenever the option is usable it can make a noticeable
difference.
Efficient placement of structure elements and alignment
are not the only aspects of data structures which influence
cache efficiency. If an array of structures is used, the en-
tire structure definition affects performance. Remember
the results in Figure 3.11: in this case we had increas-
ing amounts of unused data in the elements of the array.
The result was that prefetching was increasingly less ef-
fective and the program, for large data sets, became less
efficient.
For large working sets it is important to use the available
cache as well as possible. To achieve this, it might be
necessary to rearrange data structures. While it is easier
for the programmer to put all the data which conceptually
belongs together in the same data structure, this might
not be the best approach for maximum performance. As-
sume we have a data structure as follows:
struct order {
double price;
bool paid;
const char *buyer[5];
long buyer_id;
};
Further assume that these records are stored in a big array
and that a frequently-run job adds up the expected pay-
ments of all the outstanding bills. In this scenario, the
memory used for the buyer and buyer_id fields is un-
necessarily loaded into the caches. Judging from the data
in Figure 3.11 the program will perform up to 5 times
worse than it could.
It is much better to split the order data structure in two
pieces, storing the first two fields in one structure and the
other fields elsewhere. This change certainly increases
the complexity of the program, but the performance gains
might justify this cost.
Finally, let us consider another cache use optimization
which, while also applying to the other caches, is pri-
marily felt in the L1d access. As seen in Figure 3.8 an
increased associativity of the cache benefits normal op-
eration. The larger the cache, the higher the associativity
usually is. The L1d cache is too large to be fully associa-
tive but not large enough to have the same associativity as
L2 caches. This can be a problem if many of the objects
in the working set fall into the same cache set. If this
leads to evictions due to overuse of a set, the program
can experience delays even though much of the cache is
unused. These cache misses are sometimes called con-
flict misses. Since the L1d addressing uses virtual ad-
dresses, this is actually something the programmer can
have control over. If variables which are used together
are also stored together the likelihood of them falling into
the same set is minimized. Figure 6.5 shows how quickly
the problem can hit.
In the figure, the now familiar Follow30 with NPAD=15
test is measured with a special setup. The X–axis is the
distance between two list elements, measured in empty
30 The test was performed on a 32-bit machine, hence NPAD=15
means one 64-byte cache line per list element.
54
 Version 1.0
 What Every Programmer Should Know About Memory
z
y
10
16
9
8
 14
7
 12
6
10
5
8
4
3
 6
2
 4
1
2
0
 x
16
 32
 48
 64
 80
 96
 112
 128
Figure 6.5: Cache Associativity Effects
list elements. In other words, a distance of 2 means that
the next element’s address is 128 bytes after the previ-
ous one. All elements are laid out in the virtual ad-
dress space with the same distance. The Y–axis shows
the total length of the list. Only one to 16 elements are
used, meaning that the total working set size is 64 to
1024 bytes. The z–axis shows the average number of
cycles needed to traverse each list element.
The result shown in the figure should not be surprising.
If few elements are used, all the data fits into L1d and the
access time is only 3 cycles per list element. The same
is true for almost all arrangements of the list elements:
the virtual addresses are nicely mapped to L1d slots with
almost no conflicts. There are two (in this graph) spe-
cial distance values for which the situation is different.
If the distance is a multiple of 4096 bytes (i.e., distance
of 64 elements) and the length of the list is greater than
eight, the average number of cycles per list element in-
creases dramatically. In these situations all entries are in
the same set and, once the list length is greater than the
associativity, entries are flushed from L1d and have to be
re-read from L2 the next round. This results in the cost
of about 10 cycles per list element.
With this graph we can determine that the processor used
has an L1d cache with associativity 8 and a total size of
32kB. That means that the test could, if necessary, be
used to determine these values. The same effects can be
measured for the L2 cache but, here, it is more complex
since the L2 cache is indexed using physical addresses
and it is much larger.
Programmers will hopefully see this data as an indica-
tion that set associativity is something worth paying at-
tention to. Laying out data at boundaries that are powers
of two happens often enough in the real world, but this is
exactly the situation which can easily lead to the above
effects and degraded performance. Unaligned accesses
can increase the probability of conflict misses since each
access might require an additional cache line.
14
 7 6
 4 3
 0
Index
 Bank
 Byte
Figure 6.6: Bank Address of L1d on AMD
If this optimization is performed, another related opti-
mization is possible, too. AMD’s processors, at least,
implement the L1d as several individual banks. The L1d
can receive two data words per cycle but only if both
words are stored in different banks or in a bank with the
same index. The bank address is encoded in the low bits
of the virtual address as shown in Figure 6.6. If variables
which are used together are also stored together the like-
lihood that they are in different banks or the same bank
with the same index is high.
6.2.2
 Optimizing Level 1 Instruction Cache Access
Preparing code for good L1i use needs similar techniques
as good L1d use. The problem is, though, that the pro-
grammer usually does not directly influence the way L1i
is used unless s/he writes code in assembler. If compil-
ers are used, programmers can indirectly determine the
L1i use by guiding the compiler to create a better code
layout.
Code has the advantage that it is linear between jumps.
In these periods the processor can prefetch memory effi-
ciently. Jumps disturb this nice picture because
• the jump target might not be statically determined;
Ulrich Drepper
 Version 1.0
 55
• and even if it is static the memory fetch might take
a long time if it misses all caches.
These problems create stalls in execution with a possibly
severe impact on performance. This is why today’s pro-
cessors invest heavily in branch prediction (BP). Highly
specialized BP units try to determine the target of a jump
as far ahead of the jump as possible so that the processor
can initiate loading the instructions at the new location
into the cache. They use static and dynamic rules and are
increasingly good at determining patterns in execution.
Getting data into the cache as soon as possible is even
more important for the instruction cache. As mentioned
in section 3.1, instructions have to be decoded before
they can be executed and, to speed this up (important
on x86 and x86-64), instructions are actually cached in
the decoded form, not in the byte/word form read from
memory.
To achieve the best L1i use programmers should look out
for at least the following aspects of code generation:
1. reduce the code footprint as much as possible. This
has to be balanced with optimizations like loop un-
rolling and inlining.
2. code execution should be linear without bubbles.31
3. aligning code when it makes sense.
We will now look at some compiler techniques available
to help with optimizing programs according to these as-
pects.
Compilers have options to enable levels of optimization;
specific optimizations can also be individually enabled.
Many of the optimizations enabled at high optimization
levels (-O2 and -O3 for gcc) deal with loop optimizations
and function inlining. In general, these are good opti-
mizations. If the code which is optimized in these ways
accounts for a significant part of the total execution time
of the program, overall performance can be improved.
Inlining of functions, in particular, allows the compiler to
optimize larger chunks of code at a time which, in turn,
enables the generation of machine code which better ex-
ploits the processor’s pipeline architecture. The handling
of both code and data (through dead code elimination or
value range propagation, and others) works better when
larger parts of the program can be considered as a single
unit.
A larger code size means higher pressure on the L1i (and
also L2 and higher level) caches. This can lead to less
performance. Smaller code can be faster. Fortunately gcc
has an optimization option to specify this. If -Os is used
the compiler will optimize for code size. Optimizations
31 Bubbles describe graphically the holes in the execution in the
pipeline of a processor which appear when the execution has to wait
for resources. For more details the reader is referred to literature on
processor design.
which are known to increase the code size are disabled.
Using this option often produces surprising results. Es-
pecially if the compiler cannot really take advantage of
loop unrolling and inlining, this option is a big win.
Inlining can be controlled individually as well. The com-
piler has heuristics and limits which guide inlining; these
limits can be controlled by the programmer. The -finline-
limit option specifies how large a function must be to be
considered too large for inlining. If a function is called
in multiple places, inlining it in all of them would cause
an explosion in the code size. But there is more. As-
sume a function inlcand is called in two functions f1
and f2. The functions f1 and f2 are themselves called
in sequence.
start f1
code f1
inlined inlcand
more code f1
end f1
start f2
code f2
inlined inlcand
more code f2
end f2
start inlcand
code inlcand
end inlcand
start f1
code f1
end f1
start f2
code f2
end f2
Table 6.3: Inlining Vs Not
Table 6.3 shows how the generated code could look like
in the cases of no inline and inlining in both functions.
If the function inlcand is inlined in both f1 and f2 the
total size of the generated code is size f1 + size f2 +
2× size inlcand. If no inlining happens, the total size
is smaller by size inlcand. This is how much more L1i
and L2 cache is needed if f1 and f2 are called shortly af-
ter one another. Plus: if inlcand is not inlined, the code
might still be in L1i and it will not have to be decoded
again. Plus: the branch prediction unit might do a bet-
ter job of predicting jumps since it has already seen the
code. If the compiler default for the upper limit on the
size of inlined functions is not the best for the program,
it should be lowered.
There are cases, though, when inlining always makes
sense. If a function is only called once it might as well be
inlined. This gives the compiler the opportunity to per-
form more optimizations (like value range propagation,
which might significantly improve the code). That inlin-
ing might be thwarted by the selection limits. gcc has, for
cases like this, an option to specify that a function is al-
ways inlined. Adding the always_inline function at-
tribute instructs the compiler to do exactly what the name
suggests.
In the same context, if a function should never be inlined
despite being small enough, the noinline function at-
tribute can be used. Using this attribute makes sense even
for small functions if they are called often from different
56
 Version 1.0
 What Every Programmer Should Know About Memory
places. If the L1i content can be reused and the overall
footprint is reduced this often makes up for the additional
cost of the extra function call. Branch prediction units
are pretty good these days. If inlining can lead to more
aggressive optimizations things look different. This is
something which must be decided on a case-by-case ba-
sis.
The always_inline attribute works well if the inline
code is always used. But what if this is not the case?
What if the inlined function is called only occasionally:
void fct(void) {
... code block A ...
if (condition)
inlfct()
... code block C ...
}
The code generated for such a code sequence in general
matches the structure of the sources. That means first
comes the code block A, then a conditional jump which,
if the condition evaluates to false, jumps forward. The
code generated for the inlined inlfct comes next, and
finally the code block C. This looks all reasonable but it
has a problem.
If the condition is frequently false, the execution is
not linear. There is a big chunk of unused code in the
middle which not only pollutes the L1i due to prefetch-
ing, it also can cause problems with branch prediction. If
the branch prediction is wrong the conditional expression
can be very inefficient.
This is a general problem and not specific to inlining
functions. Whenever conditional execution is used and
it is lopsided (i.e., the expression far more often leads to
one result than the other) there is the potential for false
static branch prediction and thus bubbles in the pipeline.
This can be prevented by telling the compiler to move
the less often executed code out of the main code path.
In that case the conditional branch generated for an if
statement would jump to a place out of the order as can
be seen in the following figure.
A
 I
 B
 C
A
 I
 C
 B
The upper parts represents the simple code layout. If the
area B, e.g. generated from the inlined function inlfct
above, is often not executed because the conditional I
jumps over it, the prefetching of the processor will pull
in cache lines containing block B which are rarely used.
Using block reordering this can be changed, with a re-
sult that can be seen in the lower part of the figure. The
often-executed code is linear in memory while the rarely-
executed code is moved somewhere where it does not
hurt prefetching and L1i efficiency.
gcc provides two methods to achieve this. First, the com-
piler can take profiling output into account while recom-
piling code and lay out the code blocks according to the
profile. We will see how this works in section 7. The
second method is through explicit branch prediction. gcc
recognizes __builtin_expect:
long __builtin_expect(long EXP, long C);
This construct tells the compiler that the expression EXP
most likely will have the value C. The return value is EXP.
__builtin_expect is meant to be used in an condi-
tional expression. In almost all cases will it be used in the
context of boolean expressions in which case it is much
more convenient to define two helper macros:
#define unlikely(expr) __builtin_expect(!!(expr), 0)
#define likely(expr) __builtin_expect(!!(expr), 1)
These macros can then be used as in
if (likely(a > 1))
If the programmer makes use of these macros and then
uses the -freorder-blocks optimization option gcc
will reorder blocks as in the figure above. This option is
enabled with -O2 but disabled for -Os. There is another
gcc option to reorder block (-freorder-blocks-and-
partition) but it has limited usefulness because it does
not work with exception handling.
There is another big advantage of small loops, at least
on certain processors. The Intel Core 2 front end has a
special feature called Loop Stream Detector (LSD). If a
loop has no more than 18 instructions (none of which
is a call to a subroutine), requires only up to 4 decoder
fetches of 16 bytes, has at most 4 branch instructions, and
is executed more than 64 times, than the loop is some-
times locked in the instruction queue and therefore more
quickly available when the loop is used again. This ap-
plies, for instance, to small inner loops which are entered
many times through an outer loop. Even without such
specialized hardware compact loops have advantages.
Inlining is not the only aspect of optimization with re-
spect to L1i. Another aspect is alignment, just as for
data. There are obvious differences: code is a mostly lin-
ear blob which cannot be placed arbitrarily in the address
space and it cannot be influenced directly by the pro-
grammer as the compiler generates the code. There are
some aspects which the programmer can control, though.
Ulrich Drepper
 Version 1.0
 57
Aligning each single instruction does not make any sense.
The goal is to have the instruction stream be sequential.
So alignment only makes sense in strategic places. To
decide where to add alignments it is necessary to under-
stand what the advantages can be. Having an instruction
at the beginning of a cache line32 means that the prefetch
of the cache line is maximized. For instructions this also
means the decoder is more effective. It is easy to see that,
if an instruction at the end of a cache line is executed, the
processor has to get ready to read a new cache line and
decode the instructions. There are things which can go
wrong (such as cache line misses), meaning that an in-
struction at the end of the cache line is, on average, not
as effectively executed as one at the beginning.
Combine this with the follow-up deduction that the prob-
lem is most severe if control was just transferred to the
instruction in question (and hence prefetching is not ef-
fective) and we arrive at our final conclusion where align-
ment of code is most useful:
• at the beginning of functions;
• at the beginning of basic blocks which are reached
only through jumps;
• to some extent, at the beginning of loops.
In the first two cases the alignment comes at little cost.
Execution proceeds at a new location and, if we choose
it to be at the beginning of a cache line, we optimize
prefetching and decoding.33 The compiler accomplishes
this alignment through the insertion of a series of no-op
instructions to fill the gap created by aligning the code.
This “dead code” takes a little space but does not nor-
mally hurt performance.
The third case is slightly different: aligning the begin-
ning of each loop might create performance problems.
The problem is that beginning of a loop often follows
other code sequentially. If the circumstances are not very
lucky there will be a gap between the previous instruc-
tion and the aligned beginning of the loop. Unlike in the
previous two cases, this gap cannot be completely dead.
After execution of the previous instruction the first in-
struction in the loop must be executed. This means that,
following the previous instruction, there either must be a
number of no-op instructions to fill the gap or there must
be an unconditional jump to the beginning of the loop.
Neither possibility is free. Especially if the loop itself
is not executed often, the no-ops or the jump might cost
more than one saves by aligning the loop.
There are three ways the programmer can influence the
alignment of code. Obviously, if the code is written in
32 For some processors cache lines are not the atomic blocks for in-
structions. The Intel Core 2 front end issues 16 byte blocks to the de-
coder. They are appropriately aligned and so no issued block can span
a cache line boundary. Aligning at the beginning of a cache line still
has advantages since it optimizes the positive effects of prefetching.
33For instruction decoding processors often use a smaller unit than
cache lines, 16 bytes in case of x86 and x86-64.
assembler the function and all instructions in it can be
explicitly aligned. The assembler provides for all archi-
tectures the .align pseudo-op to do that. For high-level
languages the compiler must be told about alignment re-
quirements. Unlike for data types and variables this is not
possible in the source code. Instead a compiler option is
used:
-falign-functions=N
This option instructs the compiler to align all functions
to the next power-of-two boundary greater than N. That
means a gap of up to N bytes is created. For small func-
tions using a large value for N is a waste. Equally for
code which is executed only rarely. The latter can hap-
pen a lot in libraries which can contain both popular and
not-so-popular interfaces. A wise choice of the option
value can speed things up or save memory by avoiding
alignment. All alignment is turned off by using one as
the value of N or by using the -fno-align-functions
option.
The alignment for the second case above–beginning of
basic blocks which are not reached sequentially–can be
controlled with a different option:
-falign-jumps=N
All the other details are equivalent, the same warning
about waste of memory applies.
The third case also has its own option:
-falign-loops=N
Yet again, the same details and warnings apply. Except
that here, as explained before, alignment comes at a run-
time cost since either no-ops or a jump instruction has to
be executed if the aligned address is reached sequentially.
gcc knows about one more option for controlling align-
ment which is mentioned here only for completeness.
-falign-labels aligns every single label in the code
(basically the beginning of each basic block). This, in
all but a few exceptional cases, slows down the code and
therefore should not be used.
6.2.3
 Optimizing Level 2 and Higher Cache Access
Everything said about optimizations for level 1 caches
also applies to level 2 and higher cache accesses. There
are two additional aspects of last level caches:
• cache misses are always very expensive. While
L1 misses (hopefully) frequently hit L2 and higher
cache, thus limiting the penalties, there is obvi-
ously no fallback for the last level cache.
58
 Version 1.0
 What Every Programmer Should Know About Memory
• L2 caches and higher are often shared by multiple
cores and/or hyper-threads. The effective cache
size available to each execution unit is therefore
usually less than the total cache size.
To avoid the high costs of cache misses, the working
set size should be matched to the cache size. If data is
only needed once this obviously is not necessary since
the cache would be ineffective anyway. We are talking
about workloads where the data set is needed more than
once. In such a case the use of a working set which is
too large to fit into the cache will create large amounts
of cache misses which, even with prefetching being per-
formed successfully, will slow down the program.
A program has to perform its job even if the data set
is too large. It is the programmer’s job to do the work
in a way which minimizes cache misses. For last-level
caches this is possible–just as for L1 caches–by work-
ing on the job in smaller pieces. This is very similar to
the optimized matrix multiplication on page 50. One dif-
ference, though, is that, for last level caches, the data
blocks which are be worked on can be bigger. The code
becomes yet more complicated if L1 optimizations are
needed, too. Imagine a matrix multiplication where the
data sets–the two input matrices and the output matrix–
do not fit into the last level cache together. In this case
it might be appropriate to optimize the L1 and last level
cache accesses at the same time.
The L1 cache line size is usually constant over many pro-
cessor generations; even if it is not, the differences will
be small. It is no big problem to just assume the larger
size. On processors with smaller cache sizes two or more
cache lines will then be used instead of one. In any case,
it is reasonable to hardcode the cache line size and opti-
mize the code for it.
For higher level caches this is not the case if the program
is supposed to be generic. The sizes of those caches can
vary widely. Factors of eight or more are not uncommon.
It is not possible to assume the larger cache size as a de-
fault since this would mean the code performs poorly on
all machines except those with the biggest cache. The
opposite choice is bad too: assuming the smallest cache
means throwing away 87% of the cache or more. This is
bad; as we can see from Figure 3.14 using large caches
can have a huge impact on the program’s speed.
What this means is that the code must dynamically ad-
just itself to the cache line size. This is an optimiza-
tion specific to the program. All we can say here is
that the programmer should compute the program’s re-
quirements correctly. Not only are the data sets them-
selves needed, the higher level caches are also used for
other purposes; for example, all the executed instructions
are loaded from cache. If library functions are used this
cache usage might add up to a significant amount. Those
library functions might also need data of their own which
further reduces the available memory.
Once we have a formula for the memory requirement we
can compare it with the cache size. As mentioned before,
the cache might be shared with multiple other cores. Cur-
rently34 the only way to get correct information without
hardcoding knowledge is through the /sys filesystem.
In Table 5.2 we have seen the what the kernel publishes
about the hardware. A program has to find the directory:
/sys/devices/system/cpu/cpu*/cache
for the last level cache. This can be recognized by the
highest numeric value in the level file in that directory.
When the directory is identified the program should read
the content of the size file in that directory and divide
the numeric value by the number of bits set in the bitmask
in the file shared_cpu_map.
The value which is computed this way is a safe lower
limit. Sometimes a program knows a bit more about the
behavior of other threads or processes. If those threads
are scheduled on a core or hyper-thread sharing the cache,
and the cache usage is known to not exhaust its fraction
of the total cache size, then the computed limit might be
too low to be optimal. Whether more than the fair share
should be used really depends on the situation. The pro-
grammer has to make a choice or has to allow the user to
make a decision.
6.2.4
 Optimizing TLB Usage
There are two kinds of optimization of TLB usage. The
first optimization is to reduce the number of pages a pro-
gram has to use. This automatically results in fewer TLB
misses. The second optimization is to make the TLB
lookup cheaper by reducing the number higher level di-
rectory tables which must be allocated. Fewer tables
means less memory is used which can result in higher
cache hit rates for the directory lookup.
The first optimization is closely related to the minimiza-
tion of page faults. We will cover that topic in detail
in section 7.5. While page faults usually are a one-time
cost, TLB misses are a perpetual penalty given that the
TLB cache is usually small and it is flushed frequently.
Page faults are orders of magnitude more expensive than
TLB misses but, if a program is running long enough
and certain parts of the program are executed frequently
enough, TLB misses can outweigh even page fault costs.
It is therefore important to regard page optimization not
only from the perspective of page faults but also from the
TLB miss perspective. The difference is that, while page
fault optimizations only require page-wide grouping of
the code and data, TLB optimization requires that, at any
point in time, as few TLB entries are in use as possible.
The second TLB optimization is even harder to control.
The number of page directories which have to be used
depends on the distribution of the address ranges used in
the virtual address space of the process. Widely vary-
ing locations in the address space mean more directories.
34 There definitely will sometime soon be a better way!
Ulrich Drepper
 Version 1.0
 59
A complication is that Address Space Layout Random-
ization (ASLR) leads to exactly these situations. The
load addresses of stack, DSOs, heap, and possibly exe-
cutable are randomized at runtime to prevent attackers of
the machine from guessing the addresses of functions or
variables.
Only if maximum performance is critical ASLR should
be turned off. The costs of the extra directories is low
enough to make this step unnecessary in all but a few ex-
treme cases. One possible optimization the kernel could
at any time perform is to ensure that a single mapping
does not cross the address space boundary between two
directories. This would limit ASLR in a minimal fashion
but not enough to substantially weaken it.
The only way a programmer is directly affected by this
is when an address space region is explicitly requested.
This happens when using mmap with MAP_FIXED. Allo-
cating new a address space region this way is very dan-
gerous and hardly ever done. It is possible, though, and,
if it is used and the addresses can be freely chosen, the
programmer should know about the boundaries of the last
level page directory and select the requested address ap-
propriately.
6.3
 Prefetching
The purpose of prefetching is to hide the latency of a
memory access. The command pipeline and out-of-order
(OOO) execution capabilities of today’s processors can
hide some latency but, at best, only for accesses which
hit the caches. To cover the latency of main memory ac-
cesses, the command queue would have to be incredibly
long. Some processors without OOO try to compensate
by increasing the number of cores, but this is a bad trade
unless all the code in use is parallelized.
Prefetching can further help to hide latency. The proces-
sor performs prefetching on its own, triggered by certain
events (hardware prefetching) or explicitly requested by
the program (software prefetching).
6.3.1
 Hardware Prefetching
The trigger for the CPU to start hardware prefetching is
usually a sequence of two or more cache misses in a
certain pattern. These cache misses can be to succeed-
ing or preceding cache lines. In old implementations
only cache misses to adjacent cache lines are recognized.
With contemporary hardware, strides are recognized as
well, meaning that skipping a fixed number of cache lines
is recognized as a pattern and handled appropriately.
It would be bad for performance if every single cache
miss triggered a hardware prefetch. Random memory
access patterns, for instance to global variables, are quite
common and the resulting prefetches would mostly waste
FSB bandwidth. This is why, to kickstart prefetching,
at least two cache misses are needed. Processors today
all expect there to be more than one stream of mem-
ory accesses. The processor tries to automatically assign
each cache miss to such a stream and, if the threshold
is reached, start hardware prefetching. CPUs today can
keep track of eight to sixteen separate streams for the
higher level caches.
The units responsible for the pattern recognition are asso-
ciated with the respective cache. There can be a prefetch
unit for the L1d and L1i caches. There is most probably
a prefetch unit for the L2 cache and higher. The L2 and
higher prefetch unit is shared with all the other cores and
hyper-threads using the same cache. The number of eight
to sixteen separate streams therefore is quickly reduced.
Prefetching has one big weakness: it cannot cross page
boundaries. The reason should be obvious when one
realizes that the CPUs support demand paging. If the
prefetcher were allowed to cross page boundaries, the
access might trigger an OS event to make the page avail-
able. This by itself can be bad, especially for perfor-
mance. What is worse is that the prefetcher does not
know about the semantics of the program or the OS itself.
It might therefore prefetch pages which, in real life, never
would be requested. That means the prefetcher would
run past the end of the memory region the processor ac-
cessed in a recognizable pattern before. This is not only
possible, it is very likely. If the processor, as a side effect
of a prefetch, triggered a request for such a page the OS
might even be completely thrown off its tracks if such a
request could never otherwise happen.
It is therefore important to realize that, regardless of how
good the prefetcher is at predicting the pattern, the pro-
gram will experience cache misses at page boundaries
unless it explicitly prefetches or reads from the new page.
This is another reason to optimize the layout of data as
described in section 6.2 to minimize cache pollution by
keeping unrelated data out.
Because of this page limitation the processors today do
not have terribly sophisticated logic to recognize prefetch
patterns. With the still predominant 4k page size there is
only so much which makes sense. The address range in
which strides are recognized has been increased over the
years, but it probably does not make much sense to go
beyond the 512 byte window which is often used today.
Currently prefetch units do not recognize non-linear ac-
cess patterns. It is more likely than not that such patterns
are truly random or, at least, sufficiently non-repeating
that it makes no sense to try recognizing them.
If hardware prefetching is accidentally triggered there is
only so much one can do. One possibility is to try to
detect this problem and change the data and/or code lay-
out a bit. This is likely to prove hard. There might be
special localized solutions like using the ud2 instruc-
tion35 on x86 and x86-64 processors. This instruction,
which cannot be executed itself, is used after an indirect
jump instruction; it is used as a signal to the instruction
35 Or non-instruction. It is the recommended undefined opcode.
60
 Version 1.0
 What Every Programmer Should Know About Memory
fetcher that the processor should not waste efforts decod-
ing the following memory since the execution will con-
tinue at a different location. This is a very special sit-
uation, though. In most cases one has to live with this
problem.
It is possible to completely or partially disable hardware
prefetching for the entire processor. On Intel proces-
sors an Model Specific Register (MSR) is used for this
(IA32 MISC ENABLE, bit 9 on many processors; bit 19
disables only the adjacent cache line prefetch). This, in
most cases, has to happen in the kernel since it is a privi-
leged operation. If profiling shows that an important ap-
plication running on a system suffers from bandwidth ex-
haustion and premature cache evictions due to hardware
prefetches, using this MSR is a possibility.
6.3.2
 Software Prefetching
The advantage of hardware prefetching is that programs
do not have to be adjusted. The drawbacks, as just de-
scribed, are that the access patterns must be trivial and
that prefetching cannot happen across page boundaries.
For these reasons we now have more possibilities, soft-
ware prefetching the most important of them. Software
prefetching does require modification of the source code
by inserting special instructions. Some compilers sup-
port pragmas to more or less automatically insert pre-
fetch instructions. On x86 and x86-64 Intel’s convention
for compiler intrinsics to insert these special instructions
is generally used:
#include <xmmintrin.h>
enum _mm_hint
{
_MM_HINT_T0 = 3,
_MM_HINT_T1 = 2,
_MM_HINT_T2 = 1,
_MM_HINT_NTA = 0
};
void _mm_prefetch(void *p,
enum _mm_hint h);
Programs can use the _mm_prefetch intrinsic on any
pointer in the program. Most processors (certainly all
x86 and x86-64 processors) ignore errors resulting from
invalid pointers which makes the life of the programmer
significantly easier. If the passed pointer references valid
memory, the prefetch unit will be instructed to load the
data into cache and, if necessary, evict other data. Unnec-
essary prefetches should definitely be avoided since this
might reduce the effectiveness of the caches and it con-
sumes memory bandwidth (possibly for two cache lines
in case the evicted cache line is dirty).
The different hints to be used with the _mm_prefetch
intrinsic are implementation defined. That means each
processor version can implement them (slightly) differ-
ently. What can generally be said is that _MM_HINT_T0
fetches data to all levels of the cache for inclusive caches
and to the lowest level cache for exclusive caches. If
the data item is in a higher level cache it is loaded into
L1d. The _MM_HINT_T1 hint pulls the data into L2 and
not into L1d. If there is an L3 cache the _MM_HINT_T2
hints can do something similar for it. These are details,
though, which are weakly specified and need to be veri-
fied for the actual processor in use. In general, if the data
is to be used right away using _MM_HINT_T0 is the right
thing to do. Of course this requires that the L1d cache
size is large enough to hold all the prefetched data. If the
size of the immediately used working set is too large, pre-
fetching everything into L1d is a bad idea and the other
two hints should be used.
The fourth hint, _MM_HINT_NTA, allows telling the pro-
cessor to treat the prefetched cache line specially. NTA
stands for non-temporal aligned which we already ex-
plained in section 6.1. The program tells the processor
that polluting caches with this data should be avoided as
much as possible since the data is only used for a short
time. The processor can therefore, upon loading, avoid
reading the data into the lower level caches for inclusive
cache implementations. When the data is evicted from
L1d the data need not be pushed into L2 or higher but,
instead, can be written directly to memory. There might
be other tricks the processor designers can deploy if this
hint is given. The programmer must be careful using this
hint: if the immediate working set size is too large and
forces eviction of a cache line loaded with the NTA hint,
reloading from memory will occur.
1200
1100
1000
t
 n 900
em 800
e l 700
Et 600
s i 500
L/ s 400
el c 300
y 200
C100
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
One Thread
 With Prefetch
Figure 6.7: Average with Prefetch, NPAD=31
Figure 6.7 shows the results of a test using the now fa-
miliar pointer chasing framework. The list is randomly
laid out in memory. The difference to previous test is
that the program actually spends some time at each list
node (about 160 cycles). As we learned from the data in
Figure 3.15, the program’s performance suffers badly as
soon as the working set size is larger than the last-level
cache.
Ulrich Drepper
 Version 1.0
 61
We can now try to improve the situation by issuing pre-
fetch requests ahead of the computation. I.e., in each
round of the loop we prefetch a new element. The dis-
tance between the prefetched node in the list and the node
which is currently worked on must be carefully chosen.
Given that each node is processed in 160 cycles and that
we have to prefetch two cache lines (NPAD=31), a dis-
tance of five list elements is enough.
The results in Figure 6.7 show that the prefetch does in-
deed help. As long as the working set size does not ex-
ceed the size of the last level cache (the machine has
512kB = 219 B of L2) the numbers are identical. The
prefetch instructions do not add a measurable extra bur-
den. As soon as the L2 size is exceeded the prefetching
saves between 50 to 60 cycles, up to 8%. The use of
prefetch cannot hide all the penalties but it does help a
bit.
AMD implements, in their family 10h of the Opteron
line, another instruction: prefetchw. This instruction
has so far no equivalent on the Intel side and is not avail-
able through intrinsics. The prefetchw instruction tells
the CPU to prefetch the cache line into L1 just like the
other prefetch instructions. The difference is that the
cache line is immediately put into ’M’ state. This will be
a disadvantage if no write to the cache line follows later.
If there are one or more writes, they will be accelerated
since the writes do not have to change the cache state–
that happened when the cache line was prefetched. This
is especially important for contended cache lines where a
simple read of a cache line in another processor’s cache
would first change the state to ’S’ in both caches.
Prefetching can have bigger advantages than the mea-
ger 8% we achieved here. But it is notoriously hard
to do right, especially if the same binary is supposed
to perform well on a variety of machines. The perfor-
mance counters provided by the CPU can help the pro-
grammer to analyze prefetches. Events which can be
counted and sampled include hardware prefetches, soft-
ware prefetches, useful/used software prefetches, cache
misses at the various levels, and more. In section 7.1 we
will introduce a number of these events. All these coun-
ters are machine specific.
When analyzing programs one should first look at the
cache misses. When a large source of cache misses is
located one should try to add a prefetch instruction for
the problematic memory accesses. This should be done
in one place at a time. The result of each modifica-
tion should be checked by observing the performance
counters measuring useful prefetch instructions. If those
counters do not increase the prefetch might be wrong, it
is not given enough time to load from memory, or the pre-
fetch evicts memory from the cache which is still needed.
gcc today is able to emit prefetch instructions automati-
cally in one situation. If a loop is iterating over an array
the following option can be used:
-fprefetch-loop-arrays
The compiler will figure out whether prefetching makes
sense and, if so, how far ahead it should look. For small
arrays this can be a disadvantage and, if the size of the
array is not known at compile time, the results might be
worse. The gcc manual warns that the benefits highly
depend on the form of the code and that in some situation
the code might actually run slower. Programmers have to
use this option carefully.
6.3.3
 Special Kind of Prefetch: Speculation
The OOO execution capability of a modern processor al-
lows moving instructions around if they do not conflict
with each other. For instance (using this time IA-64 for
the example):
st8
add
st8
[r4] = 12
r5 = r6, r7;;
[r18] = r5
This code sequence stores 12 at the address specified by
register r4, adds the content of registers r6 and r7 and
stores it in register r5. Finally it stores the sum at the
address specified by register r18. The point here is that
the add instruction can be executed before–or at the same
time as–the first st8 instruction since there is no data
dependency. But what happens if one of the addends has
to be loaded?
st8
ld8
add
st8
[r4] = 12
r6 = [r8];;
r5 = r6, r7;;
[r18] = r5
The extra ld8 instruction loads the value from the ad-
dress specified by the register r8. There is an obvious
data dependency between this load instruction and the
following add instruction (this is the reason for the ;;
after the instruction, thanks for asking). What is criti-
cal here is that the new ld8 instruction–unlike the add
instruction–cannot be moved in front of the first st8.
The processor cannot determine quickly enough during
the instruction decoding whether the store and load con-
flict, i.e., whether r4 and r8 might have same value. If
they do have the same value, the st8 instruction would
determine the value loaded into r6. What is worse, the
ld8 might also bring with it a large latency in case the
load misses the caches. The IA-64 architecture supports
speculative loads for this case:
ld8.a
 r6 = [r8];;
[... other instructions ...]
st8
 [r4] = 12
62
 Version 1.0
 What Every Programmer Should Know About Memory
ld8.c.clr
 r6 = [r8];;
add
 r5 = r6, r7;;
st8
 [r18] = r5
viously be scheduled so that the prefetch thread is pop-
ulating a cache accessed by both threads. There are two
special solutions worth mentioning:
The new ld8.a and ld8.c.clr instructions belong to-
gether and replace the ld8 instruction in the previous
code sequence. The ld8.a instruction is the speculative
load. The value cannot be used directly but the processor
can start the work. At the time when the ld8.c.clr in-
struction is reached the content might have been loaded
already (given there is a sufficient number of instruc-
tions in the gap). The arguments for this instruction must
match that for the ld8.a instruction. If the preceding
st8 instruction does not overwrite the value (i.e., r4 and
r8 are the same), nothing has to be done. The speculative
load does its job and the latency of the load is hidden. If
the store and load do conflict the ld8.c.clr reloads the
value from memory and we end up with the semantics of
a normal ld8 instruction.
Speculative loads are not (yet?) widely used. But as the
example shows it is a very simple yet effective way to
hide latencies. Prefetching is basically equivalent and,
for processors with few registers, speculative loads prob-
ably do not make much sense. Speculative loads have
the (sometimes big) advantage of loading the value di-
rectly into the register and not into the cache line where
it might be evicted again (for instance, when the thread
is descheduled). If speculation is available it should be
used.
6.3.4
 Helper Threads
When one tries to use software prefetching one often runs
into problems with the complexity of the code. If the
code has to iterate over a data structure (a list in our case)
one has to implement two independent iterations in the
same loop: the normal iteration doing the work and the
second iteration, which looks ahead, to use prefetching.
This easily gets complex enough that mistakes are likely.
Furthermore, it is necessary to determine how far to look
ahead. Too little and the memory will not be loaded in
time. Too far and the just loaded data might have been
evicted again. Another problem is that prefetch instruc-
tions, although they do not block and wait for the mem-
ory to be loaded, take time. The instruction has to be
decoded, which might be noticeable if the decoder is too
busy, for instance, due to well written/generated code.
Finally, the code size of the loop is increased. This de-
creases the L1i efficiency. If one tries to avoid parts of
this cost by issuing multiple prefetch requests in a row
(in case the second load does not depend on the result
of the first) one runs into problems with the number of
outstanding prefetch requests.
An alternative approach is to perform the normal oper-
ation and the prefetch completely separately. This can
happen using two normal threads. The threads must ob-
• Use hyper-threads (see page 29) on the same core.
In this case the prefetch can go into L2 (or even
L1d).
• Use “dumber” threads than SMT threads which
can do nothing but prefetch and other simple oper-
ations. This is an option processor manufacturers
might explore.
The use of hyper-threads is particularly intriguing. As
we have seen on page 29, the sharing of caches is a prob-
lem if the hyper-threads execute independent code. If,
instead, one thread is used as a prefetch helper thread
this is not a problem. To the contrary, it is the desired
effect since the lowest level cache is preloaded. Further-
more, since the prefetch thread is mostly idle or wait-
ing for memory, the normal operation of the other hyper-
thread is not disturbed much if it does not have to access
main memory itself. The latter is exactly what the pre-
fetch helper thread prevents.
The only tricky part is to ensure that the helper thread is
not running too far ahead. It must not completely pollute
the cache so that the oldest prefetched values are evicted
again. On Linux, synchronization is easily done using
the futex system call [7] or, at a little bit higher cost,
using the POSIX thread synchronization primitives.
1200
1100
1000
t
 n 900
em 800
e l 700
Et 600
s i 500
L/ s 400
el c 300
y 200
C100
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
One Thread
 With Prefetch
 With Helper Thread
Figure 6.8: Average with Helper Thread, NPAD=31
The benefits of the approach can be seen in Figure 6.8.
This is the same test as in Figure 6.7 only with the ad-
ditional result added. The new test creates an additional
helper thread which runs about 100 list entries ahead and
reads (not only prefetches) all the cache lines of each list
element. In this case we have two cache lines per list el-
ement (NPAD=31 on a 32-bit machine with 64 byte cache
line size).
Ulrich Drepper
 Version 1.0
 63
RAM
CPU
Northbridge
Southbridge
(a) DMA Initiated
Ethernet
RAM
CPU
Northbridge
Southbridge
 Ethernet
(b) DMA and DCA Executed
Figure 6.9: Direct Cache Access
The two threads are scheduled on two hyper-threads of
the same core. The test machine has only one core but
the results should be about the same if there is more than
one core. The affinity functions, which we will introduce
in section 6.4.3, are used to tie the threads down to the
appropriate hyper-thread.
To determine which two (or more) processors the OS
knows are hyper-threads, the NUMA_cpu_level_mask
interface from libNUMA can be used (see Appendix D).
#include <libNUMA.h>
ssize_t NUMA_cpu_level_mask(size_t destsize,
cpu_set_t *dest,
size_t srcsize,
const cpu_set_t*src,
unsigned int level);
This interface can be used to determine the hierarchy of
CPUs as they are connected through caches and mem-
ory. Of interest here is level 1 which corresponds to
hyper-threads. To schedule two threads on two hyper-
threads the libNUMA functions can be used (error han-
dling dropped for brevity):
cpu_set_t self;
NUMA_cpu_self_current_mask(sizeof(self),
&self);
cpu_set_t hts;
NUMA_cpu_level_mask(sizeof(hts), &hts,
sizeof(self), &self, 1);
CPU_XOR(&hts, &hts, &self);
After this code is executed we have two CPU bit sets.
self can be used to set the affinity of the current thread
and the mask in hts can be used to set the affinity of
the helper thread. This should ideally happen before the
thread is created. In section 6.4.3 we will introduce the
interface to set the affinity. If there is no hyper-thread
available the NUMA_cpu_level_mask function will re-
turn 1. This can be used as a sign to avoid this optimiza-
tion.
The result of this benchmark might be surprising (or per-
haps not). If the working set fits into L2, the overhead
of the helper thread reduces the performance by between
10% and 60% (mostly at the lower end, ignore the small-
est working set sizes again, the noise is too high). This
should be expected since, if all the data is already in the
L2 cache, the prefetch helper thread only uses system re-
sources without contributing to the execution.
Once the L2 size is not sufficient is exhausted the pic-
ture changes, though. The prefetch helper thread helps
to reduce the runtime by about 25%. We still see a ris-
ing curve simply because the prefetches cannot be pro-
cessed fast enough. The arithmetic operations performed
by the main thread and the memory load operations of
the helper thread do complement each other, though. The
resource collisions are minimal which causes this syner-
gistic effect.
The results of this test should be transferable to many
other situations. Hyper-threads, often not useful due to
cache pollution, shine in these situations and should be
taken advantage of. The NUMA library introduced in
Appendix D makes finding thread siblings very easy (see
the example in that appendix). If the library is not avail-
able the sys file system allows a program to find the
thread siblings (see the thread_siblings column in
Table 5.3). Once this information is available the pro-
gram just has to define the affinity of the threads and
then run the loop in two modes: normal operation and
prefetching. The amount of memory prefetched should
depend on the size of the shared cache. In this example
the L2 size is relevant and the program can query the size
using
sysconf(_SC_LEVEL2_CACHE_SIZE)
Whether or not the progress of the helper thread must be
restricted depends on the program. In general it is best to
make sure there is some synchronization since schedul-
ing details could otherwise cause significant performance
degradations.
6.3.5
 Direct Cache Access
One sources of cache misses in a modern OS is the han-
dling of incoming data traffic. Modern hardware, like
Network Interface Cards (NICs) and disk controllers, has
the ability to write the received or read data directly into
64
 Version 1.0
 What Every Programmer Should Know About Memory
memory without involving the CPU. This is crucial for
the performance of the devices we have today, but it also
causes problems. Assume an incoming packet from a
network: the OS has to decide how to handle it by look-
ing at the header of the packet. The NIC places the packet
into memory and then notifies the processor about the ar-
rival. The processor has no chance to prefetch the data
since it does not know when the data will arrive, and
maybe not even where exactly it will be stored. The re-
sult is a cache miss when reading the header.
Intel has added technology in their chipsets and CPUs
to alleviate this problem [14]. The idea is to populate
the cache of the CPU which will be notified about the
incoming packet with the packet’s data. The payload of
the packet is not critical here, this data will, in general, be
handled by higher-level functions, either in the kernel or
at user level. The packet header is used to make decisions
about the way the packet has to be handled and so this
data is needed immediately.
The network I/O hardware already has DMA to write the
packet. That means it communicates directly with the
memory controller which potentially is integrated in the
Northbridge. Another side of the memory controller is
the interface to the processors through the FSB (assum-
ing the memory controller is not integrated into the CPU
itself).
The idea behind Direct Cache Access (DCA) is to ex-
tend the protocol between the NIC and the memory con-
troller. In Figure 6.9 the first figure shows the beginning
of the DMA transfer in a regular machine with North-
and Southbridge. The NIC is connected to (or is part of)
the Southbridge. It initiates the DMA access but pro-
vides the new information about the packet header which
should be pushed into the processor’s cache.
The traditional behavior would be, in step two, to simply
complete the DMA transfer with the connection to the
memory. For the DMA transfers with the DCA flag set
the Northbridge additionally sends the data on the FSB
with a special, new DCA flag. The processor always
snoops the FSB and, if it recognizes the DCA flag, it tries
to load the data directed to the processor into the lowest
cache. The DCA flag is, in fact, a hint; the processor can
freely ignore it. After the DMA transfer is finished the
processor is signaled.
The OS, when processing the packet, first has to deter-
mine what kind of packet it is. If the DCA hint is not
ignored, the loads the OS has to perform to identify the
packet most likely hit the cache. Multiply this saving of
hundreds of cycles per packet with tens of thousands of
packets which can be processed per second, and the sav-
ings add up to very significant numbers, especially when
it comes to latency.
Without the integration of I/O hardware (a NIC in this
case), chipset, and CPUs such an optimization is not pos-
sible. It is therefore necessary to make sure to select the
platform wisely if this technology is needed.
6.4
 Multi-Thread Optimizations
When it comes to multi-threading, there are three differ-
ent aspects of cache use which are important:
•••Concurrency
Atomicity
Bandwidth
These aspects also apply to multi-process situations but,
because multiple processes are (mostly) independent, it
is not so easy to optimize for them. The possible multi-
process optimizations are a subset of those available for
the multi-thread scenario. So we will deal exclusively
with the latter here.
In this context concurrency refers to the memory effects a
process experiences when running more than one thread
at a time. A property of threads is that they all share
the same address space and, therefore, can all access the
same memory. In the ideal case, the memory regions
used by the threads most of the time are distinct, in which
case those threads are coupled only lightly (common in-
put and/or output, for instance). If more than one thread
uses the same data, coordination is needed; this is when
atomicity comes into play. Finally, depending on the
machine architecture, the available memory and inter-
processor bus bandwidth available to the processors is
limited. We will handle these three aspects separately
in the following sections–although they are, of course,
closely linked.
6.4.1
 Concurrency Optimizations
Initially, in this section, we will discuss two separate is-
sues which actually require contradictory optimizations.
A multi-threaded application uses common data in some
of its threads. Normal cache optimization calls for keep-
ing data together so that the footprint of the application
is small, thus maximizing the amount of memory which
fits into the caches at any one time.
There is a problem with this approach, though: if mul-
tiple threads write to a memory location, the cache line
must be in ‘E’ (exclusive) state in the L1d of each respec-
tive core. This means that a lot of RFO messages are sent,
in the worst case one for each write access. So a normal
write will be suddenly very expensive. If the same mem-
ory location is used, synchronization is needed (maybe
through the use of atomic operations, which is handled
in the next section). The problem is also visible, though,
when all the threads are using different memory locations
and are supposedly independent.
Figure 6.10 shows the results of this “false sharing”. The
test program (shown in section A.3) creates a number of
threads which do nothing but increment a memory loca-
tion (500 million times). The measured time is from the
Ulrich Drepper
 Version 1.0
 65
18
16
14
)
 12
ces 10
(e 8
mi T 6
4
2
0
1
 2
 3
 4
Number of Threads
Figure 6.10: Concurrent Cache Line Access Overhead
program start until the program finishes after waiting for
the last thread. The threads are pinned to individual pro-
cessors. The machine has four P4 processors. The blue
values represent runs where the memory allocations as-
signed to each thread are on separate cache lines. The
red part is the penalty occurred when the locations for
the threads are moved to just one cache line.
The blue measurements (time needed when using indi-
vidual cache lines) match what one would expect. The
program scales without penalty to many threads. Each
processor keeps its cache line in its own L1d and there
are no bandwidth issues since not much code or data has
to be read (in fact, it is all cached). The measured slight
increase is really system noise and probably some pre-
fetching effects (the threads use sequential cache lines).
The measured overhead, computed by dividing the time
needed when using one single cache line versus a sep-
arate cache line for each thread, is 390%, 734%, and
1,147% respectively. These large numbers might be sur-
prising at first sight but, when thinking about the cache
interaction needed, it should be obvious. The cache line
is pulled from one processor’s cache just after it has fin-
ished writing to the cache line. All processors, except the
one which has the cache line at any given moment, are
delayed and cannot do anything. Each additional proces-
sor will just cause more delays.
It is clear from these measurements that this scenario
must be avoided in programs. Given the huge penalty,
this problem is, in many situations, obvious (profiling
will show the code location, at least) but there is a pitfall
with modern hardware. Figure 6.11 shows the equivalent
measurements when running the code on a single pro-
cessor, quad core machine (Intel Core 2 QX 6700). Even
with this processor’s two separate L2s the test case does
not show any scalability issues. There is a slight over-
head when using the same cache line more than once
but it does not increase with the number of cores.36 If
36I cannot explain the lower number when all four cores are used but
it is reproducible.
2
)
ces(e 1
miT0
1
 2
 3
 4
Number of Threads
Figure 6.11: Overhead, Quad Core
more than one of these processors were used we would,
of course, see results similar to those in Figure 6.10. De-
spite the increasing use of multi-core processors, many
machines will continue to use multiple processors and,
therefore, it is important to handle this scenario correctly,
which might mean testing the code on real SMP ma-
chines.
There is a very simple “fix” for the problem: put every
variable on its own cache line. This is where the conflict
with the previously mentioned optimization comes into
play, specifically, the footprint of the application would
increase a lot. This is not acceptable; it is therefore nec-
essary to come up with a more intelligent solution.
What is needed is to identify which variables are used by
only one thread at a time, those used by only one thread
ever, and maybe those which are contested at times. Dif-
ferent solutions for each of these scenarios are possible
and useful. The most basic criterion for the differentia-
tion of variables is: are they ever written to and how often
does this happen.
Variables which are never written to and those which are
only initialized once are basically constants. Since RFO
messages are only needed for write operations, constants
can be shared in the cache (‘S’ state). So, these vari-
ables do not have to be treated specially; grouping them
together is fine. If the programmer marks the variables
correctly with const, the tool chain will move the vari-
ables away from the normal variables into the .rodata
(read-only data) or .data.rel.ro (read-only after relo-
cation) section37 No other special action is required. If,
for some reason, variables cannot be marked correctly
with const, the programmer can influence their place-
ment by assigning them to a special section.
When the linker constructs the final binary, it first ap-
pends the sections with the same name from all input
files; those sections are then arranged in an order deter-
mined by the linker script. This means that, by mov-
ing all variables which are basically constant but are not
marked as such into a special section, the programmer
can group all of those variables together. There will not
be a variable which is often written to between them. By
aligning the first variable in that section appropriately,
37Sections, identified by their names are the atomic units containing
code and data in an ELF file.
66
 Version 1.0
 What Every Programmer Should Know About Memory
it is possible to guarantee that no false sharing happens.
Assume this little example:
int foo = 1;
int bar __attribute__((section(".data.ro"))) = 2;
int baz = 3;
int xyzzy __attribute__((section(".data.ro"))) = 4;
If compiled, this input file defines four variables. The in-
teresting part is that the variables foo and baz, and bar
and xyzzy are grouped together respectively. Without
the attribute definitions the compiler would allocate all
four variables in the sequence in which they are defined
in the source code the a section named .data.38 With the
code as-is the variables bar and xyzzy are placed in a
section named .data.ro. The section name .data.ro
is more or less arbitrary. A prefix of .data. guarantees
that the GNU linker will place the section together with
the other data sections.
The same technique can be applied to separate out vari-
ables which are mostly read but occasionally also written
to. Simply choose a different section name. This sepa-
ration seems to make sense in some cases like the Linux
kernel.
If a variable is only ever used by one thread, there is an-
other way to specify the variable. In this case it is possi-
ble and useful to use thread-local variables (see [8]). The
C and C++ language in gcc allow variables to be defined
as per-thread using the __thread keyword.
int foo = 1;
__thread int bar = 2;
int baz = 3;
__thread int xyzzy = 4;
Another drawback of using thread-local storage (TLS)
is that, if the use of the variable shifts over to another
thread, the current value of the variable in the old thread
is not available to new thread. Each thread’s copy of the
variable is distinct. Often this is not a problem at all and,
if it is, the shift over to the new thread needs coordina-
tion, at which time the current value can be copied.
A bigger problem is possible waste of resources. If only
one thread ever uses the variable at any one time, all
threads have to pay a price in terms of memory. If a
thread does not use any TLS variables, the lazy alloca-
tion of the TLS memory area prevents this from being a
problem (except for TLS in the application itself). If a
thread uses just one TLS variable in a DSO, the memory
for all the other TLS variables in this object will be allo-
cated, too. This could potentially add up if TLS variables
are used on a large scale.
In general the best advice which can be given is
1. Separate at least read-only (after initialization) and
read-write variables. Maybe extend this separation
to read-mostly variables as a third category.
2. Group read-write variables which are used together
into a structure. Using a structure is the only way
to ensure the memory locations for all of those
variables are close together in a way which is trans-
lated consistently by all gcc versions..
3. Move read-write variables which are often written
to by different threads onto their own cache line.
This might mean adding padding at the end to fill
a remainder of the cache line. If combined with
step 2, this is often not really wasteful. Extending
the example above, we might end up with code as
follows (assuming bar and xyzzy are meant to be
used together):
The variables bar and xyzzy are not allocated in the
normal data segment; instead each thread has its own
separate area where such variables are stored. The vari-
ables can have static initializers. All thread-local vari-
ables are addressable by all other threads but, unless a
thread passes a pointer to a thread-local variable to those
other threads, there is no way the other threads can find
that variable. Due to the variable being thread-local, false
sharing is not a problem–unless the program artificially
creates a problem. This solution is easy to set up (the
compiler and linker do all the work), but it has its cost.
When a thread is created, it has to spend some time on
setting up the thread-local variables, which requires time
and memory. In addition, addressing thread-local vari-
ables is usually more expensive than using global or auto-
matic variables (see [8] for explanations of how the costs
are minimized automatically, if possible).
38This is not guaranteed by the ISO C standard but it is how gcc
works.
int foo = 1;
int baz = 3;
struct {
struct al1 {
int bar;
int xyzzy;
};
char pad[CLSIZE - sizeof(struct al1)];
} rwstruct __attribute__((aligned(CLSIZE))) =
{ { .bar = 2, .xyzzy = 4 } };
Some code changes are needed (references to bar
have to be replaced with rwstruct.bar, likewise
for xyzzy) but that is all. The compiler and linker
do all the rest.39
4. If a variable is used by multiple threads, but every
use is independent, move the variable into TLS.
39So far this code has to be compiled with -fms-extensions on
the command line.
Ulrich Drepper
 Version 1.0
 67
for (i = 0; i < N; ++i)
 for (i = 0; i < N; ++i)
 for (i = 0; i < N; ++i) {
__sync_add_and_fetch(&var,1);
 __sync_fetch_and_add(&var,1);
 long v, n;
do {
v = var;
n = v + 1;
} while (!__sync_bool_compare_and_swap(&var,
v,n));
}
1. Add and Read Result
 2. Add and Return Old Value
 3. Atomic Replace with New Value
Figure 6.12: Atomic Increment in a Loop
6.4.2
 Atomicity Optimizations
If multiple threads modify the same memory location
concurrently, processors do not guarantee any specific
result. This is a deliberate decision made to avoid costs
which are unnecessary in 99.999% of all cases. For in-
stance, if a memory location is in the ‘S’ state and two
threads concurrently have to increment its value, the exe-
cution pipeline does not have to wait for the cache line to
be available in the ‘E’ state before reading the old value
from the cache to perform the addition. Instead it reads
the value currently in the cache and, once the cache line
is available in state ‘E’, the new value is written back.
The result is not as expected if the two cache reads in the
two threads happen simultaneously; one addition will be
lost.
For situations where concurrent operations can happen,
processors provide atomic operations. These atomic op-
erations would, for instance, not read the old value un-
til it is clear that the addition could be performed in a
way that the addition to the memory location appears as
atomic. In addition to waiting for other cores and proces-
sors, some processors even signal atomic operations for
specific addresses to other devices on the motherboard.
All this makes atomic operations slower.
Processor vendors decided to provide different sets of
atomic operations. Early RISC processors, in line with
the ‘R’ for reduced, provided very few atomic operations,
sometimes only an atomic bit set and test.40 At the other
end of the spectrum, we have x86 and x86-64 which pro-
vide a large number of atomic operations. The generally
available atomic operations can be categorized in four
classes:
Bit Test These operations set or clear a bit atomically
and return a status indicating whether the bit was
set before or not.
Load Lock/Store Conditional (LL/SC)41 The LL/SC
operations work as a pair where the special load
instruction is used to start an transaction and the
final store will only succeed if the location has not
been modified in the meantime. The store oper-
ation indicates success or failure, so the program
can repeat its efforts if necessary.
40 HP Parisc still does not provide more. . .
41 Some people use “linked” instead of “lock”, it is all the same.
Compare-and-Swap (CAS) This is a ternary operation
which writes a value provided as a parameter into
an address (the second parameter) only if the cur-
rent value is the same as the third parameter value;
Atomic Arithmetic These operations are only available
on x86 and x86-64, which can perform arithmetic
and logic operations on memory locations. These
processors have support for non-atomic versions of
these operations but RISC architectures do not. So
it is no wonder that their availability is limited.
An architecture supports either the LL/SC or the CAS in-
struction, not both. Both approaches are basically equiv-
alent; they allow the implementation of atomic arithmetic
operations equally well, but CAS seems to be the pre-
ferred method these days. All other operations can be
indirectly implemented using it. For instance, an atomic
addition:
int curval;
int newval;
do {
curval = var;
newval = curval + addend;
} while (CAS(&var, curval, newval));
The result of the CAS call indicates whether the operation
succeeded or not. If it returns failure (non-zero value),
the loop is run again, the addition is performed, and the
CAS call is tried again. This repeats until it is success-
ful. Noteworthy about the code is that the address of the
memory location has to be computed in two separate in-
structions.42 For LL/SC the code looks about the same:
int curval;
int newval;
do {
curval = LL(var);
newval = curval + addend;
} while (SC(var, newval));
42 The CAS opcode on x86 and x86-64 can avoid the load of the value
in the second and later iterations but, on this platform, we can write the
atomic addition in a simpler way, with a single addition opcode.
68
 Version 1.0
 What Every Programmer Should Know About Memory
Here we have to use a special load instruction (LL) and
we do not have to pass the current value of the memory
location to SC since the processor knows if the memory
location has been modified in the meantime.
The big differentiators are x86 and x86-64, where we
have the atomic operations and, here, it is important to
select the proper atomic operation to achieve the best re-
sult. Figure 6.12 shows three different ways to imple-
ment an atomic increment operation. All three produce
different code on x86 and x86-64 while the code might
be identical on other architectures. There are huge per-
formance differences. The following table shows the ex-
ecution time for 1 million increments by four concurrent
threads. The code uses the built-in primitives of gcc (_-
_sync_*).
1. Exchange Add
 2. Add Fetch
 3. CAS
0.23s
 0.21s
 0.73s
The first two numbers are similar; we see that returning
the old value is a little bit faster. The important piece of
information is the highlighted field, the cost when using
CAS. It is, unsurprisingly, a lot more expensive. There
are several reasons for this: 1. there are two memory op-
erations, 2. the CAS operation by itself is more compli-
cated and requires even conditional operation, and 3. the
whole operation has to be done in a loop in case two con-
current accesses cause a CAS call to fail.
Now a reader might ask a question: why would some-
body use the complicated and longer code which uti-
lizes CAS? The answer to this is: the complexity is usu-
ally hidden. As mentioned before, CAS is currently the
unifying atomic operation across all interesting architec-
tures. So some people think it is sufficient to define all
atomic operations in terms of CAS. This makes programs
simpler. But as the numbers show, the results can be ev-
erything but optimal. The memory handling overhead of
the CAS solution is huge. The following illustrates the
execution of just two threads, each on its own core.
Thread #1
v = var
n = v + 1
CAS(var)
Thread #2
v = var
n = v + 1
CAS(var)
var Cache State
‘E’ on Proc 1
‘S’ on Proc 1+2
‘E’ on Proc 1
‘E’ on Proc 2
We see that, within this short period of execution, the
cache line status changes at least three times; two of the
changes are RFOs. Additionally, the second CAS will
fail, so that thread has to repeat the whole operation. Dur-
ing that operation the same can happen again.
In contrast, when the atomic arithmetic operations are
used, the processor can keep the load and store opera-
tions needed to perform the addition (or whatever) to-
gether. It can ensure that concurrently-issued cache line
requests are blocked until the atomic operation is done.
Each loop iteration in the example therefore results in, at
most, one RFO cache request and nothing else.
What all this means is that it is crucial to define the ma-
chine abstraction at a level at which atomic arithmetic
and logic operations can be utilized. CAS should not be
universally used as the unification mechanism.
For most processors, the atomic operations are, by them-
selves, always atomic. One can avoid them only by pro-
viding completely separate code paths for the case when
atomicity is not needed. This means more code, a con-
ditional, and further jumps to direct execution appropri-
ately.
For x86 and x86-64 the situation is different: the same
instructions can be used in both atomic and non-atomic
ways. To make them atomic, a special prefix for the in-
struction is used: the lock prefix. This opens the door
for atomic operations to avoid the high costs if the atom-
icity requirement in a given situation is not needed. Code
in libraries, for example, which always has to be thread-
safe if needed, can benefit from this. No information is
needed when writing the code, the decision can be made
at runtime. The trick is to jump over the lock prefix.
This trick applies to all the instructions which the x86
and x86-64 processor allow to prefix with lock.
cmpl $0, multiple_threads
je
 1f
lock
1:
 add $1, some_var
If this assembler code appears cryptic, do not worry, it
is simple. The first instruction checks whether a vari-
able is zero or not. Nonzero in this case indicates that
more than one thread is running. If the value is zero,
the second instruction jumps to label 1. Otherwise, the
next instruction is executed. This is the tricky part. If
the je instruction does not jump, the add instruction is
executed with the lock prefix. Otherwise it is executed
without the lock prefix.
Adding a potentially expensive operation like a condi-
tional jump (expensive in case the branch prediction is
wrong) seems to be counter productive. Indeed it can be:
if multiple threads are running most of the time, the per-
formance is further decreased, especially if the branch
prediction is not correct. But if there are many situa-
tions where only one thread is in use, the code is sig-
nificantly faster. The alternative of using an if-then-else
construct introduces an additional unconditional jump in
both cases which can be slower. Given that an atomic
operation costs on the order of 200 cycles, the cross-
over point for using the trick (or the if-then-else block)
is pretty low. This is definitely a technique to be kept in
mind. Unfortunately this means gcc’s __sync_* primi-
tives cannot be used.
Ulrich Drepper
 Version 1.0
 69
6.4.3
 Bandwidth Considerations
When many threads are used, and they do not cause cache
contention by using the same cache lines on different
cores, there still are potential problems. Each proces-
sor has a maximum bandwidth to the memory which is
shared by all cores and hyper-threads on that processor.
Depending on the machine architecture (e.g., the one in
Figure 2.1), multiple processors might share the same
bus to memory or the Northbridge.
The processor cores themselves run at frequencies where,
at full speed, even in perfect conditions, the connection
to the memory cannot fulfill all load and store requests
without waiting. Now, further divide the available band-
width by the number of cores, hyper-threads, and pro-
cessors sharing a connection to the Northbridge and sud-
denly parallelism becomes a big problem. Efficient pro-
grams may be limited in their performance by the avail-
able memory bandwidth.
Figure 3.32 shows that increasing the FSB speed of a pro-
cessor can help a lot. This is why, with growing numbers
of cores on a processor, we will also see an increase in
the FSB speed. Still, this will never be enough if the
program uses large working sets and it is sufficiently op-
timized. Programmers have to be prepared to recognize
problems due to limited bandwidth.
The performance measurement counters of modern pro-
cessors allow the observation of FSB contention. On
Core 2 processors the NUS_BNR_DRV event counts the
number of cycles a core has to wait because the bus is
not ready. This indicates that the bus is highly used and
loads from or stores to main memory take even longer
than usual. The Core 2 processors support more events
which can count specific bus actions like RFOs or the
general FSB utilization. The latter might come in handy
when investigating the possibility of scalability of an ap-
plication during development. If the bus utilization rate
is already close to 1.0 then the scalability opportunities
are minimal.
If a bandwidth problem is recognized, there are several
things which can be done. They are sometimes con-
tradictory so some experimentation might be necessary.
One solution is to buy faster computers, if there are some
available. Getting more FSB speed, faster RAM mod-
ules, and possibly memory local to the processor, can–
and probably will–help. It can cost a lot, though. If the
program in question is only needed on one (or a few ma-
chines) the one-time expense for the hardware might cost
less than reworking the program. In general, though, it is
better to work on the program.
After optimizing the program code itself to avoid cache
misses, the only option left to achieve better bandwidth
utilization is to place the threads better on the available
cores. By default, the scheduler in the kernel will assign
a thread to a processor according to its own policy. Mov-
ing a thread from one core to another is avoided when
possible. The scheduler does not really know anything
about the workload, though. It can gather information
from cache misses etc but this is not much help in many
situations.
Core 1
 Core 2
Cache
Core 3
 Core 4
Cache
Memory
Figure 6.13: Inefficient Scheduling
One situation which can cause big memory bus usage is
when two threads are scheduled on different processors
(or cores in different cache domains) and they use the
same data set. Figure 6.13 shows such a situation. Core 1
and 3 access the same data (indicated by the same color
for the access indicator and the memory area). Similarly
core 2 and 4 access the same data. But the threads are
scheduled on different processors. This means each data
set has to be read twice from memory. This situation can
be handled better.
Core 1
 Core 2
Cache
Core 3
 Core 4
Cache
Memory
Figure 6.14: Efficient Scheduling
In Figure 6.14 we see how it should ideally look like.
Now the total cache size in use is reduced since now
core 1 and 2 and core 3 and 4 work on the same data.
The data sets have to be read from memory only once.
This is a simple example but, by extension, it applies to
many situations. As mentioned before, the scheduler in
the kernel has no insight into the use of data, so the pro-
grammer has to ensure that scheduling is done efficiently.
There are not many kernel interfaces available to commu-
nicate this requirement. In fact, there is only one: defin-
ing thread affinity.
Thread affinity means assigning a thread to one or more
cores. The scheduler will then choose among those cores
(only) when deciding where to run the thread. Even if
other cores are idle they will not be considered. This
might sound like a disadvantage, but it is the price one
has to pay. If too many threads exclusively run on a set
of cores the remaining cores might mostly be idle and
there is nothing one can do except change the affinity.
By default threads can run on any core.
There are a number of interfaces to query and change the
affinity of a thread:
70
 Version 1.0
 What Every Programmer Should Know About Memory
#define _GNU_SOURCE
#include <sched.h>
int sched_setaffinity(pid_t pid, size_t size,
const cpu_set_t *cpuset);
int sched_getaffinity(pid_t pid, size_t size,
cpu_set_t *cpuset);
#define _GNU_SOURCE
#include <sched.h>
#define CPU_ALLOC_SIZE(count)
#define CPU_ALLOC(count)
#define CPU_FREE(cpuset)
These two interfaces are meant to be used for single-
threaded code. The pid argument specifies which pro-
cess’s affinity should be changed or determined. The
caller obviously needs appropriate privileges to do this.
The second and third parameter specify the bitmask for
the cores. The first function requires the bitmask to be
filled in so that it can set the affinity. The second fills
in the bitmask with the scheduling information of the se-
lected thread. The interfaces are declared in <sched.h>.
The cpu_set_t type is also defined in that header, along
with a number of macros to manipulate and use objects
of this type.
#define _GNU_SOURCE
#include <sched.h>
#define CPU_SETSIZE
#define CPU_SET(cpu, cpusetp)
#define CPU_CLR(cpu, cpusetp)
#define CPU_ZERO(cpusetp)
#define CPU_ISSET(cpu, cpusetp)
#define CPU_COUNT(cpusetp)
CPU_SETSIZE specifies how many CPUs can be rep-
resented in the data structure. The other three macros
manipulate cpu_set_t objects. To initialize an object
CPU_ZERO should be used; the other two macros should
be used to select or deselect individual cores. CPU_-
ISSET tests whether a specific processor is part of the
set. CPU_COUNT returns the number of cores selected in
the set. The cpu_set_t type provide a reasonable de-
fault value for the upper limit on the number of CPUs.
Over time it certainly will prove too small; at that point
the type will be adjusted. This means programs always
have to keep the size in mind. The above convenience
macros implicitly handle the size according to the defi-
nition of cpu_set_t. If more dynamic size handling is
needed an extended set of macros should be used:
#define _GNU_SOURCE
#include <sched.h>
#define CPU_SET_S(cpu, setsize, cpusetp)
#define CPU_CLR_S(cpu, setsize, cpusetp)
#define CPU_ZERO_S(setsize, cpusetp)
#define CPU_ISSET_S(cpu, setsize, cpusetp)
#define CPU_COUNT_S(setsize, cpusetp)
These interfaces take an additional parameter with the
size. To be able to allocate dynamically sized CPU sets
three macros are provided:
The return value of the CPU_ALLOC_SIZE macro is the
number of bytes which have to be allocated for a cpu_-
set_t structure which can handle count CPUs. To al-
locate such a block the CPU_ALLOC macro can be used.
The memory allocated this way must be freed with a call
to CPU_FREE. These macros will likely use malloc and
free behind the scenes but this does not necessarily have
to remain this way.
Finally, a number of operations on CPU set objects are
defined:
#define _GNU_SOURCE
#include <sched.h>
#define CPU_EQUAL(cpuset1, cpuset2)
#define CPU_AND(destset, cpuset1, cpuset2)
#define CPU_OR(destset, cpuset1, cpuset2)
#define CPU_XOR(destset, cpuset1, cpuset2)
#define CPU_EQUAL_S(setsize, cpuset1, cpuset2)
#define CPU_AND_S(setsize, destset, cpuset1,
cpuset2)
#define CPU_OR_S(setsize, destset, cpuset1,
cpuset2)
#define CPU_XOR_S(setsize, destset, cpuset1,
cpuset2)
These two sets of four macros can check two sets for
equality and perform logical AND, OR, and XOR op-
erations on sets. These operations come in handy when
using some of the libNUMA functions (see Appendix D).
A process can determine on which processor it is cur-
rently running using the sched_getcpu interface:
#define _GNU_SOURCE
#include <sched.h>
int sched_getcpu(void);
The result is the index of the CPU in the CPU set. Due
to the nature of scheduling this number cannot always be
100% correct. The thread might have been moved to a
different CPU between the time the result was returned
and when the thread returns to userlevel. Programs al-
ways have to take this possibility of inaccuracy into ac-
count. More important is, in any case, the set of CPUs the
thread is allowed to run on. This set can be retrieved us-
ing sched_getaffinity. The set is inherited by child
threads and processes. Threads cannot rely on the set to
be stable over the lifetime. The affinity mask can be set
from the outside (see the pid parameter in the prototypes
Ulrich Drepper
 Version 1.0
 71
above); Linux also supports CPU hot-plugging which
means CPUs can vanish from the system–and, therefore,
also from the affinity CPU set.
In multi-threaded programs, the individual threads of-
ficially have no process ID as defined by POSIX and,
therefore, the two functions above cannot be used. In-
stead <pthread.h> declares four different interfaces:
#define _GNU_SOURCE
#include <pthread.h>
int pthread_setaffinity_np(pthread_t th,
size_t size,
const cpu_set_t *cpuset);
int pthread_getaffinity_np(pthread_t th,
size_t size,
cpu_set_t *cpuset);
int pthread_attr_setaffinity_np(
pthread_attr_t *at,
size_t size,
const cpu_set_t *cpuset);
int pthread_attr_getaffinity_np(
pthread_attr_t *at,
size_t size,
cpu_set_t *cpuset);
The first two interfaces are basically equivalent to the two
we have already seen, except that they take a thread han-
dle in the first parameter instead of a process ID. This
allows addressing individual threads in a process. It also
means that these interfaces cannot be used from another
process, they are strictly for intra-process use. The third
and fourth interfaces use a thread attribute. These at-
tributes are used when creating a new thread. By setting
the attribute, a thread can be scheduled from the start on a
specific set of CPUs. Selecting the target processors this
early–instead of after the thread already started–can be
of advantage on many different levels, including (and es-
pecially) memory allocation (see NUMA in section 6.5).
Speaking of NUMA, the affinity interfaces play a big role
in NUMA programming, too. We will come back to that
case shortly.
So far, we have talked about the case where the working
set of two threads overlaps such that having both threads
on the same core makes sense. The opposite can be true,
too. If two threads work on separate data sets, having
them scheduled on the same core can be a problem. Both
threads fight for the same cache, thereby reducing each
others effective use of the cache. Second, both data sets
have to be loaded into the same cache; in effect this in-
creases the amount of data that has to be loaded and,
therefore, the available bandwidth is cut in half.
The solution in this case is to set the affinity of the threads
so that they cannot be scheduled on the same core. This is
the opposite from the previous situation, so it is important
to understand the situation one tries to optimize before
making any changes.
Optimizing for cache sharing to optimize bandwidth is in
reality an aspect of NUMA programming which is cov-
ered in the next section. One only has to extend the no-
tion of “memory” to the caches. This will become ever
more important once the number of levels of cache in-
creases. For this reason, a solution to multi-core schedul-
ing is available in the NUMA support library. See the
code samples in Appendix D for ways to determine the
affinity masks without hardcoding system details or div-
ing into the depth of the /sys filesystem.
6.5
 NUMA Programming
For NUMA programming everything said so far about
cache optimizations applies as well. The differences only
start below that level. NUMA introduces different costs
when accessing different parts of the address space. With
uniform memory access we can optimize to minimize
page faults (see section 7.5) but that is about it. All pages
are created equal.
NUMA changes this. Access costs can depend on the
page which is accessed. Differing access costs also in-
crease the importance of optimizing for memory page
locality. NUMA is inevitable for most SMP machines
since both Intel with CSI (for x86,x86-64, and IA-64)
and AMD (for Opteron) use it. With an increasing num-
ber of cores per processor we are likely to see a sharp
reduction of SMP systems being used (at least outside
data centers and offices of people with terribly high CPU
usage requirements). Most home machines will be fine
with just one processor and hence no NUMA issues. But
this a) does not mean programmers can ignore NUMA
and b) it does not mean there are not related issues.
If one thinks about generalizations to NUMA one quickly
realizes the concept extends to processor caches as well.
Two threads on cores using the same cache will collabo-
rate faster than threads on cores not sharing a cache. This
is not a fabricated case:
• early dual-core processors had no L2 sharing.
• Intel’s Core 2 QX 6700 and QX 6800 quad core
chips, for instance, have two separate L2 caches.
• as speculated early, with more cores on a chip and
the desire to unify caches, we will have more levels
of caches.
Caches form their own hierarchy; placement of threads
on cores becomes important for sharing (or not) of the
various caches. This is not very different from the prob-
lems NUMA is facing and, therefore, the two concepts
can be unified. Even people only interested in non-SMP
machines should therefore read this section.
In section 5.3 we have seen that the Linux kernel pro-
vides a lot of information which is useful–and needed–in
NUMA programming. Collecting this information is not
that easy, though. The currently available NUMA library
72
 Version 1.0
 What Every Programmer Should Know About Memory
on Linux is wholly inadequate for this purpose. A much
more suitable version is currently under construction by
the author.
The existing NUMA library, libnuma, part of the nu-
mactl package, provides no access to system architecture
information. It is only a wrapper around the available
system calls together with some convenience interfaces
for commonly used operations. The system calls avail-
able on Linux today are:
mbind Select binding of specified memory pages.
set_mempolicy Set the default memory binding pol-
icy.
get_mempolicy Get the default memory binding pol-
icy.
migrate_pages Migrate all pages of a process on a
given set of nodes to a different set of nodes.
move_pages Move selected pages to given node or re-
quest node information about pages.
These interfaces are declared in the <numaif.h> header
which comes along with the libnuma library. Before we
go into more details we have to understand the concept
of memory policies.
6.5.1
 Memory Policy
The idea behind defining a memory policy is to allow
existing code to work reasonably well in a NUMA en-
vironment without major modifications. The policy is
inherited by child processes, which makes it possible to
use the numactl tool. This tool can be used to, among
other things, start a program with a given policy.
The Linux kernel supports the following policies:
MPOL_BIND Memory is allocated only from the given
set of nodes. If this is not possible allocation fails.
MPOL_PREFERRED Memory is preferably allocated from
the given set of nodes. If this fails memory from
other nodes is considered.
MPOL_INTERLEAVE Memory is allocated equally from
the specified nodes. The node is selected either by
the offset in the virtual memory region for VMA-
based policies, or through a free-running counter
for task-based policies.
MPOL_DEFAULT Choose the allocation based on the de-
fault for the region.
This list seems to recursively define policies. This is half
true. In fact, memory policies form a hierarchy (see Fig-
ure 6.15). If an address is covered by a VMA policy then
this policy is used. A special kind of policy is used for
System Default Policy
Task Policy
VMA Policy
 ShMem Policy
Figure 6.15: Memory Policy Hierarchy
shared memory segments. If no policy for the specific
address is present, the task’s policy is used. If this is also
not present the system’s default policy is used.
The system default is to allocate memory local to the
thread requesting the memory. No task and VMA poli-
cies are provided by default. For a process with multiple
threads the local node is the “home” node, the one which
first ran the process. The system calls mentioned above
can be used to select different policies.
6.5.2
 Specifying Policies
The set_mempolicy call can be used to set the task pol-
icy for the current thread (task in kernel-speak). Only the
current thread is affected, not the entire process.
#include <numaif.h>
long set_mempolicy(int mode,
unsigned long *nodemask,
unsigned long maxnode);
The mode parameter must be one of the MPOL_* con-
stants introduced in the previous section. The nodemask
parameter specifies the memory nodes to use for future
allocations and maxnode is the number of nodes (i.e.,
bits) in nodemask. If mode is MPOL_DEFAULT no mem-
ory nodes need to be specified and the nodemask param-
eter is ignored. If a null pointer is passed as nodemask
for MPOL_PREFERRED the local node is selected. Oth-
erwise MPOL_PREFERRED uses the lowest node number
with the corresponding bit set in nodemask.
Setting a policy does not have any effect on already-
allocated memory. Pages are not automatically migrated;
only future allocations are affected. Note the difference
between memory allocation and address space reserva-
tion: an address space region established using mmap
is usually not automatically allocated. The first read or
write operation on the memory region will allocate the
appropriate page. If the policy changes between accesses
to different pages of the same address space region, or
if the policy allows allocation of memory from different
nodes, a seemingly uniform address space region might
be scattered across many memory nodes.
Ulrich Drepper
 Version 1.0
 73
6.5.3
 Swapping and Policies
If physical memory runs out, the system has to drop clean
pages and save dirty pages to swap. The Linux swap im-
plementation discards node information when it writes
pages to swap. That means when the page is reused and
paged in the node which is used will be chosen from
scratch. The policies for the thread will likely cause a
node which is close to the executing processors to be
chosen, but the node might be different from the one used
before.
This changing association means that the node associa-
tion cannot be stored by a program as a property of the
page. The association can change over time. For pages
which are shared with other processes this can also hap-
pen because a process asks for it (see the discussion of
mbind below). The kernel by itself can migrate pages if
one node runs out of space while other nodes still have
free space.
Any node association the user-level code learns about can
therefore be true for only a short time. It is more of a hint
than absolute information. Whenever accurate knowl-
edge is required the get_mempolicy interface should
be used (see section 6.5.5).
6.5.4
 VMA Policy
To set the VMA policy for an address range a different
interface has to be used:
#include <numaif.h>
long mbind(void *start, unsigned long len,
int mode,
unsigned long *nodemask,
unsigned long maxnode,
unsigned flags);
This interface registers a new VMA policy for the ad-
dress range [start, start + len). Since memory han-
dling operates on pages the start address must be page-
aligned. The len value is rounded up to the next page
size.
The mode parameter specifies, again, the policy; the val-
ues must be chosen from the list in section 6.5.1. As with
set_mempolicy, the nodemask parameter is only used
for some policies. Its handling is identical.
The semantics of the mbind interface depends on the
value of the flags parameter. By default, if flags is
zero, the system call sets the VMA policy for the address
range. Existing mappings are not affected. If this is not
sufficient there are currently three flags to modify this
behavior; they can be selected individually or together:
MPOL_MF_STRICT The call to mbind will fail if not all
pages are on the nodes specified by nodemask. In
case this flag is used together with MPOL_MF_MOVE
and/or MPOL_MF_MOVEALL the call will fail if any
page cannot be moved.
MPOL_MF_MOVE The kernel will try to move any page
in the address range allocated on a node not in the
set specified by nodemask. By default, only pages
used exclusively by the current process’s page ta-
bles are moved.
MPOL_MF_MOVEALL Like MPOL_MF_MOVE but the ker-
nel will try to move all pages, not just those used
by the current process’s page tables alone. This
operation has system-wide implications since it in-
fluences the memory access of other processes–
which are possibly not owned by the same user–
as well. Therefore MPOL_MF_MOVEALL is a privi-
leged operation (CAP_NICE capability is needed).
Note that support for MPOL_MF_MOVE and MPOL_MF_-
MOVEALL was added only in the 2.6.16 Linux kernel.
Calling mbind without any flags is most useful when the
policy for a newly reserved address range has to be spec-
ified before any pages are actually allocated.
void *p = mmap(NULL, len,
PROT_READ|PROT_WRITE,
MAP_ANON, -1, 0);
if (p != MAP_FAILED)
mbind(p, len, mode, nodemask, maxnode,
0);
This code sequence reserve an address space range of
len bytes and specifies that the policy mode referencing
the memory nodes in nodemask should be used. Unless
the MAP_POPULATE flag is used with mmap, no memory
will have been allocated by the time of the mbind call
and, therefore, the new policy applies to all pages in that
address space region.
The MPOL_MF_STRICT flag alone can be used to deter-
mine whether any page in the address range described by
the start and len parameters to mbind is allocated on
nodes other than those specified by nodemask. No allo-
cated pages are changed. If all pages are allocated on the
specified nodes, the VMA policy for the address space
region will be changed according to mode.
Sometimes rebalancing of memory is needed, in which
case it might be necessary to move pages allocated on
one node to another node. Calling mbind with MPOL_-
MF_MOVE set makes a best effort to achieve that. Only
pages which are solely referenced by the process’s page
table tree are considered for moving. There can be multi-
ple users in the form of threads or other processes which
share that part of the page table tree. It is not possible
to affect other processes which happen to map the same
data. These pages do not share the page table entries.
74
 Version 1.0
 What Every Programmer Should Know About Memory
If both the MPOL_MF_STRICT and MPOL_MF_MOVE bits
are set in the flags parameter passed to mbind the ker-
nel will try to move all pages which are not allocated on
the specified nodes. If this is not possible the call will
fail. Such a call might be useful to determine whether
there is a node (or set of nodes) which can house all the
pages. Several combinations can be tried in succession
until a suitable node is found.
The use of MPOL_MF_MOVEALL is harder to justify unless
running the current process is the main purpose of the
computer. The reason is that even pages that appear in
multiple page tables are moved. That can easily affect
other processes in a negative way. This operation should
thus be used with caution.
6.5.5
 Querying Node Information
The get_mempolicy interface can be used to query a
variety of facts about the state of NUMA for a given ad-
dress.
#include <numaif.h>
long get_mempolicy(int *policy,
const unsigned long *nmask,
unsigned long maxnode,
void *addr, int flags);
When get_mempolicy is called with zero for the flags
parameter, the information about the policy for address
addr is stored in the word pointed to by policy and
in the bitmask for the nodes pointed to by nmask. If
addr falls into an address space region for which a VMA
policy has been specified, information about that policy
is returned. Otherwise information about the task policy
or, if necessary, system default policy will be returned.
If the MPOL_F_NODE flag is set in flags, and the policy
governing addr is MPOL_INTERLEAVE, the value stored
in the word pointed to by policy is the index of the node
on which the next allocation is going to happen. This in-
formation can potentially be used to set the affinity of
a thread which is going to work on the newly-allocated
memory. This might be a less costly way to achieve prox-
imity, especially if the thread has yet to be created.
The MPOL_F_ADDR flag can be used to retrieve yet an-
other completely different data item. If this flag is used,
the value stored in the word pointed to by policy is
the index of the memory node on which the memory for
the page containing addr has been allocated. This in-
formation can be used to make decisions about possible
page migration, to decide which thread could work on the
memory location most efficiently, and many more things.
The CPU–and therefore memory node–a thread is using
is much more volatile than its memory allocations. Mem-
ory pages are, without explicit requests, only moved in
extreme circumstances. A thread can be assigned to an-
other CPU as the result of rebalancing the CPU loads. In-
formation about the current CPU and node might there-
fore be short-lived. The scheduler will try to keep the
thread on the same CPU, and possibly even on the same
core, to minimize performance losses due to cold caches.
This means it is useful to look at the current CPU and
node information; one only must avoid assuming the as-
sociation will not change.
libNUMA provides two interfaces to query the node in-
formation for a given virtual address space range:
#include <libNUMA.h>
int NUMA_mem_get_node_idx(void *addr);
int NUMA_mem_get_node_mask(void *addr,
size_t size,
size_t __destsize,
memnode_set_t *dest);
NUMA_mem_get_node_mask sets in dest the bits for all
memory nodes on which the pages in the range [addr,
addr+size) are (or would be) allocated, according to
the governing policy. NUMA_mem_get_node only looks
at the address addr and returns the index of the mem-
ory node on which this address is (or would be) allo-
cated. These interfaces are simpler to use than get_-
mempolicy and probably should be preferred.
The CPU currently used by a thread can be queried using
sched_getcpu (see section 6.4.3). Using this informa-
tion, a program can determine the memory node(s) which
are local to the CPU using the NUMA_cpu_to_memnode
interface from libNUMA:
#include <libNUMA.h>
int NUMA_cpu_to_memnode(size_t cpusetsize,
const cpu_set_t *cpuset,
size_t memnodesize,
memnode_set_t *
memnodeset);
A call to this function will set (in the memory node set
pointed to by the fourth parameter) all the bits corre-
sponding to memory nodes which are local to any of the
CPUs in the set pointed to by the second parameter. Just
like CPU information itself, this information is only cor-
rect until the configuration of the machine changes (for
instance, CPUs get removed and added).
The bits in the memnode_set_t objects can be used in
calls to the low-level functions like get_mempolicy.
It is more convenient to use the other functions in lib-
NUMA. The reverse mapping is available through:
#include <libNUMA.h>
Ulrich Drepper
 Version 1.0
 75
int NUMA_memnode_to_cpu(size_t memnodesize,
const memnode_set_t *
memnodeset,
size_t cpusetsize,
cpu_set_t *cpuset);
The bits set in the resulting cpuset are those of the
CPUs local to any of the memory nodes with correspond-
ing bits set in memnodeset. For both interfaces, the
programmer has to be aware that the information can
change over time (especially with CPU hot-plugging). In
many situations, a single bit is set in the input bit set,
but it is also meaningful, for instance, to pass the entire
set of CPUs retrieved by a call to sched_getaffinity
to NUMA_cpu_to_memnode to determine which are the
memory nodes the thread ever can have direct access to.
6.5.6
 CPU and Node Sets
Adjusting code for SMP and NUMA environments by
changing the code to use the interfaces described so far
might be prohibitively expensive (or impossible) if the
sources are not available. Additionally, the system ad-
ministrator might want to impose restrictions on the re-
sources a user and/or process can use. For these situa-
tions the Linux kernel supports so-called CPU sets. The
name is a bit misleading since memory nodes are also
covered. They also have nothing to do with the cpu_-
set_t data type.
The interface to CPU sets is, at the moment, a special
filesystem. It is usually not mounted (so far at least).
This can be changed with
mount -t cpuset none /dev/cpuset
The mount point /dev/cpuset must of course exist at
that time. The content of this directory is a description
of the default (root) CPU set. It comprises initially all
CPUs and all memory nodes. The cpus file in that di-
rectory shows the CPUs in the CPU set, the mems file the
memory nodes, the tasks file the processes.
To create a new CPU set one simply creates a new direc-
tory somewhere in the hierarchy. The new CPU set will
inherit all settings from the parent. Then the CPUs and
memory nodes for new CPU set can be changed by writ-
ing the new values into the cpus and mems pseudo files
in the new directory.
If a process belongs to a CPU set, the settings for the
CPUs and memory nodes are used as masks for the affin-
ity and memory policy bitmasks. That means the pro-
gram cannot select any CPU in the affinity mask which
is not in the cpus file for the CPU set the process is us-
ing (i.e., where it is listed in the tasks file). Similarly
for the node masks for the memory policy and the mems
file.
The program will not experience any errors unless the
bitmasks are empty after the masking, so CPU sets are
an almost-invisible means to control program execution.
This method is especially efficient on machines with lots
of CPUs and/or memory nodes. Moving a process into a
new CPU set is as simple as writing the process ID into
the tasks file of the appropriate CPU set.
The directories for the CPU sets contain a number of
other files which can be used to specify details like be-
havior under memory pressure and exclusive access to
CPUs and memory nodes. The interested reader is re-
ferred to the file Documentation/cpusets.txt in the
kernel source tree.
6.5.7
 Explicit NUMA Optimizations
All the local memory and affinity rules cannot help out
if all threads on all the nodes need access to the same
memory regions. It is, of course, possible to simply re-
strict the number of threads to a number supportable by
the processors which are directly connected to the mem-
ory node. This does not take advantage of SMP NUMA
machines, though, and is therefore not a real option.
If the data in question is read-only there is a simple solu-
tion: replication. Each node can get its own copy of the
data so that no inter-node accesses are necessary. Code
to do this can look like this:
void *local_data(void) {
static void *data[NNODES];
int node =
NUMA_memnode_self_current_idx();
if (node == -1)
/* Cannot get node, pick one. */
node = 0;
if (data[node] == NULL)
data[node] = allocate_data();
return data[node];
}
void worker(void) {
void *data = local_data();
for (...)
compute using data
}
In this code the function worker prepares by getting a
pointer to the local copy of the data by a call to local_-
data. Then it proceeds with the loop, which uses this
pointer. The local_data function keeps a list of the al-
ready allocated copies of the data around. Each system
has a limited number of memory nodes, so the size of the
array with the pointers to the per-node memory copies
is limited in size. The NUMA_memnode_system_count
function from libNUMA returns this number. If memory
for the given node has not yet been allocated for the cur-
rent node (recognized by a null pointer in data at the in-
76
 Version 1.0
 What Every Programmer Should Know About Memory
dex returned by the NUMA_memnode_self_current_-
idx call), a new copy is allocated.
It is important to realize that nothing terrible happens if
the threads get scheduled onto another CPU connected to
a different memory node after the getcpu system call43 .
It just means that the accesses using the data variable in
worker access memory on another memory node. This
slows the program down until data is computed anew,
but that is all. The kernel will always avoid gratuitous
rebalancing of the per-CPU run queues. If such a trans-
fer happens it is usually for a good reason and will not
happen again for the near future.
Things are more complicated when the memory area in
question is writable. Simple duplication will not work in
this case. Depending on the exact situation there might a
number of possible solutions.
For instance, if the writable memory region is used to
accumulate results, it might be possible to first create a
separate region for each memory node in which the re-
sults are accumulated. Then, when this work is done, all
the per-node memory regions are combined to get the to-
tal result. This technique can work even if the work never
really stops, but intermediate results are needed. The re-
quirement for this approach is that the accumulation of
a result is stateless, i.e., it does not depend on the previ-
ously collected results.
It will always be better, though, to have direct access to
the writable memory region. If the number of accesses
to the memory region is substantial, it might be a good
idea to force the kernel to migrate the memory pages in
question to the local node. If the number of accesses
is really high, and the writes on different nodes do not
happen concurrently, this could help. But be aware that
the kernel cannot perform miracles: the page migration
is a copy operation and as such it is not cheap. This cost
has to be amortized.
6.5.8
 Utilizing All Bandwidth
The numbers in Figure 5.4 show that access to remote
memory when the caches are ineffective is not measur-
ably slower than access to local memory. This means
a program could possibly save bandwidth to the local
memory by writing data it does not have to read again
into memory attached to another processor. The band-
width of the connection to the DRAM modules and the
bandwidth of the interconnects are mostly independent,
so parallel use could improve overall performance.
Whether this is really possible depends on many fac-
tors. One really has to be sure that caches are ineffec-
tive since otherwise the slowdown related to remote ac-
cesses is measurable. Another big problem is whether
the remote node has any needs for its own memory band-
43 The user-level sched_getcpu interface is implemented using
the getcpu system call which should not be used directly and has a
different interface.
width. This possibility must be examined in detail before
the approach is taken. In theory, using all the bandwidth
available to a processor can have positive effects. A fam-
ily 10h Opteron processor can be directly connected to
up to four other processors. Utilizing all that additional
bandwidth, perhaps coupled with appropriate prefetches
(especially prefetchw) could lead to improvements if
the rest of the system plays along.
Ulrich Drepper
 Version 1.0
 77
7
 Memory Performance Tools
A wide variety of tools is available to help programmers
understand performance characteristics of a program, the
cache and memory use among others. Modern proces-
sors have performance monitoring hardware that can be
used. Some events are hard to measure exactly, so there
is also room for simulation. When it comes to higher-
level functionality, there are special tools to monitor the
execution of a process. We will introduce a set of com-
monly used tools available on most Linux systems.
7.1
 Memory Operation Profiling
Profiling memory operations requires collaboration from
the hardware. It is possible to gather some information in
software alone, but this is either coarse-grained or merely
a simulation. Examples of simulation will be shown in
section 7.2 and 7.5. Here we will concentrate on measur-
able memory effects.
Access to performance monitoring hardware on Linux is
provided by oprofile. Oprofile provides continuous pro-
filing capabilities as first described in [2]; it performs
statistical, system-wide profiling with an easy-to-use in-
terface. Oprofile is by no means the only way the per-
formance measurement functionality of processors can
be used; Linux developers are working on pfmon which
might at some point be sufficiently widely deployed to
warrant being described here, too.
The interface oprofile provides is simple and minimal but
also pretty low-level, even if the optional GUI is used.
The user has to select among the events the processor
can record. The architecture manuals for the processors
describe the events but, oftentimes, it requires extensive
knowledge about the processors themselves to interpret
the data. Another problem is the interpretation of the
collected data. The performance measurement counters
are absolute values and can grow arbitrarily. How high is
too high for a given counter?
A partial answer to this problem is to avoid looking at the
absolute values and, instead, relate multiple counters to
each other. Processors can monitor more than one event;
the ratio of the collected absolute values can then be ex-
amined. This gives nice, comparable results. Often the
divisor is a measure of processing time, the number of
clock cycles or the number of instructions. As an ini-
tial stab at program performance, relating just these two
numbers by themselves is useful.
Figure 7.1 shows the Cycles Per Instruction (CPI) for the
simple random “Follow” test case for the various work-
ing set sizes. The names of the events to collect this infor-
mation for most Intel processor are CPU_CLK_UNHALTED
and INST_RETIRED. As the names suggest, the former
counts the clock cycles of the CPU and the latter the
number of instructions. We see a picture similar to the
cycles per list element measurements we used. For small
working set sizes the ratio is 1.0 or even lower. These
20
18
16
n
o 14
itc u 12
rt s 10
nI / s 8
el c 6
yC 4
2
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
Figure 7.1: Cycles per Instruction (Follow Random)
measurements were made on a Intel Core 2 processor,
which is multi-scalar and can work on several instruc-
tions at once. For a program which is not limited by
memory bandwidth, the ratio can be significantly below
1.0 but, in this case, 1.0 is pretty good.
Once the L1d is no longer large enough to hold the work-
ing set, the CPI jumps to just below 3.0. Note that the
CPI ratio averages the penalties for accessing L2 over
all instructions, not just the memory instructions. Using
the cycles for list element data, it can be worked out how
many instructions per list element are needed. If even the
L2 cache is not sufficient, the CPI ratio jumps to more
than 20. These are expected results.
But the performance measurement counters are supposed
to give more insight into what is going on in the pro-
cessor. For this we need to think about processor im-
plementations. In this document, we are concerned with
cache handling details, so we have to look at events re-
lated to the caches. These events, their names, and what
they count, are processor–specific. This is where opro-
file is currently hard to use, irrespective of the simple
user interface: the user has to figure out the performance
counter details by her/himself. In Appendix B we will
see details about some processors.
For the Core 2 processor the events to look for are L1D_-
REPL, DTLB_MISSES, and L2_LINES_IN. The latter can
measure both all misses and misses caused by instruc-
tions instead of hardware prefetching. The results for the
random “Follow” test can be seen in Figure 7.2.
All ratios are computed using the number of retired in-
structions (INST_RETIRED). This means that to compute
the cache miss rate all the load and store instructions a
substantial number has to be subtracted from the INST_-
RETIRED value which means the actual cache miss rate
of the memory operation is even higher than the numbers
shown in the graph.
The L1d misses tower over all the others since an L2 miss
78
 Version 1.0
 What Every Programmer Should Know About Memory
22%
20%
18%
o
 16%
ita 14%
Rs 12%
si 10%
Me 8%
hc a 6%
C4%
2%
0%
210
 2
13
 2
16
 2
19
 2
22
 2
25
 2
28
Working Set Size (Bytes)
L1D Misses
 L2 Misses
 L2 Demand Misses
 DTLB Misses
3.5%
3%
o
itaRssiMehcaC2.5%
2%
1.5%
1%
0.5%
0%
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
L1D Misses
 L2 Misses
 L2 Demand Misses
 DTLB Misses
Figure 7.2: Measured Cache Misses (Follow Random)
 Figure 7.3: Measured Cache Misses (Follow Sequential)
implies, for Intel processors, an L1d miss due to the use
of inclusive caches. The processor has 32k of L1d and so
we see, as expected, the L1d rate go up from zero at about
that working set size (there are other uses of the cache
beside the list data structure, which means the increase
happens between the 16k and 32k mark). It is interesting
to see that the hardware prefetching can keep the miss
rate at 1% for a working set size up to and including 64k.
After that the L1d rate skyrockets.
The L2 miss rate stays zero until the L2 is exhausted;
the few misses due to other uses of L2 do not influence
the numbers much. Once the size of L2 (221 bytes) is
exceeded, the miss rates rise. It is important to notice
that the L2 demand miss rate is nonzero. This indicates
that the hardware prefetcher does not load all the cache
lines needed by instructions later. This is expected, the
randomness of the accesses prevents perfect prefetching.
Compare this with the data for the sequential read in Fig-
ure 7.3.
In this graph we can see that the L2 demand miss rate
is basically zero (note the scale of this graph is differ-
ent from Figure 7.2). For the sequential access case, the
hardware prefetcher works perfectly: almost all L2 cache
misses are caused by the prefetcher. The fact that the L1d
and L2 miss rates are the same shows that all L1d cache
misses are handled by the L2 cache without further de-
lays. This is the ideal case for all programs but it is, of
course, hardly ever achievable.
The fourth line in both graphs is the DTLB miss rate (In-
tel has separate TLBs for code and data, DTLB is the data
TLB). For the random access case, the DTLB miss rate
is significant and contributes to the delays. What is in-
teresting is that the DTLB penalties set in before the L2
misses. For the sequential access case the DTLB costs
are basically zero.
Going back to the matrix multiplication example in sec-
tion 6.2.1 and the example code in section A.1, we can
make use of three more counters. The SSE_HIT_PRE,
SSE_PRE_MISS, and LOAD_PRE_EXEC counters can be
used to see how effective the software prefetching is. If
the code in section A.1 is run we get the following re-
sults:
Description
Useful NTA prefetches
Late NTA prefetches
Ratio
2.84%
2.65%
The low useful NTA (non-temporal aligned) prefetch ra-
tio indicates that many prefetch instructions are executed
for cache lines which are already loaded, so no work is
needed. This means the processor wastes time to decode
the prefetch instruction and look up the cache. One can-
not judge the code too harshly, though. Much depends on
the size of the caches of the processor used; the hardware
prefetcher also plays a role.
The low late NTA prefetch ratio is misleading. The ratio
means that 2.65% of all prefetch instructions are issued
too late. The instruction which needs the data is executed
before the data could be prefetched into the cache. It
must be kept in mind that only 2.84% + 2.65% = 5.5%
of the prefetch instructions were of any use. Of the NTA
prefetch instructions which are useful, 48% did not finish
in time. The code therefore can be optimized further:
• most of the prefetch instructions are not needed.
• the use of the prefetch instruction can be adjusted
to match the hardware better.
It is left as an exercise to the reader to determine the best
solution for the available hardware. The exact hardware
specification plays a big role. On Core 2 processors the
latency of the SSE arithmetic operations is 1 cycle. Older
Ulrich Drepper
 Version 1.0
 79
$ \time ls /etc
[...]
0.00user 0.00system 0:00.02elapsed 17%CPU (0avgtext+0avgdata 0maxresident)k
0inputs+0outputs (1major+335minor)pagefaults 0swaps
Figure 7.4: Output of the time utility
versions had a latency of 2 cycles, meaning that the hard-
ware prefetcher and the prefetch instructions had more
time to bring in the data.
To determine where prefetches might be needed–or are
unnecessary–one can use the opannotate program. It lists
the source or assembler code of the program and shows
the instructions where the event was recognized. Note
that there are two sources of vagueness:
1. Oprofile performs stochastic profiling. Only every
Nth event (where N is a per-event threshold with
an enforced minimum) is recorded to avoid slow-
ing down operation of the system too much. There
might be lines which cause 100 events and yet they
might not show up in the report.
2. Not all events are recorded accurately. For exam-
ple, the instruction counter at the time a specific
event was recorded might be incorrect. Processors
being multi-scalar makes it hard to give a 100%
correct answer. A few events on some processors
are exact, though.
The annotated listings are useful for more than determin-
ing the prefetching information. Every event is recorded
with the instruction pointer; it is therefore also possible
to pinpoint other hot spots in the program. Locations
which are the source of many INST_RETIRED events are
executed frequently and deserve to be tuned. Locations
where many cache misses are reported might warrant a
prefetch instruction to avoid the cache miss.
One type of event which can be measured without hard-
ware support is page faults. The OS is responsible for re-
solving page faults and, on those occasions, it also counts
them. It distinguishes two kinds of page faults:
Minor Page Faults For anonymous (i.e., not backed by
a file) pages which have not been used so far, for
copy-on-write pages, and for other pages whose
content is already in memory somewhere.
Major Page Faults Resolving them requires access to
disk to retrieve the file-backed (or swapped-out)
data.
Obviously, major page faults are significantly more ex-
pensive than minor page faults. But the latter are not
cheap either. In either case an entry into the kernel is
necessary, a new page must be found, the page must be
cleared or populated with the appropriate data, and the
page table tree must be modified accordingly. The last
step requires synchronization with other tasks reading or
modifying the page table tree, which might introduce fur-
ther delays.
The easiest way to retrieve information about the page
fault counts is to use the time tool. Note: use the real
tool, not the shell builtin. The output can be seen in Fig-
ure 7.4.44
The interesting part here is the last line. The time tool
reports one major and 335 minor page faults. The exact
numbers vary; in particular, repeating the run immedi-
ately will likely show that there are now no major page
faults at all. If the program performs the same action, and
nothing changes in the environment, the total page fault
count will be stable.
An especially sensitive phase with respect to page faults
is program start-up. Each page which is used will pro-
duce a page fault; the visible effect (especially for GUI
applications) is that the more pages that are used, the
longer it takes for the program to start working. In sec-
tion 7.5 we will see a tool to measure this effect specifi-
cally.
Under the hood, the time tool uses the rusage function-
ality. The wait4 system call fills in a struct rusage
object when the parent waits for a child to terminate;
that is exactly what is needed for the time tool. But it is
also possible for a process to request information about
its own resource usage (that is where the name rusage
comes from) or the resource usage of its terminated chil-
dren.
#include <sys/resource.h>
int getrusage(__rusage_who_t who,
struct rusage *usage)
The who parameter specifies which process the informa-
tion is requested for. Currently, only RUSAGE_SELF and
RUSAGE_CHILDREN are defined. The resource usage of
the child processes is accumulated when each child ter-
minates. It is a total value, not the usage of an individ-
ual child process. Proposals to allow requesting thread-
specific information exist, so it is likely that we will see
RUSAGE_THREAD in the near future. The rusage struc-
ture is defined to contain all kinds of metrics, including
execution time, the number of IPC messages sent and
44 The leading backslash prevents the use of the built-in command.
80
 Version 1.0
 What Every Programmer Should Know About Memory
==19645== I
 refs:
 152,653,497
==19645== I1 misses:
 25,833
==19645== L2i misses:
 2,475
==19645== I1 miss rate:
 0.01%
==19645== L2i miss rate:
 0.00%
==19645==
==19645== D
 refs:
 56,857,129 (35,838,721 rd + 21,018,408 wr)
==19645== D1 misses:
 14,187 (
 12,451 rd +
 1,736 wr)
==19645== L2d misses:
 7,701 (
 6,325 rd +
 1,376 wr)
==19645== D1 miss rate:
 0.0% (
 0.0%
 +
 0.0% )
==19645== L2d miss rate:
 0.0% (
 0.0%
 +
 0.0% )
==19645==
==19645== L2 refs:
 40,020 (
 38,284 rd +
 1,736 wr)
==19645== L2 misses:
 10,176 (
 8,800 rd +
 1,376 wr)
==19645== L2 miss rate:
 0.0% (
 0.0%
 +
 0.0% )
Figure 7.5: Cachegrind Summary Output
memory used, and the number of page faults. The lat-
ter information is available in the ru_minflt and ru_-
majflt members of the structure.
A programmer who tries to determine where her program
loses performance due to page faults could regularly re-
quest the information and then compare the returned val-
ues with the previous results.
From the outside, the information is also visible if the
requester has the necessary privileges. The pseudo file
/proc/<PID>/stat, where <PID> is the process ID of
the process we are interested in, contains the page fault
numbers in the tenth to fourteenth fields. They are pairs
of the process’s and its children’s cumulative minor and
major page faults, respectively.
7.2
 Simulating CPU Caches
While the technical description of how a cache works is
relatively easy to understand, it is not so easy to see how
an actual program behaves with respect to cache. Pro-
grammers are not directly concerned with the values of
addresses, be they absolute nor relative. Addresses are
determined, in part, by the linker and, in part, at runtime
by the dynamic linker and the kernel. The generated as-
sembly code is expected to work with all possible ad-
dresses and, in the source language, there is not even a
hint of absolute address values left. So it can be quite
difficult to get a sense for how a program is making use
of memory. 45
CPU-level profiling tools such as oprofile (as described
in section 7.1) can help to understand the cache use. The
resulting data corresponds to the actual hardware, and it
can be collected relatively quickly if fine-grained collec-
tion is not needed. As soon as more fine-grained data is
needed, oprofile is not usable anymore; the thread would
have to be interrupted too often. Furthermore, to see the
memory behavior of the program on different processors,
one actually has to have such machines and execute the
45 When programming close to the hardware this might be different,
but this is of no concern to normal programming and, in any case, is
only possible for special addresses such as memory-mapped devices.
program on them. This is sometimes (often) not possible.
One example is the data from Figure 3.8. To collect such
data with oprofile one would have to have 24 different
machines, many of which do not exist.
The data in that graph was collected using a cache simu-
lator. This program, cachegrind, uses the valgrind frame-
work, which was initially developed to check for memory
handling related problems in a program. The valgrind
framework simulates the execution of a program and,
while doing this, it allows various extensions, such as
cachegrind, to hook into the execution framework. The
cachegrind tool uses this to intercept all uses of memory
addresses; it then simulates the operation of L1i, L1d,
and L2 caches with a given size, cache line size, and as-
sociativity.
To use the tool a program must be run using valgrind as
a wrapper:
valgrind --tool=cachegrind command arg
In this simplest form the program command is executed
with the parameter arg while simulating the three caches
using sizes and associativity corresponding to that of the
processor it is running on. One part of the output is
printed to standard error when the program is running;
it consists of statistics of the total cache use as can be
seen in Figure 7.5. The total number of instructions and
memory references is given, along with the number of
misses they produce for the L1i/L1d and L2 cache, the
miss rates, etc. The tool is even able to split the L2
accesses into instruction and data accesses, and all data
cache uses are split in read and write accesses.
It becomes even more interesting when the details of the
simulated caches are changed and the results compared.
Through the use of the --I1, --D1, and --L2 parame-
ters, cachegrind can be instructed to disregard the proces-
sor’s cache layout and use that specified on the command
line. For example:
valgrind --tool=cachegrind \
--L2=8388608,8,64 command arg
Ulrich Drepper
 Version 1.0
 81
--------------------------------------------------------------------------------
Ir I1mr I2mr
 Dr D1mr D2mr
 Dw D1mw D2mw file:function
--------------------------------------------------------------------------------
53,684,905
 9
 8 9,589,531
 13
 3 5,820,373
 14
 0 ???:_IO_file_xsputn@@GLIBC_2.2.5
36,925,729 6,267 114 11,205,241
 74
 18 7,123,370
 22
 0 ???:vfprintf
11,845,373
 22
 2 3,126,914
 46
 22 1,563,457
 0
 0 ???:__find_specmb
6,004,482
 40
 10
 697,872 1,744 484
 0
 0
 0 ???:strlen
5,008,448
 3
 2 1,450,093
 370 118
 0
 0
 0 ???:strcmp
3,316,589
 24
 4
 757,523
 0
 0
 540,952
 0
 0 ???:_IO_padn
2,825,541
 3
 3
 290,222
 5
 1
 216,403
 0
 0 ???:_itoa_word
2,628,466
 9
 6
 730,059
 0
 0
 358,215
 0
 0 ???:_IO_file_overflow@@GLIBC_2.2.5
2,504,211
 4
 4
 762,151
 2
 0
 598,833
 3
 0 ???:_IO_do_write@@GLIBC_2.2.5
2,296,142
 32
 7
 616,490
 88
 0
 321,848
 0
 0 dwarf_child.c:__libdw_find_attr
2,184,153 2,876
 20
 503,805
 67
 0
 435,562
 0
 0 ???:__dcigettext
2,014,243
 3
 3
 435,512
 1
 1
 272,195
 4
 0 ???:_IO_file_write@@GLIBC_2.2.5
1,988,697 2,804
 4
 656,112
 380
 0
 47,847
 1
 1 ???:getenv
1,973,463
 27
 6
 597,768
 15
 0
 420,805
 0
 0 dwarf_getattrs.c:dwarf_getattrs
Figure 7.6: cg annotate Output
would simulate an 8MB L2 cache with 8-way set asso-
ciativity and 64 byte cache line size. Note that the --L2
option appears on the command line before the name of
the program which is simulated.
This is not all cachegrind can do. Before the process ex-
its it writes out a file named cachegrind.out.XXXXX
where XXXXX is the PID of the process. This file contains
the summary information and detailed information about
the cache use in each function and source file. The data
can be viewed using the cg annotate program.
The output this program produces contains the cache use
summary which was printed also when the process ter-
minated, along with a detailed summary of the cache line
use in each function of the program. Generating this per-
function data requires that cg annotate is able to match
addresses to functions. This means debug information
should be available for best results. Failing that, the ELF
symbol tables can help a bit but, since internal symbols
are not listed in the dynamic symbol table, the results are
not complete. Figure 7.6 shows part of the output for the
same program run as Figure 7.5.
The Ir, Dr, and Dw columns show the total cache use,
not cache misses, which are shown in the following two
columns. This data can be used to identify the code
which produces the most cache misses. First, one proba-
bly would concentrate on L2 cache misses, then proceed
to optimizing L1i/L1d cache misses.
cg annotate can provide the data in more detail. If the
name of a source file is given, it also annotates (hence
the program’s name) each line of the source file with the
number of cache hits and misses corresponding to that
line. This information allows the programmer to drill
down to the exact line where cache misses are a problem.
The program interface is a bit raw: as of this writing, the
cachegrind data file and the source file must be in the
same directory.
It should, at this point, be noted again: cachegrind is
a simulator which does not use measurements from the
processor. The actual cache implementation in the pro-
cessor might very well be quite different. cachegrind
simulates Least Recently Used (LRU) eviction, which is
likely to be too expensive for caches with large associa-
tivity. Furthermore, the simulation does not take context
switches and system calls into account, both of which
can destroy large parts of L2 and must flush L1i and L1d.
This causes the total number of cache misses to be lower
than experienced in reality. Nevertheless, cachegrind is a
nice tool to learn about a program’s memory use and its
problems with memory.
7.3
 Measuring Memory Usage
Knowing how much memory a program allocates and
possibly where the allocation happens is the first step to
optimizing its memory use There are, fortunately, some
easy-to-use programs available which do not require that
the program be recompiled or specifically modified.
For the first tool, called massif, it is sufficient to not strip
the debug information which the compiler can automat-
ically generate. It provides an overview of the accumu-
lated memory use over time. Figure 7.7 shows an ex-
ample of the generated output. Like cachegrind (sec-
tion 7.2), massif is a tool using the valgrind infrastruc-
ture. It is started using
valgrind --tool=massif command arg
where command arg is the program which is to be ob-
served and its parameter(s), The program will be sim-
ulated and all calls to memory allocation functions are
recognized. The call site is recorded along with a times-
tamp value; the new allocation size is added to both the
whole-program total and total for the specific call site.
The same applies to the functions which free memory
where, obviously, the size of the freed block is subtracted
from the appropriated sums. This information can then
be used to create a graph showing the memory use over
the lifetime of the program, splitting each time value ac-
cording to the location which requested the allocation.
82
 Version 1.0
 What Every Programmer Should Know About Memory
src/readelf -a -w src/readelf
 106,047,201 bytes x ms
s
etyb55k
50k
45k
40k
35k
30k
25k
20k
15k
10k
5k
0k
0.0
 200.0
 400.0
 600.0
 800.0 1000.0 1200.0 1400.0 1600.0 1800.0 2000.0 2200.0 ms
x4C13D6C:__libdw_allocat
x4E29F83:file_read_elf
x4C0E7D5:Dwarf_Abbrev_Ha
stack(s)
x4C0E8BE:insert_entry_2
x4C0D64E:dwarf_begin_elf
x1027D7D:_nl_intern_loca
x102CF6C:read_alias_file
x102CEE8:read_alias_file
x9D75:_dl_new_object
heap-admin
x102D594:_nl_make_l10nfl
x102D461:_nl_make_l10nfl
x40C896:xmalloc
x40B2C0:openbackend
Figure 7.7: Massif Output
Before the process terminates massif creates two files:
massif.XXXXX.txt and massif.XXXXX.ps; XXXXX is
as before the PID of the process. The .txt file is a sum-
mary of the memory use for all call sites and the .ps is
what can be seen in Figure 7.7.
Massif can also record the program’s stack usage, which
can be useful to determine the total memory footprint of
an application. But this is not always possible. In some
situations (some thread stacks or when signaltstack
is used) the valgrind runtime cannot know about the lim-
its of the stack . In these situations, it also does not make
much sense to add these stacks’ sizes to the total. There
are several other situations where it makes no sense. If
a program is affected by this, massif should be started
with the addition option --stacks=no. Note, this is an
option for valgrind and therefore must come before the
name of the program which is being observed.
Some programs provide their own memory allocation im-
plementation or wrapper functions around the system’s
allocation functions. In the first case, allocations are nor-
mally missed; in the second case, the recorded call sites
hide information, since only the address of the call in
the wrapper function is recorded. For this reason, it is
possible to add additional functions to the list of alloca-
tion functions. The --alloc-fn=xmalloc parameter
would specify that the function xmalloc is also an al-
location function, which is often the case in GNU pro-
grams. Calls to xmalloc are recorded, but not the allo-
cation calls made from within xmalloc.
The second tool is called memusage; it is part of the GNU
C library. It is a simplified version of massif (but existed
a long time before massif). It only records the total mem-
ory use for heap (including possible calls to mmap etc. if
the -m option is given) and, optionally, the stack. The
results can be shown as a graph of the total memory use
over time or, alternatively, linearly over the calls made to
allocation functions. The graphs are created separately
by the memusage script which, just as with valgrind, has
to be used to start the application:
memusage command arg
The -p IMGFILE option must be used to specify that the
graph should be generated in the file IMGFILE. This is a
PNG file. The code to collect the data is run in the actual
program itself, it is not an simulation like valgrind. This
means memusage is much faster than massif and usable
in situations where massif would be not useful. Besides
total memory consumption, the code also records alloca-
tion sizes and, on program termination, it shows a his-
togram of the used allocation sizes. This information is
written to standard error.
Sometimes it is not possible (or feasible) to call the pro-
gram which is supposed to be observed directly. An ex-
ample is the compiler stage of gcc, which is started by
the gcc driver program. In this case the name of the pro-
Ulrich Drepper
 Version 1.0
 83
gram which should be observed must be provided to the
memusage script using the -n NAME parameter. This pa-
rameter is also useful if the program which is observed
starts other programs. If no program name is specified
all started programs will be profiled.
Both programs, massif and memusage, have additional
options. A programmer finding herself in the position
needing more functionality should first consult the man-
ual or help messages to make sure the additional func-
tionality is not already implemented.
Now that we know how the data about memory allocation
can be captured, it is necessary to discuss how this data
can be interpreted in the context of memory and cache
use. The main aspects of efficient dynamic memory allo-
cation are linear allocation and compactness of the used
portion. This goes back to making prefetching efficient
and reducing cache misses.
A program which has to read in an arbitrary amount of
data for later processing could do this by creating a list
where each of the list elements contains a new data item.
The overhead for this allocation method might be min-
imal (one pointer for a single-linked list) but the cache
effects when using the data can reduce the performance
dramatically.
One problem is, for instance, that there is no guarantee
that sequentially allocated memory is laid out sequen-
tially in memory. There are many possible reasons for
this:
• memory blocks inside a large memory chunk ad-
ministrated by the memory allocator are actually
returned from the back to the front;
• a memory chunk is exhausted and a new one is
started in a different part of the address space;
• the allocation requests are for different sizes which
are served from different memory pools;
• the interleaving allocations in the various threads
of multi-threaded programs.
If data must be allocated up front for later processing,
the linked-list approach is clearly a bad idea. There is
no guarantee (or even likelihood) that the consecutive el-
ements in the list are laid out consecutively in memory.
To ensure contiguous allocations, that memory must not
be allocated in small chunks. Another layer of memory
handling must be used; it can easily be implemented by
the programmer. An alternative is to use the obstack im-
plementation available in the GNU C library. This allo-
cator requests large blocks of memory from the system’s
allocator and then hands arbitrarily large or small blocks
of memory out. These allocations are always sequential
unless the large memory chunk is exhausted, which is,
depending on the requested allocation sizes, pretty rare.
Obstacks are not a complete replacement for a memory
allocator, they have limited abilities to free objects. See
the GNU C library manual for details.
So, how can a situation where the use of obstacks (or
similar techniques) is advisable be recognized from the
graphs? Without consulting the source, possible candi-
dates for the changes cannot be identified, but the graph
can provide an entry point for the search. If many al-
locations are made from the same location, this could
mean that allocation in bulk might help. In Figure 7.7,
we can see such a possible candidate in the allocations at
address 0x4c0e7d5. From about 800ms into the run un-
til 1,800ms into the run this is the only area (except the
top, green one) which grows. Moreover, the slope is not
steep, which means we have a large number of relatively
small allocations. This is, indeed, a candidate for the use
of obstacks or similar techniques.
Another problem the graphs can show is when the total
number of allocations is high. This is especially easy to
see if the graph is not drawn linearly over time but, in-
stead, linearly over the number of calls (the default with
memusage). In that case, a gentle slope in the graph
means a lot of small allocations. memusage will not say
where the allocations took place, but the comparison with
massif’s output can say that, or the programmer might
recognize it right away. Many small allocations should
be consolidated to achieve linear memory use.
But there is another, equally important, aspect to this lat-
ter class of cases: many allocations also means higher
overhead in administrative data. This by itself might not
be that problematic. The red area named “heap-admin”
represents this overhead in the massif graph and it is quite
small. But, depending on the malloc implementation,
this administrative data is allocated along with the data
blocks, in the same memory. For the current malloc im-
plementation in the GNU C library, this is the case: every
allocated block has at least a 2-word header (8 bytes for
32-bit platforms, 16 bytes for 64-bit platforms). In addi-
tion, block sizes are often a bit larger than necessary due
to the way memory is administrated (rounding up block
sizes to specific multiples).
This all means that memory used by the program is in-
terspersed with memory only used by the allocator for
administrative purposes. We might see something like
this:
Header
 Data
 Padding
Each block represents one memory word. In this small
region of memory we have four allocated blocks. The
overhead due to the block header and padding is 50%.
Due to the placement of the header, this automatically
means that the effective prefetch rate of the processor is
lowered by up to 50% as well. If the blocks were be
processed sequentially (to take maximum advantage of
prefetching), the processor would read all the header and
padding words into the cache, even though they are never
84
 Version 1.0
 What Every Programmer Should Know About Memory
supposed to be read from or written to by the application
itself. Only the runtime uses the header words, and the
runtime only comes into play when the block is freed.
One could at this point argue that the implementation
should be changed to put the administrative data some-
where else. This is indeed done in some implementa-
tions, and it might prove to be a good idea. There are
many aspects to be kept in mind, though, security not be-
ing the least of them. Regardless of whether we might
see a change in the future, the padding issue will never
go away (amounting to 16% of the data in the example,
when ignoring the headers). Only if the programmer di-
rectly takes control of allocations can this be avoided.
When alignment requirements come into play there can
still be holes, but this is also something under control of
the programmer.
7.4
 Improving Branch Prediction
In section 6.2.2, two methods to improve L1i use through
branch prediction and block reordering were mentioned:
static prediction through __builtin_expect and pro-
file guided optimization (PGO). Correct branch predic-
tion has performance impacts, but here we are interested
in the memory usage improvements.
The use of __builtin_expect (or better the likely
and unlikely macros) is simple. The definitions are
placed in a central header and the compiler takes care
of the rest. There is a little problem, though: it is easy
enough for a programmer to use likely when really
unlikely was meant and vice versa. Even if somebody
uses a tool like oprofile to measure incorrect branch pre-
dictions and L1i misses these problems are hard to detect.
There is one easy method, though. The code in sec-
tion A.2 shows an alternative definition of the likely
and unlikely macros which measure actively, at run-
time, whether the static predictions are correct or not.
The results can then be examined by the programmer or
tester and adjustments can be made. The measurements
do not actually take the performance of the program into
account, they simply test the static assumptions made by
the programmer. More details can be found, along with
the code, in the section referenced above.
PGO is quite easy to use with gcc these days. It is a three-
step process, though, and certain requirements must be
fulfilled. First, all source files must be compiled with the
additional -fprofile-generate option. This option
must be passed to all compiler runs and to the command
which links the program. Mixing object files compiled
with and without this option is possible, but PGO will
not do any good for those that do not have it enabled.
The compiler generates a binary which behaves normally
except that it is significantly larger and slower because it
records (and stores) information about whether branches
are taken or not. The compiler also emits a file with the
extension .gcno for each input file. This file contains
information related to the branches in the code. It must
be preserved for later.
Once the program binary is available, it should be used
to run a representative set of workloads. Whatever work-
load is used, the final binary will be optimized to do this
task well. Consecutive runs of the program are possible
and, in general necessary; all the runs will contribute to
the same output file. Before the program terminates, the
data collected during the program run is written out into
files with the extension .gcda. These files are created
in the directory which contains the source file. The pro-
gram can be executed from any directory, and the binary
can be copied, but the directory with the sources must be
available and writable. Again, one output file is created
for each input source file. If the program is run multiple
times, it is important that the .gcda files of the previous
run are found in the source directories since otherwise
the data of the runs cannot be accumulated in one file.
When a representative set of tests has been run, it is time
to recompile the application. The compiler has to be able
to find the .gcda files in the same directory which holds
the source files. The files cannot be moved since the com-
piler would not find them and the embedded checksum
for the files would not match anymore. For the recom-
pilation, replace the -fprofile-generate parameter
with -fprofile-use. It is essential that the sources
do not change in any way that would change the gener-
ated code. That means: it is OK to change white spaces
and edit comments, but adding more branches or basic
blocks invalidates the collected data and the compilation
will fail.
This is all the programmer has to do; it is a fairly sim-
ple process. The most important thing to get right is the
selection of representative tests to perform the measure-
ments. If the test workload does not match the way the
program is actually used, the performed optimizations
might actually do more harm than good. For this reason,
is it often hard to use PGO for libraries. Libraries can
be used in many–sometimes widely different–scenarios.
Unless the use cases are indeed similar, it is usually bet-
ter to rely exclusively on static branch prediction using
__builtin_expect.
A few words on the .gcno and .gcda files. These are
binary files which are not immediately usable for inspec-
tion. It is possible, though, to use the gcov tool, which is
also part of the gcc package, to examine them. This tool
is mainly used for coverage analysis (hence the name) but
the file format used is the same as for PGO. The gcov tool
generates output files with the extension .gcov for each
source file with executed code (this might include sys-
tem headers). The files are source listings which are an-
notated, according to the parameters given to gcov, with
branch counter, probabilities, etc.
Ulrich Drepper
 Version 1.0
 85
0 0x3000000000 C
1 0x 7FF000000 D
2 0x3000001000 C
3 0x3000219000 D
4 0x300021A000 D
5 0x3000008000 C
6 0x3000012000 C
7 0x3000013000 C
8 0x3000014000 C
9 0x3000002000 C
0 0x3000000B50: (within /lib64/ld-2.5.so)
3320 0x3000000B53: (within /lib64/ld-2.5.so)
58270 0x3000001080: _dl_start (in /lib64/ld-2.5.so)
128020 0x30000010AE: _dl_start (in /lib64/ld-2.5.so)
132170 0x30000010B5: _dl_start (in /lib64/ld-2.5.so)
10489930 0x3000008B20: _dl_setup_hash (in /lib64/ld-2.5.so)
13880830 0x3000012CC0: _dl_sysdep_start (in /lib64/ld-2.5.so)
18091130 0x3000013440: brk (in /lib64/ld-2.5.so)
19123850 0x3000014020: strlen (in /lib64/ld-2.5.so)
23772480 0x3000002450: dl_main (in /lib64/ld-2.5.so)
Figure 7.8: Output of the pagein Tool
7.5
 Page Fault Optimization
On operating systems like Linux with demand-paging
support, an mmap call only modifies the page tables. It
makes sure that, for file-backed pages, the underlying
data can be found and, for anonymous memory, that, on
access, pages initialized with zeros are provided. No ac-
tual memory is allocated at the time of the mmap call.46
The allocation part happens when a memory page is first
accessed, either by reading or writing data, or by exe-
cuting code. In response to the ensuing page fault, the
kernel takes control and determines, using the page table
tree, the data which has to be present on the page. This
resolution of the page fault is not cheap, but it happens
for every single page which is used by a process.
To minimize the cost of page faults, the total number of
used pages has to be reduced. Optimizing the code for
size will help with this. To reduce the cost of a spe-
cific code path (for instance, the start-up code), it is also
possible to rearrange code so that, in that code path, the
number of touched pages is minimized. It is not easy to
determine the right order, though.
The author wrote a tool, based on the valgrind toolset,
to measure page faults as they happen. Not the num-
ber of page faults, but the reason why they happen. The
pagein tool emits information about the order and tim-
ing of page faults. The output, written to a file named
pagein.<PID>, looks as in Figure 7.8. The second col-
umn specifies the address of the page which is paged-
in. Whether it is a code or data page is indicated in
the third column, which contains ‘C’ or ‘D’ respectively.
The fourth column specifies the number of cycles which
passed since the first page fault. The rest of the line is
valgrind’s attempt to find a name for the address which
caused the page fault. The address value itself is correct
but the name is not always accurate if no debug informa-
tion is available.
In the example in Figure 7.8, execution starts at address
3000000B5016 , which forces the system to page in the
page at address 300000000016 . Shortly after that, the
page after this is also brought in; the function called on
46If you want to say “Wrong!” wait a second, it will be qualified
later that there are exceptions.
that page is _dl_start. The initial code accesses a vari-
able on page 7FF00000016 . This happens just 3,320 cy-
cles after the first page fault and is most likely the second
instruction of the program (just three bytes after the first
instruction). If one looks at the program, one will notice
that there is something peculiar about this memory ac-
cess. The instruction in question is a call instruction,
which does not explicitly load or store data. It does store
the return address on the stack, though, and this is ex-
actly what happens here. This is not the official stack of
the process, though, it is valgrind’s internal stack of the
application. This means when interpreting the results of
pagein it is important to keep in mind that valgrind intro-
duces some artifacts.
The output of pagein can be used to determine which
code sequences should ideally be adjacent in the pro-
gram code. A quick look at the /lib64/ld-2.5.so
code shows that the first instructions immediately call
the function _dl_start, and that these two places are
on different pages. Rearranging the code to move the
code sequences onto the same page can avoid–or at least
delay–a page fault. It is, so far, a cumbersome process to
determine what the optimal code layout should be. Since
the second use of a page is, by design, not recorded, one
needs to use trial and error to see the effects of a change.
Using call graph analysis, it is possible to guess about
possible call sequences; this might help speed up the pro-
cess of sorting the functions and variables.
At a very coarse level, the call sequences can be seen
by looking a the object files making up the executable or
DSO. Starting with one or more entry points (i.e., func-
tion names), the chain of dependencies can be computed.
Without much effort this works well at the object file
level. In each round, determine which object files con-
tain needed functions and variables. The seed set has to
be specified explicitly. Then determine all undefined ref-
erences in those object files and add them to the set of
needed symbols. Repeat until the set is stable.
The second step in the process is to determine an order.
The various object files have to be grouped together to fill
as few pages as possible. As an added bonus, no function
should cross over a page boundary. A complication in all
this is that, to best arrange the object files, it has to be
known what the linker will do later. The important fact
86
 Version 1.0
 What Every Programmer Should Know About Memory
here is that the linker will put the object files into the exe-
cutable or DSO in the same order in which they appear in
the input files (e.g., archives), and on the command line.
This gives the programmer sufficient control.
For those who are willing to invest a bit more time, there
have been successful attempts at reordering made using
automatic call tracing via the __cyg_profile_func_-
enter and __cyg_profile_func_exit hooks gcc in-
serts when called with the -finstrument-functions
option [17]. See the gcc manual for more information
on these __cyg_* interfaces. By creating a trace of the
program execution, the programmer can more accurately
determine the call chains. The results in [17] are a 5%
decrease in start-up costs, just through reordering of the
functions. The main benefit is the reduced number of
page faults, but the TLB cache also plays a role–an in-
creasingly important role given that, in virtualized envi-
ronments, TLB misses become significantly more expen-
sive.
By combining the analysis of the pagein tool with the
call sequence information, it should be possible to opti-
mize certain phases of the program (such as start-up) to
minimize the number of page faults.
The Linux kernel provides two additional mechanisms to
avoid page faults. The first one is a flag for mmap which
instructs the kernel to not only modify the page table but,
in fact, to pre-fault all the pages in the mapped area. This
is achieved by simply adding the MAP_POPULATE flag to
the fourth parameter of the mmap call. This will cause
the mmap call to be significantly more expensive, but, if
all pages which are mapped by the call are being used
right away, the benefits can be large. Instead of having a
number of page faults, which each are pretty expensive
due to the overhead incurred by synchronization require-
ments etc., the program would have one, more expensive,
mmap call. The use of this flag has disadvantages, though,
in cases where a large portion of the mapped pages are
not used soon (or ever) after the call. Mapped, unused
pages are obviously a waste of time and memory. Pages
which are immediately pre-faulted and only much later
used also can clog up the system. The memory is allo-
cated before it is used and this might lead to shortages of
memory in the meantime. On the other hand, in the worst
case, the page is simply reused for a new purpose (since
it has not been modified yet), which is not that expensive
but still, together with the allocation, adds some cost.
The granularity of MAP_POPULATE is simply too coarse.
And there is a second possible problem: this is an op-
timization; it is not critical that all pages are, indeed,
mapped in. If the system is too busy to perform the op-
eration the pre-faulting can be dropped. Once the page
is really used the program takes the page fault, but this
is not worse than artificially creating resource scarcity.
An alternative is to use the POSIX_MADV_WILLNEED ad-
vice with the posix_madvise function. This is a hint to
the operating system that, in the near future, the program
will need the page described in the call. The kernel is
free to ignore the advice, but it also can pre-fault pages.
The advantage here is that the granularity is finer. Indi-
vidual pages or page ranges in any mapped address space
area can be pre-faulted. For memory-mapped files which
contain a lot of data which is not used at runtime, this can
have huge advantages over using MAP_POPULATE.
Beside these active approaches to minimizing the num-
ber of page faults, it is also possible to take a more pas-
sive approach which is popular with the hardware design-
ers. A DSO occupies neighboring pages in the address
space, one range of pages each for the code and the data.
The smaller the page size, the more pages are needed to
hold the DSO. This, in turn, means more page faults, too.
Important here is that the opposite is also true. For larger
page sizes, the number of necessary pages for the map-
ping (or anonymous memory) is reduced; with it falls the
number of page faults.
Most architectures support page sizes of 4k. On IA-64
and PPC64, page sizes of 64k are also popular. That
means the smallest unit in which memory is given out
is 64k. The value has to be specified when compiling the
kernel and cannot be changed dynamically (at least not
at the moment). The ABIs of the multiple-page-size ar-
chitectures are designed to allow running an application
with either page size. The runtime will make the nec-
essary adjustments, and a correctly-written program will
not notice a thing. Larger page sizes mean more waste
through partially-used pages, but, in some situations, this
is OK.
Most architectures also support very large page sizes of
1MB or more. Such pages are useful in some situations,
too, but it makes no sense to have all memory given out
in units that large. The waste of physical RAM would
simply be too large. But very large pages have their ad-
vantages: if huge data sets are used, storing them in 2MB
pages on x86-64 would require 511 fewer page faults (per
large page) than using the same amount of memory with
4k pages. This can make a big difference. The solution is
to selectively request memory allocation which, just for
the requested address range, uses huge memory pages
and, for all the other mappings in the same process, uses
the normal page size.
Huge page sizes come with a price, though. Since the
physical memory used for large pages must be contin-
uous, it might, after a while, not be possible to allo-
cate such pages due to memory fragmentation. People
are working on memory defragmentation and fragmen-
tation avoidance, but it is very complicated. For large
pages of, say, 2MB the necessary 512 consecutive pages
are always hard to come by, except at one time: when
the system boots up. This is why the current solution
for large pages requires the use of a special filesystem,
hugetlbfs. This pseudo filesystem is allocated on re-
quest by the system administrator by writing the number
of huge pages which should be reserved to
/proc/sys/vm/nr_hugepages
Ulrich Drepper
 Version 1.0
 87
This operation might fail if not enough continuous mem-
ory can be located. The situation gets especially interest-
ing if virtualization is used. A virtualized system using
the VMM model does not directly administrate physi-
cal memory and, therefore, cannot by itself allocate the
hugetlbfs. It has to rely on the VMM, and this feature
is not guaranteed to be supported. For the KVM model,
the Linux kernel running the KVM module can perform
the hugetlbfs allocation and possibly pass a subset of
the pages thus allocated on to one of the guest domains.
Later, when a program needs a large page, there are mul-
tiple possibilities:
• the program can use the System V shared memory
interfaces with the SHM_HUGETLB flag.
• a filesystem of type hugetlbfs can actually be
mounted and the program can then create a file un-
der the mount point and use mmap to map one or
more pages as anonymous memory.
In the first case, the hugetlbfs need not be mounted.
Code requesting one or more large pages could look like
this:
key_t k = ftok("/some/key/file", 42);
int id = shmget(k, LENGTH,
SHM_HUGETLB|IPC_CREAT
|SHM_R|SHM_W);
void *a = shmat(id, NULL, 0);
The critical parts of this code sequence are the use of the
SHM_HUGETLB flag and the choice of the right value for
LENGTH, which must be a multiple of the huge page size
for the system. Different architectures have different val-
ues. The use of the System V shared memory interface
has the nasty problem of depending on the key argument
to differentiate (or share) mappings. The ftok interface
can easily produce conflicts which is why, if possible, it
is better to use other mechanisms.
If the requirement to mount the hugetlbfs filesystem is
not a problem, it is better to use it instead of System V
shared memory. The only real problems with using the
special filesystem are that the kernel must support it, and
that there is no standardized mount point yet. Once the
filesystem is mounted, for instance at /dev/hugetlb, a
program can make easy use of it:
int fd = open("/dev/hugetlb/file1",
O_RDWR|O_CREAT, 0700);
void *a = mmap(NULL, LENGTH,
PROT_READ|PROT_WRITE,
fd, 0);
By using the same file name in the open call, multiple
processes can share the same huge pages and collaborate.
It is also possible to make the pages executable, in which
case the PROT_EXEC flag must also be set in the mmap
call. As in the System V shared memory example, the
value of LENGTH must be a multiple of the system’s huge
page size.
A defensively-written program (as all programs should
be) can determine the mount point at runtime using a
function like this:
char *hugetlbfs_mntpoint(void) {
char *result = NULL;
FILE *fp = setmntent(_PATH_MOUNTED, "r");
if (fp != NULL) {
struct mntent *m;
while ((m = getmntent(fp)) != NULL)
if (strcmp(m->mnt_fsname,
"hugetlbfs") == 0) {
result = strdup(m->mnt_dir);
break;
}
endmntent(fp);
}
return result;
}
More information for both these cases can be found in
the hugetlbpage.txt file which comes as part of the kernel
source tree. The file also describes the special handling
needed for IA-64.
450
400
t
 n 350
em 300
el E 250
ts i 200
L/ s 150
el c 100
yC 50
0
210
 213 216 219 222 225
 228
Working Set Size (Bytes)
4kB Pages
 2MB Pages
Figure 7.9: Follow with Huge Pages, NPAD=0
To illustrate the advantages of huge pages, Figure 7.9
shows the results of running the random Follow test for
NPAD=0. This is the same data shown in Figure 3.15,
but, this time, we measure the data also with memory al-
located in huge pages. As can be seen the performance
advantage can be huge. For 220 bytes the test using huge
pages is 57% faster. This is due to the fact that this size
88
 Version 1.0
 What Every Programmer Should Know About Memory
still fits completely into one single 2MB page and, there-
fore, no DTLB misses occur.
After this point, the winnings are initially smaller but
grow again with increasing working set size. The huge
pages test is 38% faster for the 512MB working set size.
The curve for the huge page test has a plateau at around
250 cycles. Beyond working sets of 227 bytes, the num-
bers rise significantly again. The reason for the plateau
is that 64 TLB entries for 2MB pages cover 227 bytes.
As these numbers show, a large part of the costs of using
large working set sizes comes from TLB misses. Using
the interfaces described in this section can pay off big-
time. The numbers in the graph are, most likely, upper
limits, but even real-world programs show a significant
speed-up. Databases, since they use large amounts of
data, are among the programs which use huge pages to-
day.
There is currently no way to use large pages to map file-
backed data. There is interest in implementing this ca-
pability, but the proposals made so far all involve explic-
itly using large pages, and they rely on the hugetlbfs
filesystem. This is not acceptable: large page use in this
case must be transparent. The kernel can easily deter-
mine which mappings are large and automatically use
large pages. A big problem is that the kernel does not al-
ways know about the use pattern. If the memory, which
could be mapped as a large page, later requires 4k-page
granularity (for instance, because the protection of parts
of the memory range is changed using mprotect) a lot
of precious resources, in particular the linear physical
memory, will have been wasted. So it will certainly be
some more time before such an approach is successfully
implemented.
8
 Upcoming Technology
In the preceding sections about multi-processor handling
we have seen that significant performance problems must
be expected if the number of CPUs or cores is scaled up.
But this scaling-up is exactly what has to be expected in
the future. Processors will get more and more cores, and
programs must be ever more parallel to take advantage
of the increased potential of the CPU, since single-core
performance will not rise as quickly as it used to.
8.1
 The Problem with Atomic Operations
Synchronizing access to shared data structures is tradi-
tionally done in two ways:
• through mutual exclusion, usually by using func-
tionality of the system runtime to achieve just that;
• by using lock-free data structures.
The problem with lock-free data structures is that the pro-
cessor has to provide primitives which can perform the
entire operation atomically. This support is limited. On
most architectures support is limited to atomically read
and write a word. There are two basic ways to imple-
ment this (see section 6.4.2):
• using atomic compare-and-exchange (CAS) oper-
ations;
• using a load lock/store conditional (LL/SC) pair.
It can be easily seen how a CAS operation can be im-
plemented using LL/SC instructions. This makes CAS
operations the building block for most atomic operations
and lock free data structures.
Some processors, notably the x86 and x86-64 architec-
tures, provide a far more elaborate set of atomic opera-
tions. Many of them are optimizations of the CAS op-
eration for specific purposes. For instance, atomically
adding a value to a memory location can be implemented
using CAS and LL/SC operations, but the native support
for atomic increments on x86/x86-64 processors is faster.
It is important for programmers to know about these op-
erations, and the intrinsics which make them available
when programming, but that is nothing new.
The extraordinary extension of these two architectures
is that they have double-word CAS (DCAS) operations.
This is significant for some applications but not all (see
[5]). As an example of how DCAS can be used, let us try
to write a lock-free array-based stack/LIFO data struc-
ture. A first attempt using gcc’s intrinsics can be seen in
Figure 8.1.
This code is clearly not thread-safe. Concurrent accesses
in different threads will modify the global variable top
Ulrich Drepper
 Version 1.0
 89
struct elem {
data_t d;
struct elem *c;
};
struct elem *top;
void push(struct elem *n) {
n->c = top;
top = n;
}
struct elem *pop(void) {
struct elem *res = top;
if (res != NULL)
top = res->c;
return res;
}
Figure 8.1: Not Thread-Safe LIFO
without consideration of other threads’s modifications.
Elements could be lost or removed elements can magi-
cally reappear. It is possible to use mutual exclusion but
here we will try to use only atomic operations.
The first attempt to fix the problem uses CAS operations
when installing or removing list elements. The resulting
code looks like Figure 8.2.
#define CAS __sync_bool_compare_and_swap
struct elem {
data_t d;
struct elem *c;
};
struct elem *top;
void push(struct elem *n) {
do
n->c = top;
while (!CAS(&top, n->c, n));
}
struct elem *pop(void) {
struct elem *res;
while ((res = top) != NULL)
if (CAS(&top, res, res->c))
break;
return res;
}
1. l = pop()
2. push(newelem)
3. push(l)
The end effect of this operation is that the former top
element of the LIFO is back at the top but the second
element is different. Back in the first thread, because
the top element is unchanged, the CAS operation will
succeed. But the value res->c is not the right one. It is
a pointer to the second element of the original LIFO and
not newelem. The result is that this new element is lost.
In the literature [10] you find suggestions to use a feature
found on some processors to work around this problem.
Specifically, this is about the ability of the x86 and x86-
64 processors to perform DCAS operations. This is used
in the third incarnation of the code in Figure 8.3.
#define CAS __sync_bool_compare_and_swap
struct elem {
data_t d;
struct elem *c;
};
struct lifo {
struct elem *top;
size_t gen;
} l;
void push(struct elem *n) {
struct lifo old, new;
do {
old = l;
new.top = n->c = old.top;
new.gen = old.gen + 1;
} while (!CAS(&l, old, new));
}
struct elem *pop(void) {
struct lifo old, new;
do {
old = l;
if (old.top == NULL) return NULL;
new.top = old.top->c;
new.gen = old.gen + 1;
} while (!CAS(&l, old, new));
return old.top;
}
Figure 8.2: LIFO using CAS
At first glance this looks like a working solution. top is
never modified unless it matches the element which was
at the top of the LIFO when the operation started. But
we have to take concurrency at all levels into account. It
might be that another thread working on the data struc-
ture is scheduled at the worst possible moment. One such
case here is the so-called ABA problem. Consider what
happens if a second thread is scheduled right before the
CAS operation in pop and it performs the following op-
eration:
Figure 8.3: LIFO using double-word CAS
Unlike the other two examples, this is (currently) pseudo-
code since gcc does not grok the use of structures in the
CAS intrinsics. Regardless, the example should be suf-
ficient understand the approach. A generation counter
is added to the pointer to the top of the LIFO. Since it is
changed on every operation, push or pop, the ABA prob-
lem described above is no longer a problem. By the time
the first thread is resuming its work by actually exchang-
ing the top pointer, the generation counter has been in-
cremented three times. The CAS operation will fail and,
in the next round of the loop, the correct first and second
90
 Version 1.0
 What Every Programmer Should Know About Memory
element of the LIFO are determined and the LIFO is not
corrupted. Voilà.
Is this really the solution? The authors of [10] certainly
make it sound like it and, to their credit, it should be men-
tioned that it is possible to construct data structures for
the LIFO which would permit using the code above. But,
in general, this approach is just as doomed as the previ-
ous one. We still have concurrency problems, just now
in a different place. Let us assume a thread executes pop
and is interrupted after the test for old.top == NULL.
Now a second thread uses pop and receives ownership
of the previous first element of the LIFO. It can do any-
thing with it, including changing all values or, in case of
dynamically allocated elements, freeing the memory.
Now the first thread resumes. The old variable is still
filled with the previous top of the LIFO. More specifi-
cally, the top member points to the element popped by
the second thread. In new.top = old.top->c the first
thread dereferences a pointer in the element. But the ele-
ment this pointer references might have been freed. That
part of the address space might be inaccessible and the
process could crash. This cannot be allowed for a generic
data type implementation. Any fix for this problem is ter-
ribly expensive: memory must never be freed, or at least
it must be verified that no thread is referencing the mem-
ory anymore before it is freed. Given that lock-free data
structures are supposed to be faster and more concurrent,
these additional requirements completely destroy any ad-
vantage. In languages which support it, memory han-
dling through garbage collection can solve the problem,
but this comes with its price.
The situation is often worse for more complex data struc-
tures. The same paper cited above also describes a FIFO
implementation (with refinements in a successor paper).
But this code has all the same problems. Because CAS
operations on existing hardware (x86, x86-64)47 are lim-
ited to modifying two words which are consecutive in
memory, they are no help at all in other common situa-
tions. For instance, atomically adding or removing ele-
ments anywhere in a double-linked list is not possible.
The problem is that more than one memory address is
generally involved, and only if none of the values of these
addresses is changed concurrently can the entire oper-
ation succeed. This is a well-known concept in data-
base handling, and this is exactly where one of the most
promising proposals to solve the dilemma comes from.
8.2
 Transactional Memory
In their groundbreaking 1993 paper [13] Herlihy and Moss
propose to implement transactions for memory opera-
tions in hardware since software alone cannot deal with
the problem efficiently. Digital Equipment Corporation,
at that time, was already battling with scalability prob-
47As a side note, the developers of the IA-64 did not include this
feature. They allow comparing two words, but replacing only one.
lems on their high-end hardware, which featured a few
dozen processors. The principle is the same as for data-
base transactions: the result of a transaction becomes vis-
ible all at once or the transaction is aborted and all the
values remain unchanged.
This is where memory comes into play and why the pre-
vious section bothered to develop algorithms which use
atomic operations. Transactional memory is meant as
a replacement for–and extension of–atomic operations
in many situations, especially for lock-free data struc-
tures. Integrating a transaction system into the processor
sounds like a terribly complicated thing to do but, in fact,
most processors, to some extent, already have something
similar.
The LL/SC operations implemented by some processors
form a transaction. The SC instruction aborts or commits
the transaction based on whether the memory location
was touched or not. Transactional memory is an exten-
sion of this concept. Now, instead of a simple pair of in-
structions, multiple instructions take part in the transac-
tion. To understand how this can work, it is worthwhile to
first see how LL/SC instructions can be implemented.48
8.2.1
 Load Lock/Store Conditional Implementation
If the LL instruction is issued, the value of the memory
location is loaded into a register. As part of that oper-
ation, the value is loaded into L1d. The SC instruction
later can only succeed if this value has not been tampered
with. How can the processor detect this? Looking back
at the description of the MESI protocol in Figure 3.18
should make the answer obvious. If another processor
changes the value of the memory location, the copy of
the value in L1d of the first processor must be revoked.
When the SC instruction is executed on the first proces-
sor, it will find it has to load the value again into L1d.
This is something the processor must already detect.
There are a few more details to iron out with respect to
context switches (possible modification on the same pro-
cessor) and accidental reloading of the cache line after
a write on another processor. This is nothing that poli-
cies (cache flush on context switch) and extra flags, or
separate cache lines for LL/SC instructions, cannot fix.
In general, the LL/SC implementation comes almost for
free with the implementation of a cache coherence pro-
tocol like MESI.
8.2.2
 Transactional Memory Operations
For transactional memory to be generally useful, a trans-
action must not be finished with the first store instruction.
Instead, an implementation should allow a certain num-
ber of load and store operations; this means we need sep-
arate commit and abort instructions. In a bit we will see
that we need one more instruction which allows check-
48 This does not mean it is actually implemented like this.
Ulrich Drepper
 Version 1.0
 91
ing on the current state of the transaction and whether it
is already aborted or not.
There are three different memory operations to imple-
ment:
• Read memory
• Read memory which is written to later
• Write memory
When looking at the MESI protocol it should be clear
how this special second type of read operation can be
useful. The normal read can be satisfied by a cache line
in the ‘E’ and ‘S’ state. The second type of read opera-
tion needs a cache line in state ‘E’. Exactly why the sec-
ond type of memory read is necessary can be glimpsed
from the following discussion, but, for a more complete
description, the interested reader is referred to literature
about transactional memory, starting with [13].
In addition, we need transaction handling which mainly
consists of the commit and abort operation we are al-
ready familiar with from database transaction handling.
There is one more operation, though, which is optional
in theory but required for writing robust programs using
transactional memory. This instruction lets a thread test
whether the transaction is still on track and can (perhaps)
be committed later, or whether the transaction already
failed and will in any case be aborted.
We will discuss how these operations actually interact
with the CPU cache and how they match to bus operation.
But before we do that we take a look at some actual code
which uses transactional memory. This will hopefully
make the remainder of this section easier to understand.
8.2.3
 Example Code Using Transactional Memory
For the example we revisit our running example and pro-
vide a LIFO implementation which uses the transactional
memory primitives.
This code looks quite similar to the not-thread-safe code,
which is an additional bonus as it makes writing code us-
ing transactional memory easier. The new parts of the
code are the LTX, ST, COMMIT, and VALIDATE opera-
tions. These four operations are the way to request ac-
cesses to transactional memory. There is actually one
more operation, LT, which is not used here. LT requests
non-exclusive read access, LTX requests exclusive read
access, and ST is a store into transactional memory. The
VALIDATE operation checks whether the transaction is
still on track to be committed. It returns true if this trans-
action is still OK. If the transaction is already marked as
aborting, it will be actually aborted and a value indicating
this is returned. The next transactional memory instruc-
tion will start a new transaction. For this reason, the code
uses a new if block in case the transaction is still going
on.
struct elem {
data_t d;
struct elem *c;
};
struct elem *top;
void push(struct elem *n) {
while (1) {
n->c = LTX(top);
ST(&top, n);
if (COMMIT())
return;
... delay ...
}
}
struct elem *pop(void) {
while (1) {
struct elem *res = LTX(top);
if (VALIDATE()) {
if (res != NULL)
ST(&top, res->c);
if (COMMIT())
return res;
}
... delay ...
}
}
Figure 8.4: LIFO Using Transactional Memory
The COMMIT operation finishes the transaction; if it is fin-
ished successfully the operation returns true. This means
that this part of the program is done and the thread can
move on. If the operation returns a false value, this usu-
ally means the whole code sequence must be repeated.
This is what the outer while loop is doing here. This
is not absolutely necessary, though, in some cases giving
up on the work is the right thing to do.
The interesting point about the LT, LTX, and ST opera-
tions is that they can fail without signaling this failure in
any direct way. The way the program can request this
information is through the VALIDATE or COMMIT oper-
ation. For the load operation, this can mean that the
value actually loaded into the register might be bogus;
that is why it is necessary in the example above to use
VALIDATE before dereferencing the pointer. In the next
section, we will see why this is a wise choice for an im-
plementation. It might be that, once transactional mem-
ory is actually widely available, the processors will im-
plement something different. The results from [13] sug-
gest what we describe here, though.
The push function can be summarized as this: the trans-
action is started by reading the pointer to the head of the
list. The read requests exclusive ownership since, later in
the function, this variable is written to. If another thread
has already started a transaction, the load will fail and
mark the still-born transaction as aborted; in this case,
the value actually loaded might be garbage. This value
is, regardless of its status, stored in the next field of the
92
 Version 1.0
 What Every Programmer Should Know About Memory
new list member. This is fine since this member is not
yet in use, and it is accessed by exactly one thread. The
pointer to the head of the list is then assigned the pointer
to the new element. If the transaction is still OK, this
write can succeed. This is the normal case, it can only
fail if a thread uses some code other than the provided
push and pop functions to access this pointer. If the
transaction is already aborted at the time the ST is ex-
ecuted, nothing at all is done. Finally, the thread tries
to commit the transaction. If this succeeds the work is
done; other threads can now start their transactions. If
the transaction fails, it must be repeated from the begin-
ning. Before doing that, however, it is best to insert an
delay. If this is not done the thread might run in a busy
loop (wasting energy, overheating the CPU).
The pop function is slightly more complex. It also starts
with reading the variable containing the head of the list,
requesting exclusive ownership. The code then immedi-
ately checks whether the LTX operation succeeded or not.
If not, nothing else is done in this round except delaying
the next round. If the top pointer was read successfully,
this means its state is good; we can now dereference
the pointer. Remember, this was exactly the problem
with the code using atomic operations; with transactional
memory this case can be handled without any problem.
The following ST operation is only performed when the
LIFO is not empty, just as in the original, thread-unsafe
code. Finally the transaction is committed. If this suc-
ceeds the function returns the old pointer to the head;
otherwise we delay and retry. The one tricky part of this
code is to remember that the VALIDATE operation aborts
the transaction if it has already failed. The next trans-
actional memory operation would start a new transaction
and, therefore, we must skip over the rest of the code in
the function.
How the delay code works will be something to see when
implementations of transactional memory are available
in hardware. If this is done badly system performance
might suffer significantly.
8.2.4
 Bus Protocol for Transactional Memory
Now that we have seen the basic principles behind trans-
actional memory, we can dive into the details of the im-
plementation. Note that this is not based on actual hard-
ware. It is based on the original design of transactional
memory and knowledge about the cache coherency pro-
tocol. Some details are omitted, but it still should be pos-
sible to get insight into the performance characteristics.
Despite the name, transactional memory is not actually
implemented as separate memory; that would not make
any sense given that transactions on any location in a
thread’s address space are wanted. Instead, it is imple-
mented as part of the first level cache handling. The im-
plementation could, in theory, happen in the normal L1d
but, as [13] points out, this is not a good idea. We will
more likely see the transaction cache implemented in par-
allel to L1d. All accesses will use the higher level cache
in the same way they use L1d. The transaction cache is
likely much smaller than L1d. If it is fully associative its
size is determined by the number of operations a trans-
action can comprise. Implementations will likely have
limits for the architecture and/or specific processor ver-
sion. One could easily imagine a transaction cache with
16 elements or even less. In the above example we only
needed one single memory location; algorithms with a
larger transaction working sets get very complicated. It is
possible that we will see processors which support more
than one active transaction at any one time. The num-
ber of elements in the cache then multiplies, but it is still
small enough to be fully associative.
The transaction cache and L1d are exclusive. That means
a cache line is in, at most, one of the caches but never in
both. Each slot in the transaction cache is in, at any one
time, one of the four MESI protocol states. In addition
to this, a slot has an transaction state. The states are as
follows (names according to [13]):
EMPTY the cache slot contains no data. The MESI
state is always ‘I’.
NORMAL the cache slot contains committed data. The
data could as well exist in L1d. The MESI state
can be ‘M’, ‘E’, and ‘S’. The fact that the ‘M’ state
is allowed means that transaction commits do not
force the data to be written into the main memory
(unless the memory region is declared as uncached
or write-through). This can significantly help to
increase performance.
XABORT the cache slot contains data which is to be
discarded on abort. This is obviously the opposite
of XCOMMIT. All the data created during a trans-
action is kept in the transaction cache, nothing is
written to main memory before a commit. This
limits the maximum transaction size but it means
that, beside the transaction cache, no other mem-
ory has to be aware of the XCOMMIT/XABORT
duality for a single memory location. The possible
MESI states are ‘M’, ‘E’, and ‘S’.
XCOMMIT the cache slot contains data which is dis-
carded on commit. This is a possible optimization
processors could implement. If a memory loca-
tion is changed using a transaction operation, the
old content cannot be just dropped: if the transac-
tion fails the old content needs to be restored. The
MESI states are the same as for XABORT. One
difference with regard to XABORT is that, if the
transaction cache is full, any XCOMMIT entries
in the ‘M’ state could be written back to memory
and then, for all states, discarded.
When an LT operation is started, the processor allocates
two slots in the cache. Victims are chosen by first looking
for NORMAL slots for the address of the operation, i.e.,
Ulrich Drepper
 Version 1.0
 93
a cache hit. If such an entry is found, a second slot is
located, the value copied, one entry is marked XABORT,
and the other one is marked XCOMMIT.
If the address is not already cached, EMPTY cache slots
are located. If none can be found, NORMAL slots are
looked for. The old content must then be flushed to mem-
ory if the MESI state is ‘M’. If no NORMAL slot is
available either, it is possible to victimize XCOMMIT
entries. This is likely going to be an implementation de-
tail, though. The maximum size of a transaction is de-
termined by the size of the transaction cache, and, since
the number of slots which are needed for each operation
in the transaction is fixed, the number of transactions can
be capped before having to evict XCOMMIT entries.
If the address is not found in the transactional cache, a
T READ request is issued on the bus. This is just like
the normal READ bus request, but it indicates that this
is for the transactional cache. Just like for the normal
READ request, the caches in all other processors first get
the chance to respond. If none does the value is read
from the main memory. The MESI protocol determines
whether the state of the new cache line is ‘E’ or ‘S’. The
difference between T READ and READ comes into play
when the cache line is currently in use by an active trans-
action on another processor or core. In this case the T -
READ operation plainly fails, no data is transmitted. The
transaction which generated the T READ bus request is
marked as failed and the value used in the operation (usu-
ally a simple register load) is undefined. Looking back
to the example, we can see that this behavior does not
cause problems if the transactional memory operations
are used correctly. Before a value loaded in a transaction
is used, it must be verified with VALIDATE. This is, in
almost no cases, an extra burden. As we have seen in the
attempts to create a FIFO implementation using atomic
operations, the check which we added is the one missing
feature which would make the lock-free code work.
The LTX operation is almost identical to LT. The one dif-
ference is that the bus operation is T RFO instead of T -
READ. T RFO, like the normal RFO bus message, re-
quests exclusive ownership of the cache line. The state
of the resulting cache line is ‘E’. Like the T READ bus
request, T RFO can fail, in which case the used value is
undefined, too. If the cache line is already in the local
transaction cache with ‘M’ or ‘E’ state, nothing has to be
done. If the state in the local transaction cache is ‘S’ the
bus request has to go out to invalidate all other copies.
The ST operation is similar to LTX. The value is first
made available exclusively in the local transaction cache.
Then the ST operation makes a copy of the value into a
second slot in the cache and marks the entry as XCOM-
MIT. Lastly, the other slot is marked as XABORT and
the new value is written into it. If the transaction is al-
ready aborted, or is newly aborted because the implicit
LTX fails, nothing is written.
Neither the VALIDATE nor COMMIT operations automat-
ically and implicitly create bus operations. This is the
huge advantage transactional memory has over atomic
operations. With atomic operations, concurrency is made
possible by writing changed values back into main mem-
ory. If you have read this document thus far, you should
know how expensive this is. With transactional memory,
no accesses to the main memory are forced. If the cache
has no EMPTY slots, current content must be evicted,
and for slots in the ‘M’ state, the content must be writ-
ten to main memory. This is not different from regular
caches, and the write-back can be performed without spe-
cial atomicity guarantees. If the cache size is sufficient,
the content can survive for a long time. If transactions
are performed on the same memory location over and
over again, the speed improvements can be astronomical
since, in the one case, we have one or two main memory
accesses in each round while, for transactional memory,
all accesses hit the transactional cache, which is as fast
as L1d.
All the VALIDATE and COMMIT operations do at the time
of an abort of a transaction is to mark the cache slots
marked XABORT as empty and mark the XCOMMIT
slots as NORMAL. Similarly, when COMMIT successfully
finishes a transaction, the XCOMMIT slots are marked
empty and the XABORT slots are marked NORMAL.
These are very fast operations on the transaction cache.
No explicit notification to other processors which want to
perform transactions happens; those processors just have
to keep trying. Doing this efficiently is another matter. In
the example code above we simply have ...delay...
in the appropriate place. We might see actual processor
support for delaying in a useful way.
To summarize, transactional memory operations cause
bus operation only when a new transaction is started and
when a new cache line, which is not already in the trans-
action cache, is added to a still-successful transaction.
Operations in aborted transactions do not cause bus op-
erations. There will be no cache line ping-pong due to
multiple threads trying to use the same memory.
8.2.5
 Other Considerations
In section 6.4.2, we already discussed how the lock pre-
fix, available on x86 and x86-64, can be used to avoid the
coding of atomic operations in some situations. The pro-
posed tricks falls short, though, when there are multiple
threads in use which do not contend for the same mem-
ory. In this case, the atomic operations are used unnec-
essarily. With transactional memory this problem goes
away. The expensive RFO bus message are issued only
if memory is used on different CPUs concurrently or in
succession; this is only the case when they are needed. It
is almost impossible to do any better.
The attentive reader might have wondered about delays.
What is the expected worst case scenario? What if the
thread with the active transaction is descheduled, or if it
receives a signal and is possibly terminated, or decides to
94
 Version 1.0
 What Every Programmer Should Know About Memory
use siglongjmp to jump to an outer scope? The answer
to this is: the transaction will be aborted. It is possible to
abort a transaction whenever a thread makes a system call
or receives a signal (i.e., a ring level change occurs). It
might also be that aborting the transaction is part of the
OS’s duties when performing system calls or handling
signals. We will have to wait until implementations be-
come available to see what is actually done.
The final aspect of transactional memory which should
be discussed here is something which people might want
to think about even today. The transaction cache, like
other caches, operates on cache lines. Since the transac-
tion cache is an exclusive cache, using the same cache
line for transactions and non-transaction operation will
be a problem. It is therefore important to
• move non-transactional data off of the cache line
• have separate cache lines for data used in separate
transactions
The first point is not new, the same effort will pay off for
atomic operations today. The second is more problem-
atic since today objects are hardly ever aligned to cache
lines due to the associated high cost. If the data used,
along with the words modified using atomic operations,
is on the same cache line, one less cache line is needed.
This does not apply to mutual exclusion (where the mu-
tex object should always have its own cache line), but one
can certainly make cases where atomic operations go to-
gether with other data. With transactional memory, using
the cache line for two purposes will most likely be fatal.
Every normal access to data 49 would remove the cache
line from the transactional cache, aborting the transac-
tion in the process. Cache alignment of data objects will
be in future not only a matter of performance but also of
correctness.
It is possible that transactional memory implementations
will use more precise accounting and will, as a result, not
suffer from normal accesses to data on cache lines which
are part of a transaction. This requires a lot more effort,
though, since then the MESI protocol information is not
sufficient anymore.
8.3
 Increasing Latency
One thing about future development of memory tech-
nology is almost certain: latency will continue to creep
up. We already discussed, in section 2.2.4, that the up-
coming DDR3 memory technology will have higher la-
tency than the current DDR2 technology. FB-DRAM,
if it should get deployed, also has potentially higher la-
tency, especially when FB-DRAM modules are daisy-
chained. Passing through the requests and results does
not come for free.
49From the cache line in question. Access to arbitrary other cache
lines does not influence the transaction.
The second source of latency is the increasing use of
NUMA. AMD’s Opterons are NUMA machines if they
have more than one processor. There is some local mem-
ory attached to the CPU with its own memory controller
but, on SMP motherboards, the rest of the memory has to
be accessed through the Hypertransport bus. Intel’s CSI
technology will use almost the same technology. Due to
per-processor bandwidth limitations and the requirement
to service (for instance) multiple 10Gb/s Ethernet cards,
multi-socket motherboards will not vanish, even if the
number of cores per socket increases.
A third source of latency are co-processors. We thought
that we got rid of them after math co-processors for com-
modity processors were no longer necessary at the be-
ginning of the 1990’s, but they are making a comeback.
Intel’s Geneseo and AMD’s Torrenza are extensions of
the platform to allow third-party hardware developers to
integrate their products into the motherboards. I.e., the
co-processors will not have to sit on a PCIe card but, in-
stead, are positioned much closer to the CPU. This gives
them more bandwidth.
IBM went a different route (although extensions like In-
tel’s and AMD’s are still possible) with the Cell CPU.
The Cell CPU consists, beside the PowerPC core, of 8
Synergistic Processing Units (SPUs) which are special-
ized processors mainly for floating-point computation.
What co-processors and SPUs have in common is that
they, most likely, have even slower memory logic than
the real processors. This is, in part, caused by the nec-
essary simplification: all the cache handling, prefetching
etc is complicated, especially when cache coherency is
needed, too. High-performance programs will increas-
ingly rely on co-processors since the performance differ-
ences can be dramatic. Theoretical peak performance for
a Cell CPU is 210 GFLOPS, compared to 50-60 GFLOPS
for a high-end CPU. Graphics Processing Units (GPUs,
processors on graphics cards) in use today achieve even
higher numbers (north of 500 GFLOPS) and those could
probably, with not too much effort, be integrated into the
Geneseo/Torrenza systems.
As a result of all these developments, a programmer must
conclude that prefetching will become ever more impor-
tant. For co-processors it will be absolutely critical. For
CPUs, especially with more and more cores, it is neces-
sary to keep the FSB busy all the time instead of piling
on the requests in batches. This requires giving the CPU
as much insight into future traffic as possible through the
efficient use of prefetching instructions.
8.4
 Vector Operations
The multi-media extensions in today’s mainstream pro-
cessors implement vector operations only in a limited
fashion. Vector instructions are characterized by large
numbers of operations which are performed as the re-
sult of one instruction (Single Instruction Multiple Data,
SIMD). Compared with scalar operations, this can be
Ulrich Drepper
 Version 1.0
 95
said about the multi-media instructions, but it is a far
cry from what vector computers like the Cray-1 or vector
units for machines like the IBM 3090 did.
To compensate for the limited number of operations per-
formed for one instruction (four float or two double
operations on most machines) the surrounding loops have
to be executed more often. The example in section A.1
shows this clearly, each cache line requires SM iterations.
With wider vector registers and operations, the number
of loop iterations can be reduced. This results in more
than just improvements in the instruction decoding etc.;
here we are more interested in the memory effects. With
a single instruction loading or storing more data, the pro-
cessor has a better picture about the memory use of the
application and does not have to try to piece together the
information from the behavior of individual instructions.
Furthermore, it becomes more useful to provide load or
store instructions which do not affect the caches. With
16 byte wide loads of an SSE register in an x86 CPU, it
is a bad idea to use uncached loads since later accesses to
the same cache line have to load the data from memory
again (in case of cache misses). If, on the other hand,
the vector registers are wide enough to hold one or more
cache lines, uncached loads or stores do not have nega-
tive impacts. It becomes more practical to perform oper-
ations on data sets which do not fit into the caches.
Having large vector registers does not necessarily mean
the latency of the instructions is increased; vector in-
structions do not have to wait until all data is read or
stored. The vector units could start with the data which
has already been read if it can recognize the code flow.
That means, if, for instance, a vector register is to be
loaded and then all vector elements multiplied by a scalar,
the CPU could start the multiplication operation as soon
as the first part of the vector has been loaded. It is just
a matter of sophistication of the vector unit. What this
shows is that, in theory, the vector registers can grow
really wide, and that programs could potentially be de-
signed today with this in mind. In practice, there are lim-
itations imposed on the vector register size by the fact
that the processors are used in multi-process and multi-
thread OSes. As a result, the context switch times, which
include storing and loading register values, is important.
With wider vector registers there is the problem that the
input and output data of the operations cannot be sequen-
tially laid out in memory. This might be because a ma-
trix is sparse, a matrix is accessed by columns instead of
rows, and many other factors. Vector units provide, for
this case, ways to access memory in non-sequential pat-
terns. A single vector load or store can be parametrized
and instructed to load data from many different places
in the address space. Using today’s multi-media instruc-
tions, this is not possible at all. The values would have
to be explicitly loaded one by one and then painstakingly
combined into one vector register.
The vector units of the old days had different modes to
allow the most useful access patterns:
• using striding, the program can specify how big
the gap between two neighboring vector elements
is. The gap between all elements must be the same
but this would, for instance, easily allow to read
the column of a matrix into a vector register in one
instruction instead of one instruction per row.
• using indirection, arbitrary access patterns can be
created. The load or store instruction receive a
pointer to an array which contains addresses or off-
sets of the real memory locations which have to be
loaded.
It is unclear at this point whether we will see a revival of
true vector operations in future versions of mainstream
processors. Maybe this work will be relegated to co-
processors. In any case, should we get access to vector
operations, it is all the more important to correctly or-
ganize the code performing such operations. The code
should be self-contained and replaceable, and the inter-
face should be general enough to efficiently apply vector
operations. For instance, interfaces should allow adding
entire matrixes instead of operating on rows, columns, or
even groups of elements. The larger the building blocks,
the better the chance of using vector operations.
In [11] the authors make a passionate plea for the revival
of vector operations. They point out many advantages
and try to debunk various myths. They paint an overly
simplistic image, though. As mentioned above, large reg-
ister sets mean high context switch times, which have to
be avoided in general purpose OSes. See the problems
of the IA-64 processor when it comes to context switch-
intensive operations. The long execution time for vector
operations is also a problem if interrupts are involved. If
an interrupt is raised, the processor must stop its current
work and start working on handling the interrupt. After
that, it must resume executing the interrupted code. It is
generally a big problem to interrupt an instruction in the
middle of the work; it is not impossible, but it is compli-
cated. For long running instructions this has to happen,
or the instructions must be implemented in a restartable
fashion, since otherwise the interrupt reaction time is too
high. The latter is not acceptable.
Vector units also were forgiving as far as alignment of
the memory access is concerned, which shaped the al-
gorithms which were developed. Some of today’s pro-
cessors (especially RISC processors) require strict align-
ment so the extension to full vector operations is not triv-
ial. There are big potential upsides to having vector oper-
ations, especially when striding and indirection are sup-
ported, so that we can hope to see this functionality in
the future.
96
 Version 1.0
 What Every Programmer Should Know About Memory
A
 Examples and Benchmark Programs
A.1 Matrix Multiplication
This is the complete benchmark program for the matrix multiplication in section section 6.2.1. For details on the
intrinsics used the reader is referred to Intel’s reference manual.
#include <stdlib.h>
#include <stdio.h>
#include <emmintrin.h>
#define N 1000
double res[N][N] __attribute__ ((aligned (64)));
double mul1[N][N] __attribute__ ((aligned (64)));
double mul2[N][N] __attribute__ ((aligned (64)));
#define SM (CLS / sizeof (double))
int
main (void)
{
// ... Initialize mul1 and mul2
int i, i2, j, j2, k, k2;
double *restrict rres;
double *restrict rmul1;
double *restrict rmul2;
for (i = 0; i < N; i += SM)
for (j = 0; j < N; j += SM)
for (k = 0; k < N; k += SM)
for (i2 = 0, rres = &res[i][j], rmul1 = &mul1[i][k]; i2 < SM;
++i2, rres += N, rmul1 += N)
{
_mm_prefetch (&rmul1[8], _MM_HINT_NTA);
for (k2 = 0, rmul2 = &mul2[k][j]; k2 < SM; ++k2, rmul2 += N)
{
__m128d m1d = _mm_load_sd (&rmul1[k2]);
m1d = _mm_unpacklo_pd (m1d, m1d);
for (j2 = 0; j2 < SM; j2 += 2)
{
__m128d m2 = _mm_load_pd (&rmul2[j2]);
__m128d r2 = _mm_load_pd (&rres[j2]);
_mm_store_pd (&rres[j2],
_mm_add_pd (_mm_mul_pd (m2, m1d), r2));
}
}
}
// ... use res matrix
return 0;
}
The structure of the loops is pretty much the same as in the final incarnation in section 6.2.1. The one big change is
that loading the rmul1[k2] value has been pulled out of the inner loop since we have to create a vector where both
elements have the same value. This is what the _mm_unpacklo_pd() intrinsic does.
The only other noteworthy thing is that we explicitly aligned the three arrays so that the values we expect to be in the
same cache line actually are found there.
Ulrich Drepper
 Version 1.0
 97
A.2
 Debug Branch Prediction
If, as recommended, the definitions of likely and unlikely from section 6.2.2 are used, it is easy50 to have a debug
mode to check whether the assumptions are really true. The definitions of the macros could be replaced with this:
#ifndef DEBUGPRED
# define unlikely(expr) __builtin_expect (!!(expr), 0)
# define likely(expr) __builtin_expect (!!(expr), 1)
#else
asm (".section predict_data, \"aw\"; .previous\n"
".section predict_line, \"a\"; .previous\n"
".section predict_file, \"a\"; .previous");
# ifdef __x86_64__
# define debugpred__(e, E) \
({ long int _e = !!(e); \
asm volatile (".pushsection predict_data\n" \
"..predictcnt%=: .quad 0; .quad 0\n" \
".section predict_line; .quad %c1\n" \
".section predict_file; .quad %c2; .popsection\n" \
"addq $1,..predictcnt%=(,%0,8)" \
: : "r" (_e == E), "i" (__LINE__), "i" (__FILE__)); \
__builtin_expect (_e, E); \
})
# elif defined __i386__
# define debugpred__(e, E) \
({ long int _e = !!(e); \
asm volatile (".pushsection predict_data\n" \
"..predictcnt%=: .long 0; .long 0\n" \
".section predict_line; .long %c1\n" \
".section predict_file; .long %c2; .popsection\n" \
"incl ..predictcnt%=(,%0,4)" \
: : "r" (_e == E), "i" (__LINE__), "i" (__FILE__)); \
__builtin_expect (_e, E); \
})
# else
# error "debugpred__ definition missing"
# endif
# define unlikely(expt) debugpred__ ((expr), 0)
# define likely(expr) debugpred__ ((expr), 1)
#endif
These macros use a lot of functionality provided by the GNU assembler and linker when creating ELF files. The first
asm statement in the DEBUGPRED case defines three additional sections; it mainly gives the assembler information about
how the sections should be created. All sections are available at runtime, and the predict_data section is writable.
It is important that all section names are valid C identifiers. The reason will be clear shortly.
The new definitions of the likely and unlikely macros refer to the machine-specific debugpred__ macro. This
macro has the following tasks:
1. allocate two words in the predict_data section to contain the counts for correct and incorrect predictions.
These two fields get a unique name through the use of %=; the leading dots makes sure the symbols do not
pollute the symbol table.
2. allocate one word in the predict_line section to contain the line number of the likely or unlikely macro
use.
50 At least with the GNU toolchain.
98
 Version 1.0
 What Every Programmer Should Know About Memory
3. allocate one word in the predict_file section to contain a pointer to the file name of the likely or unlikely
macro use.
4. increment the “correct” or “incorrect” counter created for this macro according to the actual value of the expres-
sion e. We do not use atomic operations here because they are massively slower and absolute precision in the
unlikely case of a collision is not that important. It is easy enough to change if wanted.
The .pushsection and .popsection pseudo-ops are described in the assembler manual. The interested reader is
asked to explore the details of these definition with the help of the manuals and some trial and error.
These macros automatically and transparently take care of collecting the information about correct and incorrect branch
predictions. What is missing is a method to get to the results. The simplest way is to define a destructor for the object
and print out the results there. This can be achieved with a function defined like this:
extern long int __start_predict_data;
extern long int __stop_predict_data;
extern long int __start_predict_line;
extern const char *__start_predict_file;
static void
__attribute__ ((destructor))
predprint(void)
{
long int *s = &__start_predict_data;
long int *e = &__stop_predict_data;
long int *sl = &__start_predict_line;
const char **sf = &__start_predict_file;
while (s < e) {
printf("%s:%ld: incorrect=%ld, correct=%ld%s\n", *sf, *sl, s[0], s[1],
s[0] > s[1] ? "
 <==== WARNING" : "");
++sl;
++sf;
s += 2;
}
}
Here the fact that the section names are valid C identifiers comes into play; it is used by the GNU linker to automatically
define, if needed, two symbols for the section. The __start_XYZ symbols corresponds to the beginning of the section
XYZ and __stop_XYZ is the location of the first byte following section XYZ. These symbols make it possible to iterate
over the section content at runtime. Note that, since the content of the sections can come from all the files the linker
uses at link time, the compiler and assembler do not have enough information to determine the size of the section. Only
with these magic linker-generated symbols is it possible to iterate over the section content.
The code does not iterate over one section only, though; there are three sections involved. Since we know that, for every
two words added to the predict_data section we add one word to each of the predict_line and predict_file
sections, we do not have to check the boundaries of these two sections. We just carry pointers along and increment
them in unison.
The code prints out a line for every prediction which appears in the code. It highlights those uses where the prediction
is incorrect. Of course, this can be changed, and the debug mode could be restricted to flag only the entries which have
more incorrect predictions than correct ones. Those are candidates for change. There are details which complicate the
issue; for example, if the branch prediction happens inside a macro which is used in multiple places, all the macro uses
must be considered together before making a final judgment.
Two last comments: the data required for this debugging operation is not small, and, in case of DSOs, expensive
(the predict_file section must be relocated). Therefore the debugging mode should not be enabled in production
binaries. Finally, each executable and DSO creates it own output, this must be kept in mind when analyzing the data.
Ulrich Drepper
 Version 1.0
 99
A.3
 Measure Cache Line Sharing Overhead
This section contains the test program to measure the overhead of using variables on the same cache line versus
variables on separate cache lines.
#include#include#include<error.h>
<pthread.h>
<stdlib.h>
#define N (atomic ? 10000000 : 500000000)
static int atomic;
static unsigned nthreads;
static unsigned disp;
static long **reads;
static pthread_barrier_t b;
static void *
tf(void *arg)
{
long *p = arg;
if (atomic)
for (int n = 0; n < N; ++n)
__sync_add_and_fetch(p, 1);
else
for (int n = 0; n < N; ++n)
{
*p += 1;
asm volatile("" : : "m" (*p));
}
}
return NULL;
int
main(int argc, char *argv[])
{
if (argc < 2)
disp = 0;
else
disp = atol(argv[1]);
if (argc < 3)
nthreads = 2;
else
nthreads = atol(argv[2]) ?: 1;
if (argc < 4)
atomic = 1;
else
atomic = atol(argv[3]);
pthread_barrier_init(&b, NULL, nthreads);
100
 Version 1.0
 What Every Programmer Should Know About Memory
void *p;
posix_memalign(&p, 64, (nthreads * disp ?: 1) * sizeof(long));
long *mem = p;
pthread_t th[nthreads];
pthread_attr_t a;
pthread_attr_init(&a);
cpu_set_t c;
for (unsigned i = 1; i < nthreads; ++i)
{
CPU_ZERO(&c);
CPU_SET(i, &c);
pthread_attr_setaffinity_np(&a, sizeof(c), &c);
mem[i * disp] = 0;
pthread_create(&th[i], &a, tf, &mem[i * disp]);
}
CPU_ZERO(&c);
CPU_SET(0, &c);
pthread_setaffinity_np(pthread_self(), sizeof(c), &c);
mem[0] = 0;
tf(&mem[0]);
if ((disp == 0 && mem[0] != nthreads * N)
|| (disp != 0 && mem[0] != N))
error(1,0,"mem[0] wrong: %ld instead of %d",
mem[0], disp == 0 ? nthreads * N : N);
for (unsigned i = 1; i < nthreads; ++i)
{
pthread_join(th[i], NULL);
if (disp != 0 && mem[i * disp] != N)
error(1,0,"mem[%u] wrong: %ld instead of %d", i, mem[i * disp], N);
}
}
return 0;
The code is provided here mainly as an illustration of how to write a program which measures effects like cache line
overhead. The interesting parts are the bodies of the loops in tf. The __sync_add_and_fetch intrinsic, known to
the compiler, generates an atomic add instruction. In the second loop we have to “consume” the result of the increment
(through the inline asm statement). The asm does not introduce any actual code; instead, it prevents the compiler from
lifting the increment operation out of the loop.
The second interesting part is that the program pins the threads onto specific processors. The code assumes the pro-
cessors are numbered 0 to 3, which is usually the case if the machine has four or more logical processors. The code
could have used the interfaces from libNUMA to determine the numbers of the usable processors, but this test program
should be widely usable without introducing this dependency. It is easy enough to fix up one way or another.
Ulrich Drepper
 Version 1.0
 101
B
 Some OProfile Tips
The following is not meant as a tutorial on how to use oprofile. There are entire documents written on that topic.
Instead it is meant to give a few higher-level hints on how to look at one’s programs to find possible trouble spots. But
before that we must at least have a minimal introduction.
B.1
 Oprofile Basics
Oprofile works in two phases: collection and then analysis. The collection is performed by the kernel; it cannot be
done at userlevel since the measurements use the performance counters of the CPU. These counters require access to
MSRs which, in turn, requires privileges.
Each modern processor provides its own set of performance counters. On some architectures a subset of the counters
are provided by all processor implementations while the others differ from version to version. This makes giving
general advice about the use of oprofile hard. There is not (yet) a higher-level abstraction for the counters which could
hide these details.
The processor version also controls how many events can be traced at any one time, and in which combination. This
adds yet more complexity to the picture.
If the user knows the necessary details about the performance counters, the opcontrol program can be used to select the
events which should be counted. For each event it is necessary to specify the “overrun number” (the number of events
which must occur before the CPU is interrupted to record an event), whether the event should be counted for userlevel
and/or the kernel, and finally a “unit mask” (it selects sub-functions of the performance counter).
To count the CPU cycles on x86 and x86-64 processors, one has to issue the following command:
opcontrol --event CPU_CLK_UNHALTED:30000:0:1:1
The number 30000 is the overrun number. Choosing a reasonable value is important for the behavior of the system
and the collected data. It is a bad idea ask to receive data about every single occurrence of the event. For many events,
this would bring the machine to a standstill since all it would do is work on the data collection for the event overrun;
this is why oprofile enforces a minimum value. The minimum values differ for each event since different events have
a different probability of being triggered in normal code.
Choosing a very high number reduces the resolution of the profile. At each overrun oprofile records the address of
the instruction which is executed at that moment; for x86 and PowerPC it can, under some circumstances, record the
backtrace as well.51 With a coarse resolution, the hot spots might not get a representative number of hits; it is all about
probabilities, which is why oprofile is called a probabilistic profiler. The lower the overrun number is the higher the
impact on the system in terms of slowdown but the higher the resolution.
If a specific program is to be profiled, and the system is not used for production, it is often most useful to use the lowest
possible overrun value. The exact value for each event can be queried using
opcontrol --list-events
This might be problematic if the profiled program interacts with another process, and the slowdown causes problems
in the interaction. Trouble can also result if a process has some realtime requirements which cannot be met when it is
interrupted often. In this case a middle ground has to be found. The same is true if the entire system is to be profiled
for extended periods of time. A low overrun number would mean the massive slowdowns. In any case, oprofile, like
any other profiling mechanism, introduces uncertainty and inaccuracy.
The profiling has to be started with opcontrol --start and can be stopped with opcontrol --stop. While
oprofile is active it collects data; this data is first collected in the kernel and then send to a userlevel daemon in batches,
where it is decoded and written to a filesystem. With opcontrol --dump it is possible to request all information
buffered in the kernel to be released to userlevel.
The collected data can contain events from different performance counters. The numbers are all kept in parallel unless
the user selects to wipe the stored data in between separate oprofile runs. It is possible to accumulate data from the
same event at different occasions. If an event is encountered during different profiling runs the numbers are added if
this is what is selected by the user.
The userlevel part of the data collection process demultiplexes the data. Data for each file is stored separately. It is
51 Backtrace support will hopefully be available for all architectures at some point.
102
 Version 1.0
 What Every Programmer Should Know About Memory
even possible to differentiate DSOs used by individual executable and, even, data for individual threads. The data
thus produced can be archived using oparchive. The file produced by this command can be transported to another
machine and the analysis can be performed there.
With the opreport program one can generate reports from the profiling results. Using opannotate it is possible to see
where the various events happened: which instruction and, if the data is available, in which source line. This makes it
easy to find hot spots. Counting CPU cycles will point out where the most time is spent (this includes cache misses)
while counting retired instructions allows finding where most of the executed instructions are–there is a big difference
between the two.
A single hit at an address usually has no meaning. A side effect of statistical profiling is that instructions which are
only executed a few times, or even only once, might be attributed with a hit. In such a case it is necessary to verify the
results through repetition.
B.2
 How It Looks Like
An oprofile session can look as simple as this:
$ opcontrol -i cachebench
$ opcontrol -e INST_RETIRED:6000:0:0:1 --start
$ ./cachebench ...
$ opcontrol -h
Note that these commands, including the actual program, are run as root. Running the program as root is done here only
for simplicity; the program can be executed by any user and oprofile would pick up on it. The next step is analyzing
the data. With opreport we see:
CPU: Core 2, speed 1596 MHz (estimated)
Counted INST_RETIRED.ANY_P events (number of instructions retired) with a unit mask of
0x00 (No unit mask) count 6000
INST_RETIRED:6000|
samples|
 %|
------------------
116452 100.000 cachebench
This means we collected a bunch of events; opannotate can now be used to look at the data in more detail. We can see
where in the program the most events were recorded. Part of the opannotate --source output looks like this:
:static void
:inc (struct l *l, unsigned n)
:{
: while (n-- > 0) /* inc total: 13980 11.7926 */
:
 {
5 0.0042 :
 ++l->pad[0].l;
13974 11.7875 :
 l = l->n;
1 8.4e-04 :
 asm volatile ("" :: "r" (l));
:
 }
:}
That is the inner function of the test, where a large portion of the time is spent. We see the samples spread out over
all three lines of the loop. The main reason for this is that the sampling is not always 100% accurate with respect
to the recorded instruction pointer. The CPU executes instructions out of order; reconstructing the exact sequence of
execution to produce a correct instruction pointer is hard. The most recent CPU versions try to do this for a select few
events but it is, in general, not worth the effort–or simply not possible. In most cases it does not really matter. The
programmer should be able to determine what is going on even if there is a normally-distributed set of samples.
B.3
 Starting To Profile
When starting to analyze a body of code, one certainly can start looking at the places in the program where the most
time is spent. That code should certainly be optimized as well as possible. But what happens next? Where is the
program spending unnecessary time? This question is not so easy to answer.
Ulrich Drepper
 Version 1.0
 103
One of the problems in this situation is that absolute values do not tell the real story. One loop in the program might
demand the majority of the time, and this is fine. There are many possible reasons for the high CPU utilization, though.
But what is more common, is that CPU usage is more evenly spread throughout the program. In this case, the absolute
values point to many places, which is not useful.
In many situations it is helpful to look at ratios of two events. For instance, the number of mispredicted branches in a
function can be meaningless if there is no measure for how often a function was executed. Yes, the absolute value is
relevant for the program’s performance. The ratio of mispredictions per call is more meaningful for the code quality
of the function. Intel’s optimization manual for x86 and x86-64 [15] describes ratios which should be investigated
(Appendix B.7 in the cited document for Core 2 events). A few of the ratios relevant for memory handling are the
following.
Instruction Fetch Stall
ITLB Miss Rate
L1I Miss Rate
L2 Instruction Miss Rate
Load Rate
Store Order Block
L1d Rate Blocking Loads
L1D Miss Rate
L2 Data Miss Rate
L2 Demand Miss Rate
Useful NTA Prefetch Rate
Late NTA Prefetch Rate
CYCLES_L1I_MEM_STALLED
/ CPU_CLK_UNHALTED.CORE
ITLB_MISS_RETIRED
 /
INST_RETIRED.ANY
L1I_MISSES
 /
 INST_-
RETIRED.ANY
L2_IFETCH.SELF.I_STATE
/ INST_RETIRED.ANY
L1D_CACHE_LD.MESI
CPU_CLK_UNHALTED.CORE
/
STORE_BLOCK.ORDER
 /
CPU_CLK_UNHALTED.CORE
LOAD_BLOCK.L1D / CPU_-
CLK_UNHALTED.CORE
L1D_REPL
 /
 INST_-
RETIRED.ANY
L2_LINES_IN.SELF.ANY /
INST_RETIRED.ANY
L2_LINES_-
IN.SELF.DEMAND / INST_-
RETIRED.ANY
SSE_PRE_MISS.NTA
 /
SSS_PRE_EXEC.NTA
LOAD_HIT_PRE
 /
 SSS_-
PRE_EXEC.NTA
Ratio of cycles during which in instruction decoder is
waiting for new data due to cache or ITLB misses.
ITLB misses per instruction. If this ratio is high the
code is spread over too many pages.
L1i misses per instruction. The execution flow is un-
predictable or the code size is too large. In the former
case avoiding indirect jumps might help. In the latter
case block reordering or avoiding inlining might help.
L2 misses for program code per instruction. Any
value larger than zero indicates code locality problems
which are even worse than L1i misses.
Read operations per cycle. A Core 2 core can service
one load operation. A high ratio means the execution
is bound by memory reads.
Ratio if stores blocked by previous stores which miss
the cache.
Loads from L1d blocked by lack of resources. Usually
this means too many concurrent L1d accesses.
L1d misses per instruction. A high rate means that
prefetching is not effective and L2 is used too often.
L2 misses for data per instruction. If the value is
significantly greater than zero, hardware and software
prefetching is ineffective. The processor needs more
(or earlier) software prefetching help.
L2 misses for data per instruction for which the hard-
ware prefetcher was not used at all. That means, pre-
fetching has not even started.
Ratio of useful non-temporal prefetch relative to the
total number of all non-temporal prefetches. A low
value means many values are already in the cache.
This ratio can be computed for the other prefetch types
as well.
Ratio of load requests for data with ongoing pre-
fetch relative to the total number of all non-temporal
prefetches. A high value means the software prefetch
instruction is issued too late. This ratio can be com-
puted for the other prefetch types as well.
For all these ratios, the program should be run with oprofile being instructed to measure both events. This guarantees
the two counts are comparable. Before the division, one has to make sure that the possibly different overrun values are
taken into account. The simplest way is to ensure this is by multiplying each events counter by the overrun value.
The ratios are meaningful for whole programs, at the executable/DSO level, or even at the function level. The deeper
one looks into the program, the more errors are included in the value.
What is needed to make sense of the ratios are baseline values. This is not as easy as it might seem. Different types of
code has different characteristics and a ratio value which is bad in one program might be normal in another program.
104
 Version 1.0
 What Every Programmer Should Know About Memory
C
 Memory Types
Though it is not necessary knowledge for efficient programming, it might be useful to describe some more technical
details of available memory types. Specifically we are here interested in the difference of “registered” versus “unregis-
tered” and ECC versus non-ECC DRAM types.
The terms “registered” and “buffered” are used synonymously when de-
scribing a DRAM type which has one additional component on the DRAM
module: a buffer. All DDR memory types can come in registered and un-
registered form. For the unregistered modules, the memory controller is
directly connected to all the chips on the module. Figure C.1 shows the
setup.
Electrically this is quite demanding. The memory controller must be able
to deal with the capacities of all the memory chips (there are more than the
six shown in the figure). If the memory controller (MC) has a limitation,
or if many memory modules are to be used, this setup is not ideal.
MC
Figure C.1: Unregistered DRAM Module
Buffered (or registered) memory changes the situation: instead of directly
connecting the RAM chips on the DRAM module to the memory, they
are connected to a buffer which, in turn, is then connected to the memory
controller. This significantly reduces the complexity of the electrical con-
nections. The ability of the memory controllers to drive DRAM modules
increases by a factor corresponding to the number of connections saved.
Buffer
With these advantages the question is: why aren’t all DRAM modules
 MC
buffered? There are several reasons. Obviously, buffered modules are
a bit more complicated and, hence, more expensive. Cost is not the only
Figure C.2: Registered DRAM Module
factor, though. The buffer delays the signals from the RAM chips a bit; the
delay must be high enough to ensure that all signals from the RAM chips are buffered. The result is that the latency of
the DRAM module increases. A last factor worth mentioning here is that the additional electrical component increases
the energy cost. Since the buffer has to operate at the frequency of the bus this component’s energy consumption can
be significant.
With the other factors of the use of DDR2 and DDR3 modules it is usually not possible to have more than two DRAM
modules per bank. The number of pins of the memory controller limit the number of banks (to two in commodity
hardware). Most memory controllers are able to drive four DRAM modules and, therefore, unregistered modules are
sufficient. In server environments with high memory requirements the situation might be different.
A different aspect of some server environments is that they cannot tolerate errors. Due to the minuscule charges held
by the capacitors in the RAM cells, errors are possible. People often joke about cosmic radiation but this is indeed a
possibility. Together with alpha decays and other natural phenomena, they lead to errors where the content of RAM
cell changes from 0 to 1 or vice versa. The more memory is used, the higher the probability of such an event.
If such errors are not acceptable, ECC (Error Correction Code) DRAM can be used. Error correction codes enable the
hardware to recognize incorrect cell contents and, in some cases, correct the errors. In the old days, parity checks only
recognized errors, and the machine had to be stopped when one was detected. With ECC, instead, a small number of
erroneous bits can be automatically corrected. If the number of errors is too high, though, the memory access cannot
be performed correctly and the machine still stops. This is a rather unlikely case for working DRAM modules, though,
since multiple errors must happen on the same module.
When we speak about ECC memory we are actually not quite correct. It is not the memory which performs the error
checking; instead, it is the memory controller. The DRAM modules simply provide more storage and transport the
additional non-data bits along with the real data. Usually, ECC memory stores one additional bit for each 8 data bits.
Why 8 bits are used will be explained a bit later.
Upon writing data to a memory address, the memory controller computes the ECC for the new content on the fly
before sending that data and ECC onto the memory bus. When reading, the data plus the ECC is received, the memory
controller computes the ECC for the data, and compares it with the ECC transmitted from the DRAM module. If the
ECCs match everything is fine. If they do not match, the memory controller tries to correct the error. If this correction
is not possible, the error is logged and the machine is possibly halted.
Ulrich Drepper
 Version 1.0
 105
SEC
 SEC/DED
Data Bits W
 ECC Bits E Overhead
 ECC Bits E Overhead
4
 3
 75.0%
 4
 100.0%
8
 4
 50.0%
 5
 62.5%
16
 5
 31.3%
 6
 37.5%
32
 6
 18.8%
 7
 21.9%
64
 7
 10.9%
 8
 12.5%
Table C.1: ECC and Data Bits Relationship
Several techniques for error correction are in use but, for DRAM ECC, usually Hamming codes are used. Hamming
codes originally were used to encode four data bits with the ability to recognize and correct one flipped bit (SEC, Single
Error Correction). The mechanism can easily be extended to more data bits. The relationship between the number of
data bits W and the number of bits for the error code E is described by the equation
E = dlog2(W + E + 1)e
Solving this equation iteratively results in the values shown in the second column of Table C.1. With an additional
bit, we can recognize two flipped bits using a simple parity bit. This is then called SEC/DED, Single Error Correc-
tion/Double Error Detection. With this additional bit we arrive at the values in the fourth column of Table C.1. The
overhead for W = 64 is sufficiently low and the numbers (64, 8) are multiples of 8, so this is a natural selection for
ECC. On most modules, each RAM chip produces 8 bits and, therefore, any other combination would lead to less
efficient solution.
The Hamming code computation is easy to demonstrate with a code
7
 6
 5
 4
 3
 2
 1
 using W = 4 and E = 3. We compute parity bits at strategic
ECC Word
 D
 D
 D
 P
 D
 P
 P
 places in the encoded word. Table C.2 shows the principle. At
P1 Parity
 D
 –
 D
 –
 D
 –
 P
 the bit positions corresponding to the powers of two the parity bits
P2 Parity
 D
 D
 –
 –
 D
 P
 –
 are added. The parity sum for the first parity bit P1 contains every
P4 Parity
 D
 D
 D
 P
 –
 –
 –
 second bit. The parity sum for the second parity bit P2 contains
data bits 1, 3, and 4 (encoded here as 3, 6, and 7). Similar ly P4 is
Table C.2: Hamming Generation Matrix Con-
 computed.
struction
The computation of the parity bits can be more elegantly described
using a matrix multiplication. We construction a matrix G = [I|A]
where I is the identity matrix and A is the parity generation matrix we can determine from Table C.2.

 1
 0
 0
 0
 1
 1
 1
 
 0
 1
 0
 0
 0
 1
 1 
G = 
 
0
 0
 1
 0
 1
 0
 1 
0
 0
 0
 1
 1
 1
 0
The columns of A are constructed from the bits used in the computation of P1 , P2 , and P4 . If we now represent each
input data item as a 4-dimensional vector d we can compute r = d · G and get a 7-dimensional vector r. This is the
data which in the case of ECC DDR is stored.
To decode the data we construct a new matrix H = [AT |I] where AT is the transposed parity generation matrix from
the computation of G. That means:

 1
 0
 1
 1
 1
 0
 0
 
H = 1
 1
 0
 1
 0
 1
 0 
1
 1
 1
 0
 0
 0
 1
The result of H · r shows whether the stored data is defective. If this is not the case, the product is the 3-dimensional
T
vector 0 0 0
 . Otherwise the value of the product, when interpreted as the binary representation of a number,
indicates the column number with the flipped bit.
As an example, assume d = 1 0 0 1 
. This results in
106
 Version 1.0
 What Every Programmer Should Know About Memory
r =
 1
 0
 0
 1
 0
 0
 1
 
Performing the test using the multiplication with H results in

 0
 
s =  0 
0
Now, assume we have a corruption of the stored data and read back from memory r 0 =
In this case we get
1
 0
 1 1
 0
 0
 1 
.

 1
 
0
s =  0 
1
The vector is not the null vector and, when interpreted as a number, s0 has the value 5. This is the number of the bit
we flipped in r0 (starting to count the bits from 1). The memory controller can correct the bit and the programs will not
notice that there has been a problem.
Handling the extra bit for the DED part is only slightly more complex. With more effort is it possible to create
codes which can correct two flipped bits and more. It is probability and risk which decide whether this is needed.
Some memory manufacturers say an error can occur in 256MB of RAM every 750 hours. By doubling the amount of
memory the time is reduced by 75%. With enough memory the probability of experiencing an error in a short time
can be significant and ECC RAM becomes a requirement. The time frame could even be so small that the SEC/DED
implementation is not sufficient.
Instead of implementing even more error correction capabilities, server motherboards have the ability to automatically
read all memory over a given timeframe. That means, whether or not the memory was actually requested by the
processor, the memory controller reads the data and, if the ECC check fails, writes the corrected data back to memory.
As long as the probablity of incurring less than two memory errors in the time frame needed to read all of memory and
write it back is acceptable, SEC/DED error correction is a perfectly reasonable solution.
As with registered DRAM, the question has to be asked: why is ECC DRAM not the norm? The answer to this question
is the same as for the equivalent question about registered RAM: the extra RAM chip increases the cost and the parity
computation increases the delay. Unregistered, non-ECC memory can be significantly faster. Because of the similarity
of the problems of registered and ECC DRAM, one usually only finds registered, ECC DRAM and not registered,
non-ECC DRAM.
There is another method to overcome memory errors. Some manufacturers offer what is often incorrectly called
“memory RAID” where the data is distributed redundantly over multiple DRAM modules, or at least RAM chips.
Motherboards with this feature can use unregistered DRAM modules, but the increased traffic on the memory busses
is likely to negate the difference in access times for ECC and non-ECC DRAM modules.
Ulrich Drepper
 Version 1.0
 107
D
 libNUMA Introduction
Although much of the information programmers need to schedule threads optimally, allocate memory appropriately,
etc. is available, this information is cumbersome to get at. The existing NUMA support library (libnuma, in the numactl
package on RHEL/Fedora systems) does not, by a long shot, provide adequate functionality.
As a response, the author has proposed a new library which provides all the functionality needed for NUMA. Due to the
overlap of memory and cache hierarchy handling, this library is also useful for non-NUMA systems with multi-thread
and multi-core processors–almost every currently-available machine.
The functionality of this new library is urgently needed to follow the advice given in this document. This is the only
reason why it is mentioned here. The library (as of this writing) is not finished, not reviewed, not polished, and not
(widely) distributed. It might change significantly in future. It is currently available at
http://people.redhat.com/drepper/libNUMA.tar.bz2
The interfaces of this library depend heavily on the information exported by the /sys filesystem. If this filesystem
is not mounted, many functions will simply fail or provide inaccurate information. This is particularly important to
remember if a process is executed in a chroot jail.
The interface header for the library contains currently the following definitions:
typedef memnode_set_t;
#define MEMNODE_ZERO_S(setsize, memnodesetp)
#define MEMNODE_SET_S(node, setsize, memnodesetp)
#define MEMNODE_CLR_S(node, setsize, memnodesetp)
#define MEMNODE_ISSET_S(node, setsize, memnodesetp)
#define MEMNODE_COUNT_S(setsize, memnodesetp)
#define MEMNODE_EQUAL_S(setsize, memnodesetp1, memnodesetp2)
#define MEMNODE_AND_S(setsize, destset, srcset1, srcset2)
#define MEMNODE_OR_S(setsize, destset, srcset1, srcset2)
#define MEMNODE_XOR_S(setsize, destset, srcset1, srcset2)
#define MEMNODE_ALLOC_SIZE(count)
#define MEMNODE_ALLOC(count)
#define MEMNODE_FREE(memnodeset)
int NUMA_cpu_system_count(void);
int NUMA_cpu_system_mask(size_t destsize, cpu_set_t *dest);
int NUMA_cpu_self_count(void);
int NUMA_cpu_self_mask(size_t destsize, cpu_set_t *dest);
int NUMA_cpu_self_current_idx(void);
int NUMA_cpu_self_current_mask(size_t destsize, cpu_set_t *dest);
ssize_t NUMA_cpu_level_mask(size_t destsize, cpu_set_t *dest,
size_t srcsize, const cpu_set_t *src,
unsigned int level);
int NUMA_memnode_system_count(void);
int NUMA_memnode_system_mask(size_t destsize, memnode_set_t *dest);
int NUMA_memnode_self_mask(size_t destsize, memnode_set_t *dest);
int NUMA_memnode_self_current_idx(void);
int NUMA_memnode_self_current_mask(size_t destsize, memnode_set_t *dest);
int NUMA_cpu_to_memnode(size_t cpusetsize, const cpu_set_t *cpuset,
size_t __memnodesize, memnode_set_t *memnodeset);
int NUMA_memnode_to_cpu(size_t memnodesize, const memnode_set_t *memnodeset,
108
 Version 1.0
 What Every Programmer Should Know About Memory
size_t cpusetsize, cpu_set_t *cpuset);
int NUMA_mem_get_node_idx(void *addr);
int NUMA_mem_get_node_mask(void *addr, size_t size,
size_t destsize, memnode_set_t *dest);
The MEMNODE_* macros are similar in form and functionality to the CPU_* macros introduced in section section 6.4.3.
There are no non-_S variants of the macros, they all require a size parameter. The memnode_set_t type is the
equivalent of cpu_set_t, but this time for memory nodes. Note that the number of memory nodes need not have
anything to do with the number of CPUs and vice versa. It is possible to have many CPUs per memory node or even
no CPU at all. The size of dynamically allocated memory node bit sets should, therefore, not be determined by the
number of CPUs.
Instead, the NUMA_memnode_system_count interface should be used. It returns the number of nodes currently reg-
istered. This number might grow or shrink over time. More often than not, though, it will remain constant, and is
therefore a good value to use for sizing memory node bit sets. The allocation, again similar to the CPU_ macros,
happens using MEMNODE_ALLOC_SIZE, MEMNODE_ALLOC and MEMNODE_FREE.
As a last parallel with the CPU_* macros, the library also provides macros to compare memory node bit sets for equality
and to perform logical operations.
The NUMA_cpu_* functions provide functionality to handle CPU sets. In part, the interfaces only make existing func-
tionality available under a new name. NUMA_cpu_system_count returns the number of CPUs in the system, the
NUMA_CPU_system_mask variant returns a bit mask with the appropriate bits set–functionality which is not otherwise
available.
NUMA_cpu_self_count and NUMA_cpu_self_mask return information about the CPUs the current thread is cur-
rently allowed to run on. NUMA_cpu_self_current_idx returns the index of the currently used CPU. This informa-
tion might already be stale when returned, due to scheduling decisions the kernel can make; it always has to be assumed
to be inaccurate. The NUMA_cpu_self_current_mask returns the same information and sets the appropriate bit in
the bit set.
NUMA_memnode_system_count has already been introduced. NUMA_memnode_system_mask is the equivalent func-
tion which fills in a bit set. NUMA_memnode_self_mask fills in a bit set according to the memory nodes which are
directly attached to any of the CPUs the thread can currently run on.
Even more specialized information is returned by the NUMA_memnode_self_current_idx and NUMA_memnode_-
self_current_mask functions. The information returned is the memory node which is connected to the processor
the thread is currently running on. Just as for the NUMA_cpu_self_current_* functions, this information can already
be stale when the function returns; it can only be used as a hint.
The NUMA_cpu_to_memnode function can be used to map a set of CPUs to the set of directly-attached memory nodes.
If only a single bit is set in the CPU set, one can determine which memory node each CPU belongs to. Currently, there
is no support in Linux for a single CPU belonging to more than one memory node; this could, theoretically, change in
future. To map in the other direction the NUMA_memnode_to_cpu function can be used.
If memory is already allocated, it is sometimes useful to know where it is allocated. This is what the NUMA_mem_-
get_node_idx and NUMA_mem_get_node_mask allow the programmer to determine. The former function returns
the index of the memory node on which the page corresponding to the address specified by the parameter is allocated–
or will be allocated according to the currently installed policy if the page is not yet allocated. The second function can
perform the work for a whole address range; it returns the information in the form of a bit set. The function’s return
value is the number of different memory nodes which are used.
In the remainder of this section we will see a few example for use cases of these interfaces. In all cases we skip the
error handling and the case where the number of CPUs and/or memory nodes is too large for the cpu_set_t and
memnode_set_t types respectively. Making the code robust is left as an exercise to the reader.
Determine Thread Sibling of Given CPU
To schedule helper threads, or other threads which benefit from being scheduled on a thread of a given CPU, a code
sequence like the following can be used.
Ulrich Drepper
 Version 1.0
 109
cpu_set_t cur;
CPU_ZERO(&cur);
CPU_SET(cpunr, &cur);
cpu_set_t hyperths;
NUMA_cpu_level_mask(sizeof(hyperths), &hyperths, sizeof(cur), &cur, 1);
CPU_CLR(cpunr, &hyperths);
The code first generates a bit set for the CPU specified by cpunr. This bit set is then passed to NUMA_cpu_level_-
mask along with the fifth parameter specifying that we are looking for hyper-threads. The result is returned in the
hyperths bit set. All that remains to be done is to clear the bit corresponding to the original CPU.
Determine Core Siblings of Given CPU
If two threads should not be scheduled on two hyper-threads, but can benefit from cache sharing, we need to determine
the other cores of the processor. The following code sequence does the trick.
cpu_set_t cur;
CPU_ZERO(&cur);
CPU_SET(cpunr, &cur);
cpu_set_t hyperths;
int nhts = NUMA_cpu_level_mask(sizeof(hyperths), &hyperths, sizeof(cur), &cur, 1);
cpu_set_t coreths;
int ncs = NUMA_cpu_level_mask(sizeof(coreths), &coreths, sizeof(cur), &cur, 2);
CPU_XOR(&coreths, &coreths, &hyperths);
ncs -= nhts;
The first part of the code is identical to the code to determine hyper-threads. This is no coincidence since we have to
distinguish the hyper-threads of the given CPU from the other cores. This is implemented in the second part which
calls NUMA_cpu_level_mask again, but, this time, with a level of 2. All that remains to be done is to remove all
hyper-threads of the given CPU from the result. The variables nhts and ncs are used to keep track of the number of
bits set in the respective bit sets.
The resulting mask can be used to schedule another thread. If no other thread has to be explicitly scheduled, the
decision about the core to use can be left to the OS. Otherwise one can iteratively run the following code:
while (ncs > 0) {
size_t idx = 0;
while (! CPU_ISSET(idx, &ncs))
++idx;
CPU_ZERO(&cur);
CPU_SET(idx, &cur);
nhts = NUMA_cpu_level_mask(sizeof(hyperths), &hyperths, sizeof(cur), &cur, 1);
CPU_XOR(&coreths, &coreths, hyperths);
ncs -= nhts;
... schedule thread on CPU idx ...
}
The loop picks, in each iteration, a CPU number from the remaining, used cores. It then computes all the hyper-threads
for the this CPU. The resulting bit set is then subtracted (using CPU_XOR) from the bit set of the available cores. If the
XOR operation does not remove anything, something is really wrong. The ncs variable is updated and we are ready
for the next round, but not before the scheduling decisions are made. At the end, any of idx, cur, or hyperths can
be used to schedule a thread, depending on the requirements of the program. Often it is best to leave the OS as much
freedom as possible and, therefore, to use the hyperths bit set so that the OS can select the best hyper-thread.
110
 Version 1.0
 What Every Programmer Should Know About Memory
E
 Index
ABI, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
Access Pattern, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
Activation, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
Address Demultiplexing, . . . . . . . . . . . . . . . . . . . . . . . . . 6
Address Snooping, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
Address Space Layout Randomization, . . . . . . . . . . . 59
Alias, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
Aliasing, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
Alpha Decay, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
ALU, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Application Binary Interface, . . . . . . . . . . . . . . . . . . . . 54
Arithmetic Logic Unit, . . . . . . . . . . . . . . . . . . . . . . . . . . 29
ASID, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
ASLR, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
Asynchronous DRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
Bottleneck, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
Branch Prediction, . . . . . . . . . . . . . . . . . . . . . . . . . . 14, 56
Burst Speed, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
Cache, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1, 13
Access Cost, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
Associativity, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
Coherency, . . . . . . . . . . . . . . . . . . . . . . . . . . . 16, 25f.
Pollution, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
Replacement Strategy, . . . . . . . . . . . . . . . . . . . . . . 14
Tag, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
Capacitor, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
Charge Curve, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
Capacity Cache Miss, . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
CAS, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68, 89
CAS, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7ff.
Latency, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
Cell CPU, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
CL, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
Co-processors, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
Column Address Selection, . . . . . . . . . . . . . . . . . . . . . . . 7
Command Rate, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
Commodity Hardware, . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
Common System Interface, . . . . . . . . . . . . . . . . . . . . . . . 4
Compare-And-Swap, . . . . . . . . . . . . . . . . . . . . . . . . 68, 89
Compulsory Cache Miss, . . . . . . . . . . . . . . . . . . . . . . . . 34
Conflict Misses, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
Cosmic Rays, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
CPU Pipeline,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .14, 30
Critical Word, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
CSI, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
DCAS, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
dcbz, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
DDR, DDR1, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
DDR2, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8, 10
DDR3, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
Demultiplexer,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
Direct Cache Access, . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
Direct-Mapped Cache, . . . . . . . . . . . . . . . . . . . . . . . . . . 18
Dirty Flag, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
Ulrich Drepper
 VersionDMA, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3, 12
Double Error Detection, . . . . . . . . . . . . . . . . . . . . . . . . 106
Double-Pumped, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
DRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3, 5
Dynamic RAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
ECC DRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
Error Correction Code, . . . . . . . . . . . . . . . . . . . . . . . . . 105
Exclusive Cache, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
False Sharing, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
FB-DRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
Fortran, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
Front Side Bus, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
FSB, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3, 8f., 11, 14, 70
Fully Associative Cache, . . . . . . . . . . . . . . . . . . . . . . . . 17
Fully Duplex Bus, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
gcov, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
Geneseo, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
getconf, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
Hamming Code, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
Hardware Prefetching, . . . . . . . . . . . . . . . . . . . . . . . . . . 60
Helper Thread, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
HUGE_TLB, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
hugetlbfs, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
Hyper Transport, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
Hyper-Thread, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29, 63
Hypervisor, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31, 39
Inclusive Cache, . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16, 78
Latency, . . . . . . . . . . . . . . . . . . . . . . . . . . . 8, 23, 30, 60, 95
Leakage, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
LL/SC, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68, 89
Load Lock/Store Conditional, . . . . . . . . . . . . . . . . 68, 89
Loop Stream Detector, . . . . . . . . . . . . . . . . . . . . . . . . . . 57
LSD, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
MAP_FIXED, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
Mass Storage, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
Memory Channel, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
Memory Controller, . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1, 4
Memory Management Unit, . . . . . . . . . . . . . . . . . . . . . 30
Memory Model, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
Memory Ordering, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Memory RAID, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
MESI, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
MMU, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
MPI, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
Multi-core, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
Multiplexer, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
Northbridge, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3, 70
NUMA, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
NUMA factor, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4, 43
OOO, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
1.0
 111
opcontrol,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .102
opreport,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .103
OProfile,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .102
Out-Of-Order, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
Pacifica, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
Page Table Walk, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
Parallel Port, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
PATA, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
PCI Express, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
PCI-E, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
PGO, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
Physical Address, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
Pre-Faulting, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
Precharge, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
Prefetch Trigger, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
Prefetching, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14, 20, 60
Profile Guide Optimization, . . . . . . . . . . . . . . . . . . . . . . 85
Quad-Pumped, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
RAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
Rambus, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3, 8
RAS, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7ff.
RAS-to-CAS Delay,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
Recharging, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
Refresh, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
Registered DRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
Request For Ownership, . . . . . . . . . . . . . . . . . . . . . . . . . 26
RFO, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26, 65
Row Address Selection, . . . . . . . . . . . . . . . . . . . . . . . . . . 7
SATA, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
_SC_LEVEL1_DCACHE_LINESIZE, . . . . . . . . . . . . . . 50
SDRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3, 8, 10
Self Modifying Code, . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
Sense Amplifier, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
Serial Port, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
Set Associative, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
SIMD, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51, 95
Single Error Correction, . . . . . . . . . . . . . . . . . . . . . . . . 105
SMC, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
SMP, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
SMT, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Software Prefetching, . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
Southbridge, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
Spatial Locality, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
Speculation, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
Speculative Load, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
SPU, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
SRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
Static RAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
Stream, Prefetch, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
Streaming Read Buffer, . . . . . . . . . . . . . . . . . . . . . . . . . 48
Strides, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60, 96
Symmetric Multi-Thread, . . . . . . . . . . . . . . . . . . . . . . . . 29
Tag, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
Temporal Locality, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
TLB, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22, 39
112
 Version 1.0
Torrenza, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
Trace Cache, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
Transactional Memory, . . . . . . . . . . . . . . . . . . . . . . . . . . 91
Transistor, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
Translation Look-Aside Buffer, . . . . . . . . . . . . . . . . . . 39
tRAS , . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
tRCD, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
tRP , . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
Unregistered DRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . 105
USB, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3, 12
Vector Operation, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
Virtual Address, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
Virtual Machine Monitor, . . . . . . . . . . . . . . . . . . . . . . . . 31
VMM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31, 39
von Neumann Architecture, . . . . . . . . . . . . . . . . . . . . . . 14
WE, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
Write Enable, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
Write-Combining, . . . . . . . . . . . . . . . . . . . . . . . . . . . 25, 48
What Every Programmer Should Know About Memory
F
 Bibliography
[1] Performance Guidelines for AMD AthlonTM 64 and AMD OpteronTM ccNUMA Multiprocessor Systems.
Advanced Micro Devices, June 2006. URL
http://www.amd.com/us-en/assets/content type/white papers and tech docs/40555.pdf. 5.4
[2] Jennifer M. Anderson, Lance M. Berc, Jeffrey Dean, Sanjay Ghemawat, Monika R. Henzinger, Shun-Tak A.
Leung, Richard L. Sites, Mark T. Vandevoorde, Carl A. Waldspurger, and William E. Weihl. Continuous
profiling: Where have all the cycles gone. In Proceedings of the 16th ACM Symposium of Operating Systems
Principles, pages 1–14, October 1997. URL http://citeseer.ist.psu.edu/anderson97continuous.html. 7.1
[3] Vinodh Cuppu, Bruce Jacob, Brian Davis, and Trevor Mudge. High-Performance DRAMs in Workstation
Environments. IEEE Transactions on Computers, 50(11):1133–1153, November 2001. URL
http://citeseer.ist.psu.edu/476689.html. 2.1.2, 2.2, 2.2.1, 2.2.3, 10
[4] Arnaldo Carvalho de Melo. The 7 dwarves: debugging information beyond gdb. In Proceedings of the Linux
Symposium, 2007. URL https://ols2006.108.redhat.com/2007/Reprints/melo-Reprint.pdf. 6.2.1
[5] Simon Doherty, David L. Detlefs, Lindsay Grove, Christine H. Flood, Victor Luchangco, Paul A. Martin, Mark
Moir, Nir Shavit, and Jr. Guy L. Steele. DCAS is not a Silver Bullet for Nonblocking Algorithm Design. In
SPAA ’04: Proceedings of the Sixteenth Annual ACM Symposium on Parallelism in Algorithms and
Architectures, pages 216–224, New York, NY, USA, 2004. ACM Press. ISBN 1-58113-840-7. URL
http://research.sun.com/scalable/pubs/SPAA04.pdf. 8.1
[6] M. Dowler. Introduction to DDR-2: The DDR Memory Replacement.
http://www.pcstats.com/articleview.cfm?articleID=1573, May 2004. 2.2.1
[7] Ulrich Drepper. Futexes Are Tricky, December 2005. URL http://people.redhat.com/drepper/futex.pdf. 6.3.4
[8] Ulrich Drepper. ELF Handling For Thread-Local Storage. Technical report, Red Hat, Inc., 2003. URL
http://people.redhat.com/drepper/tls.pdf. 6.4.1
[9] Ulrich Drepper. Security Enhancements in Red Hat Enterprise Linux, 2004. URL
http://people.redhat.com/drepper/nonselsec.pdf. 4.2
[10] Dominique Fober, Yann Orlarey, and Stephane Letz. Lock-Free Techiniques for Concurrent Access to Shared
Objects. In GMEM, editor, Actes des Journes d’Informatique Musicale JIM2002, Marseille, pages 143–150,
2002. URL http://www.grame.fr/pub/fober-JIM2002.pdf. 8.1, 8.1
[11] Joe Gebis and David Patterson. Embracing and Extending 20th -Century Instruction Set Architectures.
Computer, 40(4):68–75, April 2007. 8.4
[12] David Goldberg. What Every Computer Scientist Should Know About Floating-Point Arithmetic. ACM
Computing Surveys, 23(1):5–48, March 1991. URL http://citeseer.ist.psu.edu/goldberg91what.html. 1
[13] Maurice Herlihy and J. Eliot B. Moss. Transactional memory: Architectural support for lock-free data
structures. In Proceedings of 20th International Symposium on Computer Architecture, 1993. URL
http://citeseer.ist.psu.edu/herlihy93transactional.html. 8.2, 8.2.2, 8.2.3, 8.2.4
[14] Ram Huggahalli, Ravi Iyer, and Scott Tetrick. Direct Cache Access for High Bandwidth Network I/O, 2005.
URL http://www.stanford.edu/group/comparch/papers/huggahalli05.pdf. 6.3.5
[15] Intel R 64 and IA-32 Architectures Optimization Reference Manual. Intel Corporation, May 2007. URL
http://www.intel.com/design/processor/manuals/248966.pdf. B.3
[16] William Margo, Paul Petersen, and Sanjiv Shah. Hyper-Threading Technology: Impact on Compute-Intensive
Workloads. Intel Technology Journal, 6(1), 2002. URL
ftp://download.intel.com/technology/itj/2002/volume06issue01/art06 computeintensive/vol6iss1 art06. 3.3.4
[17] Caolán McNamara. Controlling symbol ordering.
http://blogs.linux.ie/caolan/2007/04/24/controlling-symbol-ordering/, April 2007. 7.5
[18] Double Data Rate (DDR) SDRAM MT46V. Micron Technology, 2003. Rev. L 6/06 EN. 2.2.2, 10
[19] Jon “Hannibal” Stokes. Ars Technica RAM Guide, Part II: Asynchronous and Synchronous DRAM.
http://arstechnica.com/paedia/r/ram guide/ram guide.part2-1.html, 2004. 2.2
[20] Wikipedia. Static random access memory. http://en.wikipedia.org/wiki/Static Random Access Memory, 2006.
2.1.1
Ulrich Drepper
 Version 1.0
 113
G
 Revision History
2007-6-24 First internal draft.
2007-8-11 Add first set of Johnray’s edits.
2007-9 to 2007-11 Lots of language cleanup from Jonathan Corbet and team.
2007-11-21 Version 1.0.
Source File Identification
Last Modification
Source File Size
Source File MD5
MPost Last Modification
MPost Source File Size
MPost Source File MD5
D:20071121175205-08’00’
553180 Bytes
0299BAC7C5B6A501077DEB5714C944DF
D:20071104104838-08’00’
138021 Bytes
7732297065D660A939A9D651A7992FA2
114
 Version 1.0
 What Every Programmer Should Know About Memory

Code Kitchen
exception handling
But why don’t
you just use
Ready-bake code?
There is NO way
I’m letting Betty win the
code-off this year, so I’m
gonna make it myself from
scratch.
You don’t have to do it
yourself, but it’s a lot
more fun if you do.
The rest of this chapter
is optional; you can use
Ready-bake code for all
the music apps.
But if you want to learn
more about JavaSound,
turn the page.
you are here�
 339
JavaSound MIDI classes
Making actual sound
Remember near the beginning of the chapter, we looked at how MIDI data holds
the instructions for what should be played (and how it should be played) and we
also said that MIDI data doesn’t actually create any sound that you hear. For sound
to come out of the speakers, the MIDI data has to be sent through some kind of
MIDI device that takes the MIDI instructions and renders them in sound, either
by triggering a hardware instrument or a ‘virtual’ instrument (software synthe-
sizer). In this book, we’re using only software devices, so here’s how it works in
JavaSound:
You need FOUR things:
1
The thing that
plays the music
Sequencer
plays
The Sequencer is the thing
that actually causes a song
to be played. Think of it like
a music CD player.
2
 The music to be
 3
 The part of the
 4
 The actual music
played...a song.
 Sequence that
 information:
holds the actual
 notes to play,
information
 how long, etc.
Midi
has a
 holds
 Event
Sequence
 Track
The Sequence is the
song, the musical piece
that the Sequencer will
play. For this book, think
of the Sequence as a
music CD, but the whole
CD plays just one song.
For this book, we only
need one Track, so just
imagine a music CD
with only one song. A
single Track. This Track
is where all the song
data (MIDI information)
lives.
For
 single-so informat lives of t
h
 the
 is
 on b
 ng t io
 o
 S
 h
 ok, n e
 CD e quence.
 about Track, think (h
as
 how of
 only and t
 to h
 the
 one e
 play S
e
 Track). Track quence the
 is son
 as Th
 part
 g
 a
 e
Midi
Event
Midi
Event
Midi
Event
A MIDI event is a message
that the Sequencer can
understand. A MIDI event
might say (if it spoke
English), “At this moment
in time, play middle C, play
it this fast and this hard,
and hold it for this long. “
A MIDI event might
also say something like,
“Change the current
instrument to Flute.”
340 chapter 11
And you need FIVE steps:
1
 Get a Sequencer and open it
Sequencer player = MidiSystem.getSequencer();
player.open();
2
Make a new Sequence
Sequence seq = new Sequence(timing,4);
3
Get a new Track from the Sequence
Track t = seq.createTrack();
4
Fill the Track with MidiEvents and
give the Sequence to the Sequencer
t.add(myMidiEvent1);
player.setSequence(seq);
Uh, hate to break it
to you, but that’s only
FOUR steps.
exception handling
Ahhhh. We
forgot to push the
PLAY button. You have to
start() the Sequencer!
player.start();
you are here�
 341
a sound application
Your very first sound player app
Type it in and run it. You’ll hear the sound of someone playing a
single note on a piano! (OK, maybe not someone, but something.)
import javax.sound.midi.*;
 Don’t forget to im
port the midi pack
age
public class MiniMiniMusicApp {
public static void main(String[] args) {
MiniMiniMusicApp mini = new MiniMiniMusicApp();
mini.play();
} // close main
public try {
void play() {
 get (so a we Se c
 q
 a
 c
 u
 n ome e
n
 use c
 e
r
 already it... and a open Seq op
 u
 e
 it
 encer
 n
)
doesn’t
1
 Sequencer player = MidiSystem.getSequencer();
player.open();
2
 Sequence seq = new Sequence(Sequence.PPQ, 4);
 Don’t Sequence of ‘em worry as co Re
 n
ab
 st
 ad
 ructor. o
 y-bake u
t
 t
h
e
 Just arguments a
r
g
uments copy these
 ).
 to th (th
 e
 ink
3
 Track track = seq.createTrack();
lives Track Ask the in lives the Sequence Track.
 in the Sequen for a
 Tr
 ce, ack. andRemember, the MIDI the
 data
ShortMessage a = new ShortMessage();
4
 a.setMessage(144, 1, 44, 100);
MidiEvent track.add(noteOn);
 noteOn = new MidiEvent(a, 1);
 is Put some Ready-bake MidiEvents into code. the The Track.
 only thi
ng This part
 you’ll
mostlyhave to care about are the arguments
 to the
b.setMessage(128, MidiEvent ShortMessage noteOff b = new = 1, new ShortMessage();
 44,MidiEvent(b, 100);
 16);
 the setMessage() MidiEvent method, constructor. and the We’ll argume
nts look
 at those
 to
track.add(noteOff);
 arguments on the next page.
player.setSequence(seq);
 Give the Sequence to the Sequencer (like
putting the CD in the CD player)
player.start();
} catch ex.printStackTrace();
 (Exception ex) {
 Start() the
 Sequencer (l
ike pushing
 PLAY)
}
} // close play
} // close class
342 chapter 11
exception handling
Making a MidiEvent (song data)
A MidiEvent is an instruction for part of a song. A series of MidiEvents is
kind of like sheet music, or a player piano roll. Most of the MidiEvents we
care about describe a thing to do and the moment in time to do it. The moment
in time part matters, since timing is everything in music. This note follows
this note and so on. And because MidiEvents are so detailed, you have to say
at what moment to start playing the note (a NOTE ON event) and at what
moment to stop playing the notes (NOTE OFF event). So you can imagine
that firing the “stop playing note G” (NOTE OFF message) before the “start
playing Note G” (NOTE ON) message wouldn’t work.
The MIDI instruction actually goes into a Message object; the MidiEvent is
a combination of the Message plus the moment in time when that message
should ‘fire’. In other words, the Message might say, “Start playing Middle
C” while the MidiEvent would say, “Trigger this message at beat 4”.
So we always need a Message and a MidiEvent.
The Message says what to do, and the MidiEvent says when to do it.
1
Make a Message
ShortMessage a = new ShortMessage();
A MidiEvent says
what to do and
when to do it.
Every instruction
must include the
timing for that
instruction.
In other words, at
which beat that
thing should happen.
2
3
4
a.setMessage(144, Put the Instruction 1, in 44, the 100);
 Message
 This (we’ll messag look
 a
 e t
 says, the other “st
a
r
t
 numbers p
la
y
in
g on
 note the
 44”
Make a new MidiEvent using the Message
 next page)
track.add(noteOn);
 MidiEvent Add the MidiEvent noteOn to = new the Track
 MidiEvent(a, 1);
 message should Event The instructions be adds ‘a’ triggered. at the the moment are firs
 T
 in
 t his th
 beat in MidiEvent etime message, (beat when 1).
 says but the the to
 instr
 trigger
 Midi-
 uction
A
 them the events yo
 instruments Tr
 u
 Sequencer mi
 ac
 according k
 gh
 happening ho
 t
 lds
 wa
 playing nt
 all
 plays to tw
 at th
 when o
 e
 differen
 the them no
 M
 te
 idi
 ea ex
 s
 Event ba ch act
 played ck
 t event sounds same in objects. that simultaneously, is moment at supposed order. the The in same Sequence You to time. or happ
 time.
 can eve
 For en,
 have n organizes
 different
 example,
 and lots then
 of
you are here�
 343
contents of a Midi event
MIDI message: the heart of a MidiEvent
A MIDI message holds the part of the event that says what to do. The actual instruction
you want the sequencer to execute. The first argument of an instruction is always the type
of the message.The values you pass to the other three arguments depend on the type of
message. For example, a message of type 144 means “NOTE ON”. But in order to carry
out a NOTE ON, the sequencer needs to know a few things. Imagine the sequencer saying,
“OK, I’ll play a note, but which channel? In other words, do you want me to play a Drum
note or a Piano note? And which note? Middle-C? D Sharp? And while we’re at it, at which
velocity should I play the note?
To make a MIDI message, make a ShortMessage instance and invoke setMessage(), passing
in the four arguments for the message. But remember, the message says only what to do, so
you still need to stuff the message into an event that adds when that message should ‘fire’.
Anatomy of a message
The Message says what to do, the
The first argument to setMessage() always
MidiEvent says when to do it.
represents the message ‘type’, while the other
three arguments represent different things
depending on the message type.
mes
sag
e
 typ c
hann e
 el
 n
ote t
 o
 play v
 y
 2 Think Channel
 of a channel like a musician in
elocit
a.setMessage(144,
 1,
 44,
 100);
 a band. Channel 1 is musician 1 (the
to other type. The know last This args in 3 or is are a
 der rgs a for NOTE vary to things play depending ON a
 the message, note.
 Sequence on so the
 the
 r message
 needs
 3 Note drummer, keyboard to player), play
 etc.
 channel 9 is the
A number from 0 to 127, going
1
 Message type
from low to high notes.
144 means
NOTE ON
 1
 127
p
lay
ing
 0 1
 2
 3 4 5
 6
 7 8
sta
rt128 means
NOTE OFF
 4 Velocity
How fast and hard
 did
sto
p
 p
layin
g
 good probably you press default.
 won’t the hear key? anything, 0 is so soft but 100 you
 is a
344 chapter 11
Change a message
Now that you know what’s in a Midi message, you can start experimenting. You
can change the note that’s played, how long the note is held, add more notes,
and even change the instrument.
1Change the note
Try a number between 0 and 127 in the note
on and note off messages.
a.setMessage(144, 1, 20, 100);
0 1
 2 3
 4 5
 6
 7 8
127
2 Change the duration of the note
Change the note off event (not the message) so
that it happens at an earlier or later beat.
b.setMessage(128, 1, 44, 100);
MidiEvent noteOff = new MidiEvent(b, 3 );
exception handling
3Change the instrument
Add a new message, BEFORE the note-playing message,
that sets the instrument in channel 1 to something other
than the default piano. The change-instrument message
is ‘192’, and the third argument represents the actual
instrument (try a number between 0 and 127)
first.setMessage(192, 1, 102, 0);
m
e
s
sage si
cian 1) m
ent 10
2
c
han
g
e
-
instr
 in
 u
m
ent l 1 (mu t
o
 instru
channeyou are here�
 345
change the instrument and note
Version 2: Using command-line args to experiment with sounds
This version still plays just a single note, but you get to use command-line argu-
ments to change the instrument and note. Experiment by passing in two int values
from 0 to 127. The first int sets the instrument, the second int sets the note to play.
import javax.sound.midi.*;
public class MiniMusicCmdLine {
 // this is the first one
public static void main(String[] args) {
MiniMusicCmdLine mini = new MiniMusicCmdLine();
if (args.length < 2) {
System.out.println(“Don’t forget the instrument and note args”);
} else {
int instrument = Integer.parseInt(args[0]);
int note = Integer.parseInt(args[1]);
mini.play(instrument, note);
}
} // close main
public void play(int instrument, int note) {
try {
Sequencer player = MidiSystem.getSequencer();
player.open();
Sequence seq = new Sequence(Sequence.PPQ, 4);
Track track = seq.createTrack();
MidiEvent event = null;
ShortMessage first = new ShortMessage();
first.setMessage(192, 1, instrument, 0);
MidiEvent changeInstrument = new MidiEvent(first, 1);
track.add(changeInstrument);
ShortMessage a = new ShortMessage();
 Run it with two int args from 0
a.setMessage(144, 1, note, 100);
 to 127. Try these for starters:
MidiEvent noteOn = new MidiEvent(a, 1);
track.add(noteOn);
 File Edit Window Help Attenuate
%java MiniMusicCmdLine 102 30
ShortMessage b = new ShortMessage();
b.setMessage(128, 1, note, 100);
 %java MiniMusicCmdLine 80 20
MidiEvent noteOff = new MidiEvent(b, 16);
track.add(noteOff);
 %java MiniMusicCmdLine 40 70
player.setSequence(seq);
player.start();
} catch (Exception ex) {ex.printStackTrace();}
} // close play
} // close class
346 chapter 11
Where we’re headed with the rest
of the CodeKitchens
Chapter 15: the goal
When we’re done, we’ll have a working
BeatBox that’s also a Drum Chat Client.
We’ll need to learn about GUIs (includ-
ing event handling), I/O, networking, and
threads. The next three chapters (12, 13,
and 14) will get us there.
Chapter 12: MIDI events
This CodeKitchen lets us build a little
“music video” (bit of a stretch to call it
that...) that draws random rectangles to
the beat of the MIDI music. We’ll learn
how to construct and play a lot of MIDI
events (instead of just a couple, as we do
in the current chapter).
Chapter 13: Stand-alone
BeatBox
Now we’ll actually build the real BeatBox,
GUI and all. But it’s limited—as soon as you
change a pattern, the previous one is lost.
There’s no Save and Restore feature, and
it doesn’t communicate with the network.
(But you can still use it to work on your
drum pattern skills.)
beat one
Chapter 14: Save and
Restore
You’ve made the perfect pattern, and
now you can save it to a file, and reload it
when you want to play it again. This gets
us ready for the final version (chapter 15),
where instead of writing the pattern to a
file, we send it over a network to the chat
server.
exception handling
dance beat
Andy: groove #2
Chris: groove2 revised
Nigel: dance beat
beat two
 beat three
 beat four ...
you are here�
 347
exercise: True or False
This chapter explored the wonderful world of
exceptions. Your job is to decide whether each of the
Exercise
 following exception-related statements is true or false.
CTrue or FalseD
1. A try block must be followed by a catch and a finally block.
2. If you write a method that might cause a compiler-checked exception, you
must wrap that risky code in a try / catch block.
3. Catch blocks can be polymorphic.
4. Only ‘compiler checked’ exceptions can be caught.
5. If you define a try / catch block, a matching finally block is optional.
6. If you define a try block, you can pair it with a matching catch or finally block,
or both.
7. If you write a method that declares that it can throw a compiler-checked ex-
ception, you must also wrap the exception throwing code in a try / catch block.
8. The main( ) method in your program must handle all unhandled exceptions
thrown to it.
9. A single try block can have many different catch blocks.
10. A method can only throw one kind of exception.
11. A finally block will run regardless of whether an exception is thrown.
12. A finally block can exist without a try block.
13. A try block can exist by itself, without a catch block or a finally block.
14. Handling an exception is sometimes referred to as ‘ducking’.
15. The order of catch blocks never matters.
16. A method with a try block and a finally block, can optionally declare the
exception.
17. Runtime exceptions must be handled or declared.
348 chapter 11
Exercise
File Edit Window Help ThrowUp
% java ExTestDrive yes
thaws
% java ExTestDrive no
throws
exception handling
Code Magnets
A working Java program is scrambled up on the fridge. Can you
reconstruct all the code snippets to make a working Java program
that produces the output listed below? Some of the curly braces fell
on the floor and they were too small to pick up, so feel free to add as
many of those as you need!
System.out.print(“
r“);
 try {
System.out.print
(“t“);
doRisky(test);
} finally {
System.out.println(“s“);
System.out.print(“o“);
class MyEx extends Exception { }
public class ExTestDrive {
System.out.pri
nt(“w“);
if (“yes
”.equals
(t)) {
System.out.print(“a“);
throw new MyEx();
 } catch (MyEx e)
 {
static void doRisky(String t) throws MyEx {
System.out.print(“h”);
public static void
 main(String [] ar
gs)
 {
String test = args
[0];
you are here�
 349
puzzle: crossword
JavaCross 7.0
1
 2
 3
 4
5
6
 7
 8
9
10
 11
 12
 13
14
 15
 16
17
 18
 19
20
21
 22
 23
24
 25
26
27
 28
29
You know what to do!
Across
1. To give value
4. Flew off the top
6. All this and more!
8. Start
10. The family tree
13. No ducking
15. Problem objects
18. One of Java’s ‘49’
20. Class hierarchy
21. Too hot to handle
24. Common primitive
25. Code recipe
27. Unruly method action
28. No Picasso here
29. Start a chain of events
Down
2. Currently usable
3. Template’s creation
4. Don’t show the kids
5. Mostly static API class
7. Not about behavior
9. The template
11. Roll another one off
the line
12. Javac saw it coming
14. Attempt risk
16. Automatic acquisition
17. Changing method
19. Announce a duck
22. Deal with it
23. Create bad news
26. One of my roles
More Hints:
’
retteg‘ a toN .71e
nutrof ylimaf eht _____ .61t
luafed ro cilbup ylnO .9.
 . . srebmuN .5)
elpmaxe ton( ______ roF .3h
sawhtuom a rO .2
	 nwoDt
cartsbA toN .82m
elborp a stratS .72k
cauQ .12n
oitcelloc fo epyt a oslA .02e
ralced fo daetsnI .31d
ohtem a tratS .8d
lihc avaJ A .6
	ssorcA350 chapter 11
Exercise Solutions
True or False
1. False, either or both.
2. False, you can declare the exception.
3. True.
4. False, runtime exception can be caught.
5. True.
6. True, both are acceptable.
7. False, the declaration is sufficient.
8. False, but if it doesn’t the JVM may shut down.
9. True.
10. False.
11. True. It’s often used to clean-up partially
completed tasks.
12. False.
13. False.
14. False, ducking is synonymous with declaring.
15. False, broadest exceptions must be caught by
the last catch blocks.
16. False, if you don’t have a catch block, you must
declare.
17. False.
exception handling
Code Magnets
class MyEx extends Exception { }
public class ExTestDrive {
public static void main(String [] args) {
String test = args[0];
try {
System.out.print(“t”);
doRisky(test);
System.out.print(“o”);
} catch ( MyEx e) {
System.out.print(“a”);
} finally {
System.out.print(“w”);
}
System.out.println(“s”);
}
static void doRisky(String t) throws MyEx {
System.out.print(“h”);
if (“yes”.equals(t)) {
throw new MyEx();
}
System.out.print(“r”);
}
}
 File Edit Window Help Chill
% java ExTestDrive yes
thaws
% java ExTestDrive no
throws
you are here�
 351
puzzle answers
JavaCross Answers
A 1A S 2 S
 S I S G 3 I N M
 G E N N M T E N T
 4
 P O P P E D
5
M
 C
 N
 R
A
 O
 6
 S U B C L A 7
 S S
 8
 I N V O K E
T
 P
 T
 T
 V
 9
 C
10
 H 11
 I E R A R 12
 C H Y
 A
 13
 H A N D L E
 L
N
 N
 H
 T
 T
 A
S
 C
 E
 14
 T
 15
E X C E P T 16
 I O N S
T
 E
 C
 R
 N
 S
A
 17
 S
 18
 K E Y W O R 19
 D
 H
N
 E
 E
 E
 20
 T R E E
T
 T
 21
 D U 22
 C K
 C
 R
 23
 T
24
 I N T
 A
 25
 A L G O R I T H M
A
 E
 26
 I
 T
 A
 T
 R
27
 T H R O W S
 28
 C O N C R E T E
 O
E
 A
 H
 E
 29
 N E W
352 chapter 11
12 getting gui
A VeryStory
Graphic
Wow! This looks great.
I guess presentation
really is everything.
I heard your
ex-wife could only cook
command-line meals.
Face it, you need to make GUIs. If you’re building applications that other people
are going to use, you need a graphical interface. If you’re building programs for yourself, you
want a graphical interface. Even if you believe that the rest of your natural life will be spent
writing server-side code, where the client user interface is a web page, sooner or later you’ll
need to write tools, and you’ll want a graphical interface. Sure, command-line apps are retro,
but not in a good way. They’re weak, inflexible, and unfriendly. We’ll spend two chapters
working on GUIs, and learn key Java language features along the way including Event
Handling and Inner Classes. In this chapter, we’ll put a button on the screen, and make it
do something when you click it. We’ll paint on the screen, we’ll display a jpeg image, and we’ll
even do some animation.
this is a new chapter
 353
your first gui
It all starts with a window
A JFrame is the object that represents
a window on the screen. It’s where you
put all the interface things like buttons,
checkboxes, text fields, and so on. It can
have an honest-to-goodness menu bar
with menu items. And it has all the little
windowing icons for whatever platform
you’re on, for minimizing, maximizing, and
closing the window.
The JFrame looks different depending on
the platform you’re on. This is a JFrame on
Mac OS X:
“If I see one more
command-line app,
you’re fired.”
a and and JFra two a m
 rad
 e
 ‘w w
 io idgets’ it
 button)
 h
 a menu (a butt
 bar
 on
Put widgets in the window
Once you have a JFrame, you can put
things (‘widgets’) in it by adding them
to the JFrame. There are a ton of Swing
components you can add; look for them
in the javax.swing package. The most
common include JButton, JRadioButton,
JCheckBox, JLabel, JList, JScrollPane,
JSlider, JTextArea, JTextField, and
JTable. Most are really simple to use, but
some (like JTable) can be a bit more
complicated.
Making a GUI is easy:
1
 Make a frame (a JFrame)
JFrame frame = new JFrame();
2
 Make a widget (button, text field, etc.)
JButton button = new JButton(“click me”);
3
Add the widget to the frame
frame.getContentPane().add(button);
things trim directly.Think You don’t around to the add the wind of thin
 w ow the indow, gs pane.
 to frame and the as you frame
 the
 add
4
Display it (give it a size and make it visible)
frame.setSize(300,300);
frame.setVisible(true);
354
 chapter 12
getting gui
Your first GUI: a button on a frame
don’t forget to import
 this
import javax.swing.*;
 swing package
public public class JFrame JButton static SimpleGui1 frame button void = = new main {
 new JFrame();
 JButton(“click (String[] args)me”);
 {
 make a
 (you the f
 r
a
 text m
 can e
 a
 pass nd you a the want butt
 button
 on
 on th
e constructor
 button)
frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
frame.getContentPane().add(button);
 just close this sit line the there makes window on the the (if
 progra
m yo
u
 screen lea
ve forever)
 quit this as out soon it as will
 you
frame.setSize(300,300);
 content add the pane
 button to
 the frame’s
give the fr
ame a size,
 in pixels
frame.setVisible(true);
}
}
 you this finally, run step, this make you code)
 it won’t visibl
 se e!! e (if anything you forget
 when
Let’s see what happens when we run it:
%java SimpleGui1
Whoa! That’s a
Really Big Button.
The button fills all the
available space in the frame.
Later we’ll learn to control
where (and how big) the
button is on the frame.
you are here�
 355
user interface events
But nothing happens when I click it...
That’s not exactly true. When you press the button it shows that
‘pressed’ or ‘pushed in’ look (which changes depending on the
platform look and feel, but it always does something to show when
it’s being pressed).
The real question is, “How do I get the button to do something
specific when the user clicks it?”
We need two things:
1
A method to be called when the user
clicks (the thing you want to happen as
a result of the button click).
2
A way to know when to trigger
that method. In other words, a way
to know when the user clicks the
button!
When the user clicks, we want
to know.
We’re interested in the user-
takes-action-on-a-button event.
356
 chapter 12
Dumb there are Questions
 no
Q:
 Will a button look like a
Windows button when you run on
Windows?
A:
 If you want it to. You can
choose from a few “look and
feels”—classes in the core library
that control what the interface looks
like. In most cases you can choose
between at least two different looks:
the standard Java look and feel, also
known as Metal, and the native look
and feel for your platform. The Mac
OS X screens in this book use either
the OS X Aqua look and feel, or the
Metal look and feel.
Q:
 Can I make a program look
like Aqua all the time? Even when
it’s running under Windows?
A:
 Nope. Not all look and feels
are available on every platform. If
you want to be safe, you can either
explicitly set the look and feel to
Metal, so that you know exactly what
you get regardless of where the app
is running, or don’t specify a look
and feel and accept the defaults.
Q:
 I heard Swing was dog-slow
and that nobody uses it.
A:
 This was true in the past,
but isn’t a given anymore. On weak
machines, you might feel the pain of
Swing. But on the newer desktops,
and with Java version 1.3 and be-
yond, you might not even notice the
difference between a Swing GUI and
a native GUI. Swing is used heavily
today, in all sorts of applications.
Getting a user event
Imagine you want the text on the button to
change from click me to I’ve been clicked when
the user presses the button. First we can write a
method that changes the text of the button (a
quick look through the API will show you the
method):
public void changeIt() {
button.setText(“I’ve been clicked!”);
}
But now what? How will we know when this
method should run? How will we know when the
button is clicked?
In Java, the process of getting and handling a
user event is called event-handling. There are
many different event types in Java, although
most involve GUI user actions. If the user clicks
a button, that’s an event. An event that says
“The user wants the action of this button to
happen.” If it’s a “Slow Tempo” button, the user
wants the slow-tempo action to occur. If it’s a
Send button on a chat client, the user wants the
send-my-message action to happen. So the most
straightforward event is when the user clicked
the button, indicating they want an action to
occur.
With buttons, you usually don’t care about any
intermediate events like button-is-being-pressed
and button-is-being-released. What you want to
say to the button is, “I don’t care how the user
plays with the button, how long they hold the
mouse over it, how many times they change their
mind and roll off before letting go, etc. Just tell
me when the user means business! In other words,
don’t call me unless the user clicks in a way that
indicates he wants the darn button to do what it
says it’ll do!”
getting gui
First, the button needs to know
that we care.
1
Hey button, I care about
what happens to you.
your code
bu
ttont
obj
 c
 e2
 The user clicked me!
Second, the button needs a way
to call us back when a button-
clicked event occurs.
A
 brain
 power
1) How could you tell a button object that you
care about its events? That you’re a concerned
listener?
2) How will the button call you back? Assume
that there’s no way for you to tell the button the
name of your unique method (changeIt()). So
what else can we use to reassure the button that
we have a specific method it can call when the
event happens? [hint: think Pet]
you are here�
 357
event listeners
If you care about the button’s events,
implement an interface that says,
“I’m listening for your events.”
A listener interface is the bridge between the
listener (you) and event source (the button).
The Swing GUI components are event sources. In Java terms,
an event source is an object that can turn user actions (click
a mouse, type a key, close a window) into events. And like
virtually everything else in Java, an event is represented as an
object. An object of some event class. If you scan through the
java.awt.event package in the API, you’ll see a bunch of event
classes (easy to spot—they all have Event in the name). You’ll
find MouseEvent, KeyEvent, WindowEvent, ActionEvent, and
several others.
An event source (like a button) creates an event object when the
user does something that matters (like click the button). Most
of the code you write (and all the code in this book) will receive
events rather than create events. In other words, you’ll spend
most of your time as an event listener rather than an event source.
Every event type has a matching listener interface. If you want
MouseEvents, implement the MouseListener interface. Want
WindowEvents? Implement WindowListener. You get the idea.
And remember your interface rules—to implement an interface
you declare that you implement it (class Dog implements Pet),
which means you must write implementation methods for every
method in the interface.
Some interfaces have more than one method because the
event itself comes in different flavors. If you implement
MouseListener, for example, you can get events for
mousePressed, mouseReleased, mouseMoved, etc. Each of
those mouse events has a separate method in the interface,
even though they all take a MouseEvent. If you implement
MouseListener, the mousePressed() method is called when the
user (you guessed it) presses the mouse. And when the user lets
go, the mouseReleased() method is called. So for mouse events,
there’s only one event object, MouseEvent, but several different
event methods, representing the different types of mouse events.
358
 chapter 12
When you implement a
listener interface, you give
the button a way to call
you back. The interface is
where the call-back method
is declared.
<<in
t
Acti erface>
>
actio
 onL
istener
nPer
f
orme
d(Ac
tionE
vent
ev)
<<interface>>
ItemLis
te
ne
r
Change
d(ItemEv
entitemState
ev)
<<inter
fac
e>>
KeyListener
keyPre
ssed(K
eyEven
t ev)
keyRele
ased(K
eyEven
t ev)
keyTyp
ed(Key
Event e
v)
How the listener and source
communicate:
getting gui
“Button, please add me to
your list of listeners and call
my actionPerformed() method
when the user clicks you.”
n.a
 ddA
 ctionListener(t
his
)
t
to
u
b“OK, you’re an ActionListener,
so I know how to call you back
when there’s an event -- I’ll call
the actionPerformed() method
that I know you have.”
a c tio
 nP e rfor m e d ( the E v
 ent
 )
The Listener
If your class wants to know
about a button’s ActionEvents, you
implement the ActionListener
interface. The button needs to
know you’re interested, so you
register with the button by calling its
addActionListener(this) and passing
an ActionListener reference to it (in
this case, you are the ActionListener
so you pass this).The button needs a
way to call you back when the event
happens, so it calls the method in the
listener interface. As an ActionListener,
you must implement the interface’s sole
method, actionPerformed(). The compiler
guarantees it.
The Event Source
A button is a source of ActionEvents,
so it has to know which objects are
interested listeners. The button has an
addActionListener() method to give
interested objects (listeners) a way to
tell the button they’re interested.
When the button’s
addActionListener() runs (because
a potential listener invoked it), the
button takes the parameter (a
reference to the listener object) and
stores it in a list. When the user clicks
the button, the button ‘fires’ the event
by calling the actionPerformed()
method on each listener in the list.
you are here�
 359
getting events
Getting a button’s ActionEvent
1
 Implement the ActionListener interface
2
 Register with the button (tell it you
want to listen for events)
3
 Define the event-handling method (implement
the actionPerformed() method from the
ActionListener interface)
import import public public JButton gui.go();
 java.awt.event.*;
 SimpleGui1B javax.swing.*;
 class static button;
 SimpleGui1B void gui main = new implements (String[] SimpleGui1B();
 a ActionListener ne
w
 im
 1
 ActionListener po
rt
 args) st
at
 an
 em
 d {
ActionEvent ent for {
 the packag ar
 e
 I
 “an m
 in
 ActionL
 p
 (The .
 lement ActionL
 e insta that
 bu nce
 istener”.
 tton t is
 he tener of int
 wil
 e
 l
 r
 implemen
 f
 g
iv
 a
c
 e
 e
.
 e
v
 This ents ters)
 IS-A
 say
 only
 s,
 to
SimpleGui1B
}
2
 public button button.addActionListener(this);
 JFrame void = frame new go() = JButton(“click {
new JFrame();
me”);
 register to The the argu b you
 u
m
 tton, ent r
 in
t
 you “Add e
r
 e
st pass me with Act
 MU tot io
 S
 y
 h
 T o
 nListener!!
 e
 ur b
 be u
 list t
 an t
 o
n. of object
 This listene s fro
 ays
 r
s”.
 m a
class tha
 t
 implementsframe.getContentPane().add(button);
3
 }
 frame.setVisible(true);
 frame.setSize(300,300);
 frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
 Im
 actio
 ac
 plement t
u
a
 n
 l
 P
 e
 e
 r
 v
e
 f
 th
 n
 o
t
 r
m
 e
 -
handling e
 A
 d
 c
 () t
io
 method n
L
is
 met
 tener .. hod
 This !
 interf is th
 ace’s
 e
public void actionPerformed(ActionEvent event) {
button.setText(“I’ve been clicked!”);
}
 }
 happened argument, happened. The
 button is It but enough calls sends we this don’t
 info
 you meth
 an for need ActionEvent od us.
 it. to Knowing let you object know the ev as an ent
 th
 event
 e
360
 chapter 12
Listeners, Sources, and Events
For most of your stellar Java career, you will not be the source
of events.
(No matter how much you fancy yourself the center of your social
universe.)
Get used to it. Your job is to be a good listener.
(Which, if you do it sincerely, can improve your social life.)
getting gui
As a listener, my job is to
implement the interface,
register with the button, and
provide the event-handling.
As an event source, my job is to
accept registrations (from listeners),
get events from the user. and
call the listener’s event-handling
method (when the user clicks me)
Listener GETS the
event
Source SENDS
the event
Hey, what about me? I’m a player too, you
know! As an event object, I’m the argument
to the event call-back method (from the
interface) and my job is to carry data about
the event back to the listener.
Ev
ent objec
 t
Event object
HOLDS DATA
about the event
you are here�
 361
event handling
Dumb there are Questions
 no
Q:
 Why can’t I be a source of events?
 Q:
 I don’t see the importance of the event object
that’s passed to the event call-back methods. If
A:
 You CAN. We just said that most of the time
 other somebody info would calls my I need?
 mousePressed method, what
you’ll be the receiver and not the originator of the
event career). (at Most least of in the the events early days you might of your care brilliant about
 Java
 A:
 A lot of the time, for most designs, you don’t
are ‘fired’ by classes in the Java API, and all you have
 need the event object. It’s nothing more than a little
to do is be a listener for them. You might, however,
 data carrier, to send along more info about the event.
design a program where you need a custom event, say,
 But sometimes you might need to query the event for
StockMarketEvent thrown when your stock market
 specific details about the event. For example, if your
watcher app finds something it deems important. In
 mousePressed() method is called, you know the mouse
that case, you’d make the StockWatcher object be an
 was pressed. But what if you want to know exactly
event source, and you’d do the same things a button
 where the mouse was pressed? In other words, what if
(or any other source) does—make a listener interface
 you want to know the X and Y screen coordinates for
for your custom event, provide a registration method
 where the mouse was pressed?
(addStockListener()), and when somebody calls it, add
Or sometimes you might want to register the same
the caller (a listener) to the list of listeners. Then, when
listener with multiple objects. An onscreen calculator,
a stock event happens, instantiate a StockEvent object
for example, has 10 numeric keys and since they all do
(another class you’ll write) and send it to the listeners
the same thing, you might not want to make a separate
in your list by calling their stockChanged(StockEvent
listener for every single key. Instead, you might
ev) method. And don’t forget that for every event type
register a single listener with each of the 10 keys, and
there must be a matching listener interface (so you’ll
when you get an event (because your event call-back
create a StockListener interface with a stockChanged()
method is called) you can call a method on the event
method).
object to find out who the real event source was. In
Sharpen your pencil
 other words, which key sent this event.
Each of these widgets (user interface objects) are the
 How do you KNOW if
source of one or more events. Match the widgets with
 an object is an event
the events they might cause. Some widgets might be a
 source?
source of more than one event, and some events can be
generated by more than one widget.
 Look in the API.
Widgets
 Event methods
 OK. Look for what?
check box
 windowClosing()
A method that starts with
‘add’, ends with ‘Listener’,
text field
 actionPerformed()
and takes a listener inter-
scrolling list
 itemStateChanged()
 face argument. If you see:
button
 mousePressed()
 addKeyListener(KeyListener k)
dialog box
radio button
menu item
keyTyped()
mouseExited()
focusGained()
you know that a class
with this method is a
source of KeyEvents.
There’s a naming pattern.
362
 chapter 12
Getting back to graphics...
Now that we know a little about how events work (we’ll learn
more later), let’s get back to putting stuff on the screen.
We’ll spend a few minutes playing with some fun ways to get
graphic, before returning to event handling.
Three ways to put things on your GUI:
1
Put widgets on a frame
Add buttons, menus, radio buttons, etc.
frame.getContentPane().add(myButton);
The javax.swing package has more than a dozen
widget types.
2
Draw 2D graphics on a widget
Use a graphics object to paint shapes.
graphics.fillOval(70,70,100,100);
You can paint a lot more than boxes and circles;
the Java2D API is full of fun, sophisticated
graphics methods.
simulat art,
 games,
 ions, et
c.
3
Put a JPEG on a widget
You can put your own images on a widget.
graphics.drawImage(myPic,10,10,this);
getting gui
Number of Head
First Java books
mistakenly
bought by coffee
house baristas.
charts,business
graph
ics,
etc.
you are here�
 363
making a drawing panel
Make your own drawing widget
If you want to put your own graphics on the screen, your best
bet is to make your own paintable widget. You plop that widget
on the frame, just like a button or any other widget, but when it
shows up it will have your images on it. You can even make those
images move, in an animation, or make the colors on the screen
change every time you click a button.
It’s a piece of cake.
Make a subclass of JPanel and override one
method, paintComponent().
All of your graphics code goes inside the paintComponent()
method. Think of the paintComponent() method as the method
called by the system to say, “Hey widget, time to paint yourself.”
If you want to draw a circle, the paintComponent() method will
have code for drawing a circle. When the frame holding your
drawing panel is displayed, paintComponent() is called and your
circle appears. If the user iconifies/minimizes the window, the
JVM knows the frame needs “repair” when it gets de-iconified,
so it calls paintComponent() again. Anytime the JVM thinks the
display needs refreshing, your paintComponent() method will be
called.
One more thing, you never call this method yourself! The argument
to this method (a Graphics object) is the actual drawing canvas
that gets slapped onto the real display. You can’t get this by
yourself; it must be handed to you by the system. You’ll see
later, however, that you can ask the system to refresh the display
(repaint()), which ultimately leads to paintComponent() being
called.
 import import class MyDrawPanel public java.awt.*;
 javax.swing.*;
 g.setColor(Color.orange);
 void paintComponent(Graphics extends you JPanel nee
d
 both {
 of
 these
 Make that anything own g) cust
 you a { su omized b c else. a c This Y
 n la
 system fresh ou that ss
 add Except will o
 is widget.
 f
 you to the
 drawing J
 calls NEVER P
 a a
 may this B
 n
 frame el, ig
 it Im
 pa
 a surface, and o
ne call widget po
 int ju rt
 is says, st
 th
 on an
 your
 like
 is of t now.”.
 “Here’
 yourself. Graphics type s
 Graphics,
 a
 ni
 Th
 met
 ce
 e
 hod.
}
 }
 g.fillRect(20,50,100,100);
 where w
hat telling Imagine shape it it that goes what to and ‘g paint co
 ’ is
 lo
 ho a rw (with painting to big paint it coordinates is)
 machine.
 with and You’re
 for
 then
364
 chapter 12
getting gui
Fun things to do in paintComponent()
Let’s look at a few more things you can do in paintComponent().
The most fun, though, is when you start experimenting yourself.
Try Graphics what’s Display public playing in the (later void with a Graphics JPEG
 we’ll paintComponent(Graphics the numbers, see class).
 that there’s and check even the more API g) you {
 for can class
 do besides
 Your you’re try Image getRe
 file this using image source(“catz
 nam
 line an e
 of = g
 IDE o
 new e
 co s
 de h
 and ere. instead: ha
 N v o e te: If
 tClass().
 Image();
difficulty,
ImageIcon(geilla.jpg”)).get
Image image = new ImageIcon(“catzilla.jpg”).getImage();
g.drawImage(image,3,4,this);
}
subclass), relative top the The left edge left x,y corner to not coordin
 of edge the the shou the of w ates idget ld panel”. entire the go. for panel (in This These frame.
 where this and says numbers case the 4 “3 pixels your picture’s pixels are JPane from
 from always
 to
 l
the
 p
Paint a randomly-colored circle
on a black background
public void paintComponent(Graphics g) {
 fil (the l
 t
h
 d
 e
 efault e
ntire c
olo panel r)
 w
ith black
g.fillRect(0,0,this.getWidth(), int int int red blue green === (int) (int) (int) (Math.random() (Math.random() (Math.random() * this.getHeight());
 ** 256);
 256);
 256);
 make rectangle other The left starts, corner, first edge the two so relative height as and two 0, args wide 0 0 args say, means to as pixels as the tall define “Make the “start from panel, as panel the
 the the the 0 for (this.w panel (x,y)
 width pixels top whe (this.height)”
 upper idth()), edge.” re of from drawing
 this
 left
 the
 The
 and
Color randomColor = new Color(red, green, blue);
}
 g.setColor(randomColor);
 g.fillOval(70,70,100,100);
 100 the start top, pixels 70
 ma pixels t ke all.
 it from 100 pixels the left,
 wide, 70 and
 from
 to You represent can mak
e
 the a color RGBby va
lues.
 passing
 in 3 ints
you are here�
 365
drawing gradients with Graphics2D
Behind every good Graphics reference
is a Graphics2D object.
The argument to paintComponent() is declared as type
Graphics (java.awt.Graphics).
public void paintComponent(Graphics g) { }
So the parameter ‘g’ IS-A Graphics object. Which means it
could be a subclass of Graphics (because of polymorphism).
And in fact, it is.
The object referenced by the ‘g’ parameter is actually an
instance of the Graphics2D class.
Why do you care? Because there are things you can do with
a Graphics2D reference that you can’t do with a Graphics
reference. A Graphics2D object can do more than a Graphics
object, and it really is a Graphics2D object lurking behind the
Graphics reference.
Remember your polymorphism. The compiler decides which
methods you can call based on the reference type, not the
object type. If you have a Dog object referenced by an Animal
reference variable:
Animal a = new Dog();
You CANNOT say:
a.bark();
Even though you know it’s really a Dog back there. The
compiler looks at ‘a’, sees that it’s of type Animal, and finds
that there’s no remote control button for bark() in the Animal
class. But you can still get the object back to the Dog it really is
by saying:
Dog d = (Dog) a;
d.bark();
So the bottom line with the Graphics object is this:
If you need to use a method from the Graphics2D class, you
can’t use the the paintComponent parameter (‘g’) straight
from the method. But you can cast it with a new Graphics2D
variable.
Graphics2D g2d = (Graphics2D) g;
366
 chapter 12
Methods you can call on a
Graphics reference:
drawImage()
drawLine()
drawPolygon
drawRect()
drawOval()
fillRect()
fillRoundRect()
setColor()
To cast the Graphics2D object to
a Graphics2D reference:
Graphics2D g2d = (Graphics2D) g;
Methods you can call on
a Graphics2D reference:
fill3DRect()
draw3DRect()
rotate()
scale()
shear()
transform()
setRenderingHints()
(these are not complete method lists,
check the API for more)
getting gui
Because life’s too short to paint the
circle a solid color when there’s a
gradient blend waiting for you.
it’s masquer
 re
a
ll
 y
 ading a
 G
r
 as aphics2D a mer
 e G object
 raphics
object.
public void paintComponent(Graphics g) {
Graphics2D g2d = (Graphics2D) g;
cast it so we can call something that
Graphics2D has but Graphics doesn’t
GradientPaint gradient = new GradientPaint(70,70,Color.blue, 150,150, Color.orange);
startin
g point
 startin
g color ending p
 oint
 ending
 color
g2d.setPaint(gradient);
 gradient this sets instead the virtu
 of al a pa so int lid brush color
 to a
g2d.fillOval(70,70,100,100);
}
 the the paintbrush
 fillOva oval wit l() (i.e. h m
 whatever e
 the t
h
o
d
 gradient)”
 re
 is a
 lly loaded means o
n “f y il o l ur
public void paintComponent(Graphics g) {
Graphics2D g2d = (Graphics2D) g;
int int Color int green blue red startColor ==(int) = (int) (int) (Math.random() (Math.random() =(Math.random() new Color(red, **256);
 *256);
 green, 256);
 blue);
 this except the gradien
 is sta ju
 it rt t. st
makes Try and li
k
e
 the stop it!
 random one color
 ab colors sove,
 of the
 for
red = (int) (Math.random() * 256);
green = (int) (Math.random() * 256);
blue = (int) (Math.random() * 256);
Color endColor = new Color(red, green, blue);
GradientPaint gradient = new GradientPaint(70,70,startColor, 150,150, endColor);
g2d.setPaint(gradient);
g2d.fillOval(70,70,100,100);
}
you are here�
367
events and graphics
����������BULLET POINTS
EVENTS
To make a GUI, start with a window, usually a JFrame
JFrame frame = new JFrame();
You can add widgets (buttons, text fields, etc.) to the
JFrame using:
frame.getContentPane().add(button);
Unlike most other components, the JFrame doesn’t let
you add to it directly, so you must add to the JFrame’s
content pane.
To make the window (JFrame) display, you must give it
a size and tell it be visible:
frame.setSize(300,300);
frame.setVisible(true);
To know when the user clicks a button (or takes some
other action on the user interface) you need to listen for
a GUI event.
To listen for an event, you must register your interest
with an event source. An event source is the thing
(button, checkbox, etc.) that ‘fires’ an event based on
user interaction.
The listener interface gives the event source a way
to call you back, because the interface defines the
method(s) the event source will call when an event
happens.
To register for events with a source, call the source’s
registration method. Registration methods always take
the form of: add<EventType>Listener. To register for a
button’s ActionEvents, for example, call:
button.addActionListener(this);
Implement the listener interface by implementing all of
the interface’s event-handling methods. Put your event-
handling code in the listener call-back method. For
ActionEvents, the method is:
public void actionPerformed(ActionEvent
event) {
button.setText(“you clicked!”);
}
The event object passed into the event-handler method
carries information about the event, including the source
of the event.
368
 chapter 12
��������GRAPHICS
You can draw 2D graphics directly on to a widget.
You can draw a .gif or .jpeg directly on to a widget.
To draw your own graphics (including a .gif or
.jpeg), make a subclass of JPanel and override the
paintComponent() method.
The paintComponent() method is called by the GUI
system. YOU NEVER CALL IT YOURSELF. The
argument to paintComponent() is a Graphics object that
gives you a surface to draw on, which will end up on
the screen. You cannot construct that object yourself.
Typical methods to call on a Graphics object (the
paintComponent paramenter) are:
g.setColor(Color.blue);
g.fillRect(20,50,100,120);
To draw a .jpg, construct an Image using:
Image image = new ImageIcon(“catzilla.
jpg”).getImage();
and draw the image using:
g.drawImage(image,3,4,this);
The object referenced by the Graphics parameter
to paintComponent() is actually an instance of the
Graphics2D class. The Graphics 2D class has a variety
of methods including:
fill3DRect(), draw3DRect(), rotate(), scale(), shear(),
transform()
To invoke the Graphics2D methods, you must cast the
parameter from a Graphics object to a Graphics2D
object:
Graphics2D g2d = (Graphics2D) g;
We can get an event.
We can paint graphics.
But can we paint graphics when we get an event?
Let’s hook up an event to a change in our drawing panel. We’ll make the circle
change colors each time you click the button. Here’s how the program flows:
Start the app
1
The frame is built with the two widgets
(your drawing panel and a button). A
listener is created and registered with
the button. Then the frame is displayed
and it just waits for the user to click.
getting gui
2The user clicks the button and the
button creates an event object and
calls the listener’s event handler.
3The event handler calls repaint() on the
frame. The system calls paintComponent()
on the drawing panel.
4
Voila! A new color is painted because
paintComponent() runs again, filling the
circle with a random color.
you are here�
 369
building a GUI frame
Wait a minute...how
do you put TWO
things on a frame?
GUI layouts: putting more than one
widget on a frame
We cover GUI layouts in the next chapter, but we’ll do a
quickie lesson here to get you going. By default, a frame
has five regions you can add to. You can add only one thing
to each region of a frame, but don’t panic! That one thing
might be a panel that holds three other things including a
panel that holds two more things and... you get the idea. In
fact, we were ‘cheating’ when we added a button to the frame
using:
frame.getContentPane().add(button);
This mandat default WHERE widg
 When met the the is
 e
 h
 t
 t
 wid ory) o
 cen
 h
 y
 d
 content t
 o
 e
 (which o u
 get t
 (the b
 er go.
 c
 e
 way a
 t
 ll
 ter will region.
 one t
 region) to pane. he automa
 (and add we single- sho Alw u you
 t su t
 o u ays ally
 ic
 a ld a r want a
 n’t g lly frame’s
 specify
 add
 land use)
 the
in
 frame.getContentPane().add(BorderLayout.CENTER, north
 to This do and th
 we isn
 it at call ’t the (t takes really he the widget one-arg
 a two-argu
 the region to way add ad (u you
 me
 d sin
 meth to nt ’re
 g button);
 a that add s o upposed
 constant)
 d).
method,
 region.
east
default region
 west
 center
 Sharpen your pencil
south
Given the pictures on page 369, write the
code that adds the button and the panel to
the frame.
370
 chapter 12
getting gui
The click circle the button.
 changes color each time you
 (instance The cust
o o m f drawing panel
of is in the the fra C me.
 ENTER MyDrawPanel)
 region
import javax.swing.*;
import java.awt.*;
import java.awt.event.*;
public class SimpleGui3C implements ActionListener {
JFrame frame;
public SimpleGui3C static void gui = main new(String[] SimpleGui3C();
 args) {
 Button SOUTHis region in the
 of
gui.go();
 the fra
me
}
public void go() {
frame = new JFrame();
frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
button.addActionListener(this);
 JButton button = new JButton(“Change colors”);
 Add to the
 the bu li tton.
 stener (th
is)
MyDrawPanel drawPanel = new MyDrawPanel();
frame.setSize(300,300);
 frame.getContentPane().add(BorderLayout.CENTER, frame.getContentPane().add(BorderLayout.SOUTH, button);
 drawPanel);
 frame.
 the Add ton two and thedrawing regions two wi
dg of pa et ne th s l) e
 (but-
 to
frame.setVisible(true);
}
public void actionPerformed(ActionEvent event) {
frame.repaint();
}
 }
 When to paintCompone widget repaint() the in user th
 itse e nt() frame!
 cl lf
. ic
ks, is That called tell means
 the on frame
 every
class MyDrawPanel extends JPanel {
public void paintComponent(Graphics g) {
// Code to fill the oval with a random color
// See page 367 for the code
}
}
clicks.
 The method drawing
 is call panel’s ed every time
 the user
paintComponen
t()
you are here�
371
multiple listeners
Let’s try it with TWO buttons
The south button will act as it does now, simply calling repaint on the
frame. The second button (which we’ll stick in the east region) will
change the text on a label. (A label is just text on the screen.)
So now we need FOUR widgets
label will
go here
west
north
center
south
east
lab butto
 e
l
-
c
 n h
anging will b
e here
drawing panel goes
in the center
color-changing
button will go here
And we need to get
TWO events
Uh-oh.
Is that even possible? How do
you get two events when you
have only one actionPerformed()
method?
This button changes the text
on the opposite side
372
 chapter 12
This of the button circle
 changes the color
How do you get action events for t wo different buttons,
when each button needs to do something different?
getting gui
1
 option one
Implement two actionPerformed() methods
class MyGui implements ActionListener {
// lots of code here and then:
public void actionPerformed(ActionEvent event) {
frame.repaint();
 But this is impossible!
}
public void actionPerformed(ActionEvent event) {
label.setText(“That hurt!”);
}
}
Flaw: You can’t! You can’t implement the same method twice in a Java class. It won’t compile.
And even if you could, how would the event source know which of the two methods to call?
2
 option two
Register the same listener with both buttons.
class MyGui implements ActionListener {
// declare a bunch of instance variables here
public void go() {
// build gui
colorButton = new JButton();
labelButton = new JButton();
colorButton.addActionListener(this);
 Register the same listener
labelButton.addActionListener(this);
 with both buttons
// more gui code here ...
}
public void actionPerformed(ActionEvent event) {
}
 if } else (event.getSource() label.setText(“That frame.repaint();
 {
 == hurt!”);
 colorButton) {
 Query to actually that find to the out fir dec
 ev ed w ide en hi it, t ch what object
 and button
 to use
 do.
}
}
Flaw: this does work, but in most cases it’s not very OO. One event handler
doing many different things means that you have a single method doing many different things.
If you need to change how one source is handled, you have to mess with everybody’s event
handler. Sometimes it is a good solution, but usually it hurts maintainability and extensibility.
you are here�
 373
multiple listeners
How do you get action events for t wo different buttons,
when each button needs to do something different?
3
option three
Create two separate ActionListener classes
class MyGui {
JFrame frame;
JLabel label;
void gui() {
// code to instantiate the two listeners and register one
// with the color button and the other with the label button
}
} // close class
class ColorButtonListener implements ActionListener {
public void actionPerformed(ActionEvent event) {
frame.repaint();
}
 }
 the Won’t ‘frame’ work! variable This class ofdoesn’t the MyGui have class
 a referenc
 e to
class LabelButtonListener implements ActionListener {
public void actionPerformed(ActionEvent event) {
label.setText(“That hurt!”);
}
}
 Problem! This class has no refere
nce to the variable ‘label’
Flaw: these classes won’t have access to the variables they need
to act on, ‘frame’ and ‘label’. You could fix it, but you’d have to give each of the
listener classes a reference to the main GUI class, so that inside the actionPerformed()
methods the listener could use the GUI class reference to access the variables of the GUI
class. But that’s breaking encapsulation, so we’d probably need to make getter methods
for the gui widgets (getFrame(), getLabel(), etc.). And you’d probably need to add a
constructor to the listener class so that you can pass the GUI reference to the listener at
the time the listener is instantiated. And, well, it gets messier and more complicated.
There has got to be a better way!
374
 chapter 12
getting gui
Wouldn’t it be wonderful if you
could have two different listener classes,
but the listener classes could access the
instance variables of the main GUI class,
almost as if the listener classes belonged
to the other class. Then you’d have the best
of both worlds. Yeah, that would be dreamy.
But it’s just a fantasy...
you are here�
 375
inner classes
Inner class to the rescue!
You can have one class nested inside another. It’s easy.
Just make sure that the definition for the inner class is
inside the curly braces of the outer class.
Simple inner class:
class MyOuterClass
 {
class void MyInnerClass go() {
 {
 I
n
n
e
r
 c
la
ss by is ou
 fu t ll er y
 class
enclosed
}
}
}
An inner class gets a special pass to use the outer class’s stuff. Even
the private stuff. And the inner class can use those private variables
and methods of the outer class as if the variables and members
were defined in the inner class. That’s what’s so handy about inner
classes—they have most of the benefits of a normal class, but with
special access rights.
Inner class using an outer class variable
class MyOuterClass
 {
private int x;
class MyInnerClass {
void }
 x go() = 42;
{
 use of the ‘x’ as inner if it class!
 were a va
riable
} // close inner class
} // close outer class
An inner class can
use all the methods
and variables of the
outer class, even the
private ones.
The inner class gets
to use those variables
and methods just
as if the methods
and variables were
declared within the
inner class.
376
 chapter 12
getting gui
An inner class instance must be tied to
an outer class instance*.
 An inner object
shares a special
Remember, when we talk about an inner class accessing
 bond with an
something in the outer class, we’re really talking about an
 outer object.
instance of the inner class accessing something in an instance of
the outer class. But which instance?
Can any arbitrary instance of the inner class access the methods
and variables of any instance of the outer class? No!
An inner object must be tied to a specific outer object on
1
 Make an instance of
the heap.
the outer class
ies
 sold!
6 cop
 t
65,53
 cOver
 My
Outer obje
Gett
 t
o
 your u
c
 i
 h
 n
g
 i
 w
 n
 i
 i
 n
 n
 t
h
 e
r
 2
 the of Make using the inner the an outer instance class, instance
 class.
 by
 My
Inner of
 obje
 t
 cclas
s
 3 The outer and inner objects
D
r
 .
 P
oly M
orph
ism
 are now intimately linked.
the
 int x
b
e
 l
e
r
 f
rom
 d
 m
y Ch
ar?”
The
 auth
 new
 or
 o
f
 “
 stsel
 Who
 Move
 These have can use a two sp th ec o e ia b outer’s l je bond. cts on The variables
 theinner hea
p
 String s
 oute
 r
(and vice-
versa).
 inner
*There’s an exception to this, for a very special case—an inner class defined
within a static method. But we’re not going there, and you might go your entire
Java life without ever encountering one of these.
you are here�
 377
inner class instances
How to make an instance of an inner class
If you instantiate an inner class from code within an outer class, the instance
of the outer class is the one that the inner object will ‘bond’ with. For
example, if code within a method instantiates the inner class, the inner
object will bond to the instance whose method is running.
Code in an outer class can instantiate one of its own inner classes, in exactly
the same way it instantiates any other class... new MyInner()
class private MyOuter
 int x;
 {
 The instance outer va
 c ri la able ss has ‘x’
 a privat
e
MyInner inner = new MyInner();
 inner Make class an inst
ance of the
public void doStuff() {
inner.go();
 call a method on the
}
 inner class
class MyInner {
void go() {
}
 x = 42;
 The outer method class instance in the inner variab
 clas
 le
 s ‘x’, uses as the
 if
 ‘x’
} // close inner class
 belonged to the
 inn
er
 clas
s.} // close outer class
outer
MyOuter
outer
MyOuter
special
bond
x
int
x
int
Side bar
You can instantiate an inner instance from code running outside the outer class, but you
have to use a special syntax. Chances are you’ll go through your entire Java life and never
need to make an inner class from outside, but just in case you’re interested...
class Foo {
public static void main (String[] args) {
MyOuter outerObj = new MyOuter();
MyOuter.MyInner innerObj = outerObj.new MyInner();
}
}
378
 chapter 12
inner
MyInner
getting gui
Now we can get the two-button
code working
public class TwoButtons {
 the implement main GUI Ac
 ti
 cl on as L s ist
ener doesn’t
 now
JFrame frame;
JLabel label;
public static void main (String[] args) {
TwoButtons gui = new TwoButtons ();
gui.go();
}
public void go() {
frame = new JFrame();
frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
labelButton.addActionListener(new JButton JButton labelButton colorButton = = new new JButton(“Change JButton(“Change LabelListener());
 Label”);
 Circle”);
 inst
 button’s method, the e
a
 appro
 d
 o
f
 list
 pa p p
 ss riate a
 ener ss
 a ing new listener regist (this) inst rat
 ance
 to class.
 ion t
he
 of
colorButton.addActionListener(new ColorListener());
label = new JLabel(“I’m a label”);
 TwoButtons
MyDrawPanel drawPanel = new MyDrawPanel();
 object
frame.getContentPane().add(BorderLayout.SOUTH, frame.getContentPane().add(BorderLayout.CENTER,colorButton);
 drawPanel);
 inner
 outer
frame.getContentPane().add(BorderLayout.EAST, labelButton);
frame.getContentPane().add(BorderLayout.WEST, label);
 LabelListener
 object
 inner
frame.setSize(300,300);
 ColorListener
frame.setVisible(true);
 object
}
class LabelListener implements ActionListener {
 Now TWO
 in a we single A
ct
 get io
clas
 nL
 to iste
ners
 s!
 have
public void actionPerformed(ActionEvent event) {
label.setText(“Ouch!”);
}
 inner class kn
ows
} // close inner class
 about ‘label’
class ColorListener implements ActionListener {
public void actionPerformed(ActionEvent event) {
frame.repaint();
} }
 // close inner class
 object.
 explicit in
stance the inner reference
 variable, class ge
ts
 witho
 to to the ut use outer having
 the ‘frame’
 cl an
 ass
}
you are here�
 379
inner classes
Java Exposed
This weeks interview:
Instance of an Inner Class
HeadFirst: What makes inner classes important?
Inner object: Where do I start? We give you a chance to
implement the same interface more than once in a class.
Remember, you can’t implement a method more than
once in a normal Java class. But using inner classes, each
inner class can implement the same interface, so you can
have all these different implementations of the very same
interface methods.
HeadFirst: Why would you ever want to implement the
same method twice?
Inner object: Let’s revisit GUI event handlers. Think
about it... if you want three buttons to each have a
different event behavior, then use three inner classes, all
implementing ActionListener­­ —which means each class
gets to implement its own actionPerformed method.
HeadFirst: So are event handlers the only reason to use
inner classes?
Inner object: Oh, gosh no. Event handlers are just an
obvious example. Anytime you need a separate class, but
still want that class to behave as if it were part of another
class, an inner class is the best—and sometimes only—way
to do it.
HeadFirst: I’m still confused here. If you want the inner
class to behave like it belongs to the outer class, why have
a separate class in the first place? Why wouldn’t the inner
class code just be in the outer class in the first place?
Inner object: I just gave you one scenario, where you
need more than one implementation of an interface. But
even when you’re not using interfaces, you might need
two different classes because those classes represent two
different things. It’s good OO.
HeadFirst: Whoa. Hold on here. I thought a big part of
OO design is about reuse and maintenance. You know, the
idea that if you have two separate classes, they can each
be modified and used independently, as opposed to stuffing
it all into one class yada yada yada. But with an inner class,
you’re still just working with one real class in the end, right?
The enclosing class is the only one that’s reusable and
380
 chapter 12
separate from everybody else. Inner classes aren’t exactly
reusable. In fact, I’ve heard them called “Reuseless—
useless over and over again.”
Inner object: Yes it’s true that the inner class is not as
reusable, in fact sometimes not reusable at all, because it’s
intimately tied to the instance variables and methods of
the outer class. But it—
HeadFirst: —which only proves my point! If they’re not
reusable, why bother with a separate class? I mean, other
than the interface issue, which sounds like a workaround
to me.
Inner object: As I was saying, you need to think about
IS-A and polymorphism.
HeadFirst: OK. And I’m thinking about them because...
Inner object: Because the outer and inner classes
might need to pass different IS-A tests! Let’s start with the
polymorphic GUI listener example. What’s the declared
argument type for the button’s listener registration
method? In other words, if you go to the API and check,
what kind of thing (class or interface type) do you have to
pass to the addActionListener() method?
HeadFirst: You have to pass a listener. Something that
implements a particular listener interface, in this case
ActionListener. Yeah, we know all this. What’s your point?
Inner object: My point is that polymorphically, you have
a method that takes only one particular type. Something
that passes the IS-A test for ActionListener. But—and
here’s the big thing—what if your class needs to be
an IS-A of something that’s a class type rather than an
interface?
HeadFirst: Wouldn’t you have your class just extend the
class you need to be a part of ? Isn’t that the whole point
of how subclassing works? If B is a subclass of A, then
anywhere an A is expected a B can be used. The whole
pass-a-Dog-where-an-Animal-is-the-declared-type thing.
Inner object: Yes! Bingo! So now what happens if you
need to pass the IS-A test for two different classes? Classes
that aren’t in the same inheritance hierarchy?
getting gui
HeadFirst: Oh, well you just... hmmm. I think I’m get-
ting it. You can always implement more than one interface,
but you can extend only one class. You can only be one kind
of IS-A when it comes to class types.
Inner object: Well done! Yes, you can’t be both a Dog
and a Button. But if you’re a Dog that needs to some-
times be a Button (in order to pass yourself to methods
that take a Button), the Dog class (which extends Animal
so it can’t extend Button) can have an inner class that acts
on the Dog’s behalf as a Button, by extending Button,
and thus wherever a Button is required the Dog can
pass his inner Button instead of himself. In other words,
instead of saying x.takeButton(this), the Dog object calls
x.takeButton(new MyInnerButton()).
HeadFirst: Can I get a clear example?
Inner object: Remember the drawing panel we used,
where we made our own subclass of JPanel? Right now,
that class is a separate, non-inner, class. And that’s fine,
because the class doesn’t need special access to the instance
variables of the main GUI. But what if it did? What if
we’re doing an animation on that panel, and it’s getting its
coordinates from the main application (say, based on some-
thing the user does elsewhere in the GUI). In that case, if
we make the drawing panel an inner class, the drawing
panel class gets to be a subclass of JPanel, while the outer
class is still free to be a subclass of something else.
HeadFirst: Yes I see! And the drawing panel isn’t reus-
able enough to be a separate class anyway, since what it’s
actually painting is specific to this one GUI application.
Inner object: Yes! You’ve got it!
HeadFirst: Good. Then we can move on to the nature of
the relationship between you and the outer instance.
Inner object: What is it with you people? Not enough
sordid gossip in a serious topic like polymorphism?
HeadFirst: Hey, you have no idea how much the public is
willing to pay for some good old tabloid dirt. So, someone
creates you and becomes instantly bonded to the outer
object, is that right?
Inner object: Yes that’s right. And yes, some have
compared it to an arranged marriage. We don’t have a say
in which object we’re bonded to.
HeadFirst: Alright, I’ll go with the marriage analogy.
Can you get a divorce and remarry something else?
Inner object: No, it’s for life.
HeadFirst: Whose life? Yours? The outer object? Both?
Inner object: Mine. I can’t be tied to any other outer
object. My only way out is garbage collection.
HeadFirst: What about the outer object? Can it be
associated with any other inner objects?
Inner object: So now we have it. This is what you really
wanted. Yes, yes. My so-called ‘mate’ can have as many
inner objects as it wants.
HeadFirst: Is that like, serial monogamy? Or can it have
them all at the same time?
Inner object: All at the same time. There. Satisfied?
HeadFirst: Well, it does make sense. And let’s not
forget, it was you extolling the virtues of “multiple
implementations of the same interface”. So it makes sense
that if the outer class has three buttons, it would need
three different inner classes (and thus three different inner
class objects) to handle the events. Thanks for everything.
Here’s a tissue.
He thinks he’s
got it made, having two
inner class objects. But we
have access to all his private
data, so just imagine the damage
we could do...
you are here�
 381
inner classes
Using an inner class for animation
We saw why inner classes are handy for event listeners, because
you get to implement the same event-handling method more
than once. But now we’ll look at how useful an inner class is when
used as a subclass of something the outer class doesn’t extend. In
other words, when the outer class and inner class are in different
inheritance trees!
Our goal is to make a simple animation, where the circle moves
across the screen from the upper left down to the lower right.
start
 finish
How simple animation works
1
 Paint an object at a particular x and y coordinate
g.fillOval(20,50,100,100);
20 pixels from the left,
50 pixels from the top
2
Repaint the object at a different x and y coordinate
g.fillOval(25,55,100,100);
25 pixels from the left, 55
pixels from the top
(the object moved a little
down and to the right)
382
3
 Repeat the previous step with changing x and y values
for as long as the animation is supposed to continue.
chapter 12
Dumb there are Questions
 no
Q:
Why are we learning about
animation here? I doubt if I’m
going to be making games.
A:
 You might not be making
games, but you might be
creating simulations, where
things change over time to show
the results of a process. Or you
might be building a visualization
tool that, for example, updates
a graphic to show how much
memory a program is using,
or to show you how much
traffic is coming through
your load-balancing server.
Anything that needs to take a
set of continuously-changing
numbers and translate them into
something useful for getting
information out of the numbers.
Doesn’t that all sound business-
like? That’s just the “official
justification”, of course. The real
reason we’re covering it here is
just because it’s a simple way
to demonstrate another use
of inner classes. (And because
we just like animation, and our
next Head First book is about
J2EE and we know we can’t get
animation in that one.)
What we really want is something like...
class MyDrawPanel extends JPanel {
public void paintComponent(Graphics g) {
g.setColor(Color.orange);
g.fillOval(x,y,100,100);
}
 }
 e
 called, different
 a
c
h
 t
im
 the e
 p
 lo a
 oc va in
 ation
 tComponent l gets pa
in
t () ed is
 at a
Sharpen your pencil
But where do we get the new x and y
coordinates?
And who calls repaint()?
See if you can design a simple solution to get the ball to animate from the top left of the
drawing panel down to the bottom right. Our answer is on the next page, so don’t turn
this page until you’re done!
Big Huge Hint: make the drawing panel an inner class.
Another Hint: don’t put any kind of repeat loop in the paintComponent() method.
Write your ideas (or the code) here:
getting gui
you are here�
 383
animation using an inner class
The complete simple animation code
import javax.swing.*;
import java.awt.*;
public int int class x y = = 70;
 70;
 SimpleAnimation make main coordinates
 GUI two {
 in
 class, st
 of a
n
for ce
 the va
 the circle.
 r
 iables x and in the
 y
public static void main (String[] args) {
SimpleAnimation gui = new SimpleAnimation ();
gui.go();
}
public void go() {
JFrame frame = new JFrame();
frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
frame.getContentPane().add(drawPanel);
 MyDrawPanel drawPanel = new MyDrawPanel();
 and Nothing put them new he
 in re th
 . Make e frame.
 the widge
ts
frame.setSize(300,300);
frame.setVisible(true);
This is where
 the
 for (int i = 0; i < 130; i++) {
 repeat this 130 times
action is!
x++;
 increment the x and y
y++;
 drawPanel.repaint();
 coordinates
 tell can th
 see e
 the p
a
n
el
 cir
 t
 cle o
 re
 in p
aint the itself new
 loca
 (so tion we
 )
try {
}// close }
 } Thread.sleep(50);
 go() catch(Exception method
 ex) { }
 threads weren’t quickly Slow it you in supposed down chapter won’t a little to SEE 15.
 already (ot it he
 mo
ve)
 rw
ise know . Don’t it this. will We’ll worry, move get so
 you
 to
it’s an
 class MyDrawPanel extends JPanel {
Now inne
r class.
 public void paintComponent(Graphics g) {
g.setColor(Color.green);
}
 g.fillOval(x,y,40,40);
 coordinates Use the contin
 of ua th lly-updated e outer class.
 x and y
} // close inner class
} // close outer class
384
 chapter 12
getting gui
Uh-oh. It didn’t move... it smeared.
What did we do wrong?
There’s one little flaw in the paintComponent()
method.
We forgot to erase what was
already there! So we got trails.
To fix it, all we have to do is fill in the entire panel with
the background color, before painting the circle each
time. The code below adds two lines at the start of the
method: one to set the color to white (the background
color of the drawing panel) and the other to fill the
entire panel rectangle with that color. In English, the
code below says, “Fill a rectangle starting at x and y of
0 (0 pixels from the left and 0 pixels from the top) and
make it as wide and as high as the panel is currently.
Not exactly th
e look
we were going for.public void paintComponent(Graphics g) {
g.setColor(Color.white);
g.fillRect(0,0,this.getWidth(), this.getHeight());
}
 g.setColor(Color.green);
 g.fillOval(x,y,40,40);
 methods getWidth() inherite and
 ge
 d tHeight() from JPanel.
 are
Sharpen your pencil (optional, just for fun)
What changes would you make to the x and y coordinates to produce the animations below?
(assume the first one example moves in 3 pixel increments)
1
start
finish
X
Y+3
+3
1
start
finish
X
Y
2
3
startstart

finish
finish
X
Y
X
Y
2
3
startstart

X
Y
finish
X
Y
finish
you are here�
385
Code Kitchen
Code Kitchen
beat one
 beat two
 beat three
 beat four ...
Let’s make a music video. We’ll use Java-generated random
graphics that keep time with the music beats.
Along the way we’ll register (and listen for) a new kind of
non-GUI event, triggered by the music itself.
Remember, this part is all optional. But we think it’s good for you.
And you’ll like it. And you can use it to impress people.
(Ok, sure, it might work only on people who are really easy to impress,
but still...)
386
 chapter 12
Listening for a non-GUI event
OK, maybe not a music video, but we will make
a program that draws random graphics on the
screen with the beat of the music. In a nutshell,
the program listens for the beat of the music
and draws a random graphic rectangle with each
beat.
That brings up some new issues for us. So far,
we’ve listened for only GUI events, but now
we need to listen for a particular kind of MIDI
event. Turns out, listening for a non-GUI event is
just like listening for GUI events: you implement
a listener interface, register the listener with an
event source, then sit back and wait for the event
source to call your event-handler method (the
method defined in the listener interface).
The simplest way to listen for the beat of the
music would be to register and listen for the
actual MIDI events, so that whenever the
sequencer gets the event, our code will get it
too and can draw the graphic. But... there’s a
problem. A bug, actually, that won’t let us listen
for the MIDI events we’re making (the ones for
NOTE ON).
So we have to do a little work-around. There
is another type of MIDI event we can listen
for, called a ControllerEvent. Our solution
is to register for ControllerEvents, and then
make sure that for every NOTE ON event,
there’s a matching ControllerEvent fired at
the same ‘beat’. How do we make sure the
ControllerEvent is fired at the same time? We
add it to the track just like the other events! In
other words, our music sequence goes like this:
BEAT 1 - NOTE ON, CONTROLLER EVENT
BEAT 2 - NOTE OFF
BEAT 3 - NOTE ON, CONTROLLER EVENT
BEAT 4 - NOTE OFF
and so on.
Before we dive into the full program, though,
let’s make it a little easier to make and add MIDI
messages/events since in this program, we’re
gonna make a lot of them.
getting gui
What the music art program
needs to do:
123Make a series of MIDI messages/
events to play random notes on a piano
(or whatever instrument you choose)
Register a listener for the events
Start the sequencer playing
4Each time the listener’s event
handler method is called, draw a
random rectangle on the drawing
panel, and call repaint.
We’ll build it in three iterations:
1Version One: Code that simplifies mak-
ing and adding MIDI events, since we’ll
be making a lot of them.
2Version Two: Register and listen for
the events, but without graphics.
Prints a message at the command-line
with each beat.
3Version Three: The real deal. Adds
graphics to version two.
you are here�
 387
utility method for events
An easier way to make
messages / events
Right now, making and adding messages and
events to a track is tedious. For each message,
we have to make the message instance (in this
case, ShortMessage), call setMessage(), make a
MidiEvent for the message, and add the event
to the track. In last chapter’s code, we went
through each step for every message. That
means eight lines of code just to make a note
play and then stop playing! Four lines to add a
NOTE ON event, and four lines to add a NOTE
OFF event.
ShortMessage a = new ShortMessage();
a.setMessage(144, 1, note, 100);
MidiEvent noteOn = new MidiEvent(a, 1);
track.add(noteOn);
ShortMessage b = new ShortMessage();
b.setMessage(128, 1, note, 100);
MidiEvent noteOff = new MidiEvent(b, 16);
track.add(noteOff);
Things that have to happen for
each event:
1 Make a message instance
ShortMessage first = new ShortMessage();
2Call setMessage() with the instructions
first.setMessage(192, 1, instrument, 0)
3Make a MidiEvent instance for the message
MidiEvent noteOn = new MidiEvent(first, 1);
4Add the event to the track
track.add(noteOn);
Let’s build a static utility method that
 The event ‘tick’ for
makes a message and returns a MidiEvent
 the for fou th
e r m a essage
 rguments
 should WHEN happen
 this message
public static MidiEvent makeEvent(int comd, int chan, int one, int two, int tick) {
MidiEvent event = null;
 whoo! A method with five parameters.
try {
ShortMessage a = new ShortMessage();
a.setMessage(comd, event = new MidiEvent(a, chan, tick);
 one, two);
 the make meth th
e od message parameters
 and the even
t, using
}catch(Exception e) { }
return event;
 return the event (a MidiEvent all
}
 loaded up with the messag
e)388
 chapter 12
getting gui
Example: how to use the new static
makeEvent() method
There’s no event handling or graphics here, just a sequence of 15
notes that go up the scale. The point of this code is simply to learn
how to use our new makeEvent() method. The code for the next
two versions is much smaller and simpler thanks to this method.
import javax.sound.midi.*;
 don’t for
get the im
port
public class MiniMusicPlayer1 {
public static void main(String[] args) {
try {
Sequencer sequencer = MidiSystem.getSequencer();
 make (and
 open) a
 sequencer
sequencer.open();
Sequence seq = new Sequence(Sequence.PPQ, 4);
 make a sequence
Track track = seq.createTrack();
 and a track
for (int i = 5; i < 61; i+= 4) {
 make a bunch of events to make the notes keep
going up (from piano note 5 to piano note 61)
} // track.add(makeEvent(128,1,i,100,i track.add(makeEvent(144,1,i,100,i));
 sequencer.setSequence(seq);
 end loop
 + 2));
 NOTE the MidiEvent message call track. our OFF and new These returned (128) ev mak
 en
t,
 eEve
nt() ar pa th
 e irs
 from NOTE en add method makeEvent()) ON the (144) result to mak
 and
 (the
 to
 e the
sequencer.setTempoInBPM(220);
 start it running
sequencer.start();
} catch (Exception ex) {ex.printStackTrace();}
} // close main
public static MidiEvent makeEvent(int comd, int chan, int one, int two, int tick) {
MidiEvent event = null;
try {
ShortMessage a = new ShortMessage();
a.setMessage(comd, chan, one, two);
event = new MidiEvent(a, tick);
}catch(Exception e) { }
return event;
}
} // close class
you are here�
 389
controller events
Version Two: registering and getting ControllerEvents
We need to listen for Contro
llerEvents,
so we implement the
 list
ene
r
 interface
import javax.sound.midi.*;
public class MiniMusicPlayer2 implements ControllerEventListener {
public static void main(String[] args) {
MiniMusicPlayer2 mini = new MiniMusicPlayer2();
mini.go();
public }
trySequencer sequencer.open();
 {
 void go()sequencer {
 = MidiSystem.getSequencer();
 The Register listener the We want event list AN of for on
 re D C ly gi ev ontrollerEvents an st one en ration ts int event, with array method the #127.
 representing
 sequen
 you takes want.
 cer.
 the
int[] eventsIWant = {127};
sequencer.addControllerEventListener(this, eventsIWant);
Sequence seq = new Sequence(Sequence.PPQ, 4);
Track track = seq.createTrack();
} //}close
 catch sequencer.start();
 for sequencer.setSequence(seq);
 sequencer.setTempoInBPM(220);
 } // track.add(makeEvent(144,1,i,100,i));
 track.add(makeEvent(128,1,i,100,i track.add(makeEvent(176,1,127,0,i));
 (int end (Exception loop
 i = 5; ex) i < 60; {ex.printStackTrace();}
 i+= 4) {
 + 2));
 will happens, the making for fire words, an ING! event type our Here’s fire event NOTE NOTE that OWN We is number this its at we’ll how ControllerEvent) put each WE sole ON. the ControllerE
ve
nt
 ON/OFF event we know it can #127. time purpose sam So pick inhappen when ab
 JUST e listen a up time.
 out events This note is the
 th fo it at so so with e r ). event is because beat (176 that the that NOTE (we Note played. an SAME can’t -- says will something we argument that ON OUR we can In do the listen
 event
 other
 tick insert
 NOTH-
 we’re
 event
 get
 event
 will
 for
 as
}
 public System.out.println(“la”);
 void controlChange(ShortMessage event) {
 The Event event, event listene we’ll
 hand
 print r
 le
 in
te
 r
 “la” m
 rf
 et
 ac
 to ho
 e)
 d
 .
 the E
 (f
 ach ro
 command-
 m
 time the we Controller-
 lin ge e. t
 the
public MidiEvent makeEvent(int comd, int chan, int one, int two, int tick) {
MidiEvent event = null;
try {
ShortMessage a = new ShortMessage();
a.setMessage(comd, chan, one, two);
event = new MidiEvent(a, tick);
Code that’s different from the previous
}catch(Exception e) { }
 version is highlighted in gray. (and we’re
return event;
 not running it all within main() this time)
}
} // close class
390
 chapter 12
Version Three: drawing graphics in time with the music
This final version builds on version two by adding the GUI parts. We build a
frame, add a drawing panel to it, and each time we get an event, we draw a
new rectangle and repaint the screen. The only other change from version
two is that the notes play randomly as opposed to simply moving up the
scale.
The most important change to the code (besides building a simple GUI)
is that we make the drawing panel implement the ControllerEventListener
rather than the program itself. So when the drawing panel (an inner class)
gets the event, it knows how to take care of itself by drawing the rectangle.
Complete code for this version is on the next page.
getting gui
The drawing panel inner class:
The drawing pane
l
 is a listener
class MyDrawPanel extends JPanel implements ControllerEventListener {
boolean msg = false;
 to We true set a only flag when to false, we get and an we’ll even
t.
 set it
public void controlChange(ShortMessage event) {
msg = true;
repaint();
 We got an event, so we set the flag to
}
 true and call repaint()
public void paintComponent(Graphics g) {
if (msg) {
 and We have we want to use toa paint flag ONLY becausewhe
n OTHER there’s thin
gs a ControllerEvent might trigger a
 repaint(),
Graphics2D g2 = (Graphics2D) g;
int r = (int) (Math.random() * 250);
int gr = (int) (Math.random() * 250);
int b = (int) (Math.random() * 250);
 The rest is code to generate
a random color and paint a
g.setColor(new Color(r,gr,b));
 semi-random rectangle.
int ht = (int) ((Math.random() * 120) + 10);
int width = (int) ((Math.random() * 120) + 10);
int x = (int) ((Math.random() * 40) + 10);
int y = (int) ((Math.random() * 40) + 10);
g.fillRect(x, y, width, ht);
msg = false;
} // close if
} // close method
} // close inner class
you are here�
 391
MiniMusicPlayer3 code
Sharpen your pencil
import javax.sound.midi.*;
 This is the complete code listing for Version
import java.io.*;
 Three. It builds directly on Version Two. Try
import javax.swing.*;
 to annotate it yourself, without looking at the
import java.awt.*;
 previous pages.
public class MiniMusicPlayer3 {
static JFrame f = new JFrame(“My First Music Video”);
static MyDrawPanel ml;
public static void main(String[] args) {
MiniMusicPlayer3 mini = new MiniMusicPlayer3();
mini.go();
} // close method
public void setUpGui() {
ml = new MyDrawPanel();
f.setContentPane(ml);
f.setBounds(30,30, 300,300);
f.setVisible(true);
} // close method
public void go() {
setUpGui();
try {
Sequencer sequencer = MidiSystem.getSequencer();
sequencer.open();
sequencer.addControllerEventListener(ml, new int[] {127});
Sequence seq = new Sequence(Sequence.PPQ, 4);
Track track = seq.createTrack();
int r = 0;
for (int i = 0; i < 60; i+= 4) {
r = (int) ((Math.random() * 50) + 1);
track.add(makeEvent(144,1,r,100,i));
track.add(makeEvent(176,1,127,0,i));
track.add(makeEvent(128,1,r,100,i + 2));
} // end loop
sequencer.setSequence(seq);
sequencer.start();
sequencer.setTempoInBPM(120);
} catch (Exception ex) {ex.printStackTrace();}
} // close method
392
 chapter 12
getting gui
public MidiEvent makeEvent(int comd, int chan, int one, int two, int tick) {
MidiEvent event = null;
try {
ShortMessage a = new ShortMessage();
a.setMessage(comd, chan, one, two);
event = new MidiEvent(a, tick);
}catch(Exception e) { }
return event;
} // close method
class MyDrawPanel extends JPanel implements ControllerEventListener {
boolean msg = false;
public void controlChange(ShortMessage event) {
msg = true;
repaint();
}
public void paintComponent(Graphics g) {
if (msg) {
int r = (int) (Math.random() * 250);
int gr = (int) (Math.random() * 250);
int b = (int) (Math.random() * 250);
g.setColor(new Color(r,gr,b));
int ht = (int) ((Math.random() * 120) + 10);
int width = (int) ((Math.random() * 120) + 10);
int x = (int) ((Math.random() * 40) + 10);
int y = (int) ((Math.random() * 40) + 10);
g.fillRect(x,y,ht, width);
msg = false;
} // close if
} // close method
} // close inner class
} // close class
you are here�
 393
exercise: Who Am I
Exercise
Who am
 I?
A bunch of Java hot-shots, in full costume, are playing the party game “Who
am I?” They give you a clue, and you try to guess who they are, based on
what they say. Assume they always tell the truth about themselves. If they
happen to say something that could be true for more than one guy, then
write down all for whom that sentence applies. Fill in the blanks next to the
sentence with the names of one or more attendees.
Tonight’s attendees:
Any of the charming personalities from this chapter just
might show up!
I got the whole GUI, in my hands.
Every event type has one of these.
The listener’s key method.
This method gives JFrame its size.
You add code to this method but never call it.
When the user actually does something, it’s an _____ .
Most of these are event sources.
I carry data back to the listener.
An addXxxListener( ) method says an object is an _____ .
How a listener signs up.
The method where all the graphics code goes.
I’m typically bound to an instance.
The ‘g’ in (Graphics g), is really of class.
The method that gets paintComponent( ) rolling.
The package where most of the Swingers reside.
394
 chapter 12
Exercise
importimportimportjavax.swing.*;
java.awt.event.*;
java.awt.*;
class InnerButton {
JFrame frame;
JButton b;
public static void main(String [] args) {
InnerButton gui = new InnerButton();
gui.go();
}
public void go() {
frame = new JFrame();
frame.setDefaultCloseOperation(

 JFrame.EXIT_ON_CLOSE);
b = new JButton(“A”);
b.addActionListener();
frame.getContentPane().add(

 BorderLayout.SOUTH, b);
frame.setSize(200,100);
frame.setVisible(true);
}
class BListener extends ActionListener {
public void actionPerformed(ActionEvent e) {
if (b.getText().equals(“A”)) {
b.setText(“B”);
} else {
b.setText(“A”);
}
}
}
}
getting gui
BE the compiler
The Java file on this page represents a
complete source file. Your job is to play
compiler and determine whether this file
will compile. If it won’t compile, how
would you fix it, and if it does
compile, what would it do?
you are here�
 395
puzzle: Pool Puzzle
Pool Puzzle
Your job is to take code snippets from
the pool and place them into the blank
lines in the code. You may use the
same snippet more than once, and
you won’t need to use all the snip-
pets. Your goal is to make a class
that will compile and run and
produce the output listed.
Output
The Amazing, Shrinking, Blue Rectangle.
This program will produce a blue
rectangle that will shrink and shrink and
disappear into a field of white.
Note: Each snippet
from the pool can be
used more than once!
import javax.swing.*;
import java.awt.*;
public class Animate {
int x = 1;
int y = 1;
public static void main (String[] args) {
Animate gui = new Animate ();
gui.go();
}
public void go() {
JFrame _________ = new JFrame();
frame.setDefaultCloseOperation(

 JFrame.EXIT_ON_CLOSE);
______________________________________;
_________.getContentPane().add(drawP);
__________________________;
_________.setVisible(true);
for (int i=0; i<124; _______________) {
_____________________;
_____________________;
try {
Thread.sleep(50);
} catch(Exception ex) { }
}
}
class MyDrawP extends JPanel {
public void paintComponent (Graphics
_________) {
__________________________________;
__________________________________;
__________________________________;
__________________________________;
}
}
}
g.fillRect(x,y,x-500,y-250)
i++
g.fillRect(x,y,500-x*2,250-y*2)
i++, y++
g.fillRect(500-x*2,250-y*2,x,y)
x++ g.fillRect(0,0,250,500)
 i++, y++, x++
y++ g.fillRect(0,0,500,250)
 Animate frame = new Animate()
MyDrawP drawP = new MyDrawP()
g.setColor(blue)
 g
drawP.paint() ContentPane drawP = new ContentPane()
g.setColor(white)
 draw
g.setColor(Color.blue)
 frame
 draw.repaint()
 drawP.setSize(500,270)
g.setColor(Color.white)
 panel
 drawP.repaint()
 frame.setSize(500,270)
panel.setSize(500,270)
396
 chapter 12
Exercise Solutions
Who am I?
I got the whole GUI, in my hands.
Every event type has one of these.
The listener’s key method.
This method gives JFrame its size.
You add code to this method but
never call it.
When the user actually does
something, it’s an ____
Most of these are event sources.
I carry data back to the listener.
An addXxxListener( ) method
says an object is an ___
How a listener signs up.
The method where all the
graphics code goes.
I’m typically bound to an instance.The ‘g’ in (Graphics g), is
really of this class.
The method that gets
paintComponent( ) rolling.
The package where most of the
Swingers reside.
JFrame
listener interface
actionPerformed( )
setSize( )
paintComponent( )
event
swing components
event object
event source
addActionListener( )
paintComponent( )
inner class
Graphics2D
repaint( )
javax.swing
BE the compiler
getting gui
import javax.swing.*;
import java.awt.event.*;
import java.awt.*;
class InnerButton {
JFrame frame;
JButton b;
Once this code
is fixed, it will
create a GUI with
a button that
toggles between
A and B when you
click it.
public static void main(String [] args) {
InnerButton gui = new InnerButton();
gui.go();
}
public void go() {
frame = new JFrame();
frame.setDefaultCloseOperation(

 JFrame.EXIT_ON_CLOSE);
The addActionListener( )
method takes a class that
implements the ActionLis-
tener interface
b = new JButton(“A”);
b.addActionListener( new BListener( ) );
frame.getContentPane().add(

 BorderLayout.SOUTH, b);
frame.setSize(200,100);
frame.setVisible(true);
}
class BListener implements ActionListener {
public void actionPerformed(ActionEvent e) {
if (b.getText().equals(“A”)) {
b.setText(“B”);
} else {
 ActionListener is an
interface, interfaces
b.setText(“A”);are implemented, not
}
 extended
}
}
}
you are here�
 397
puzzle answers
The Amazing, Shrinking, Blue
Rectangle.
398
 chapter 12
Pool Puzzle
import javax.swing.*;
import java.awt.*;
public class Animate {
int x = 1;
int y = 1;
public static void main (String[] args) {
Animate gui = new Animate ();
gui.go();
}
public void go() {
JFrame frame = new JFrame();
frame.setDefaultCloseOperation(

 JFrame.EXIT_ON_CLOSE);
MyDrawP drawP = new MyDrawP();
frame.getContentPane().add(drawP);
frame.setSize(500,270);
frame.setVisible(true);
for (int i = 0; i < 124; i++,y++,x++ ) {
x++;
drawP.repaint();
try {
Thread.sleep(50);
} catch(Exception ex) { }
}
}
class MyDrawP extends JPanel {
public void paintComponent(Graphics g ) {
g.setColor(Color.white);
g.fillRect(0,0,500,250);
g.setColor(Color.blue);
g.fillRect(x,y,500-x*2,250-y*2);
}
}
}
13 using swing
Work on Your
Swing
Why won’t
the ball go where I want
it to go? (like, smack in Suzy
Smith’s face) I’ve gotta learn
to control it.
Swing is easy. Unless you actually care where things end up on the screen. Swing code
looks easy, but then you compile it, run it, look at it and think, “hey, that’s not supposed to
go there.” The thing that makes it easy to code is the thing that makes it hard to control—the
Layout Manager . Layout Manager objects control the size and location of the widgets in a
Java GUI. They do a ton of work on your behalf, but you won’t always like the results. You want
two buttons to be the same size, but they aren’t. You want the text field to be three inches
long, but it’s nine. Or one. And under the label instead of next to it. But with a little work, you
can get layout managers to submit to your will. In this chapter, we’ll work on our Swing and in
addition to layout managers, we’ll learn more about widgets. We’ll make them, display them
(where we choose), and use them in a program. It’s not looking too good for Suzy.
this is a new chapter
 399
components and containers
Swing components
Component is the more correct term for what we’ve been calling a widget.
The things you put in a GUI. The things a user sees and interacts with. Text
fields, buttons, scrollable lists, radio buttons, etc. are all components. In
fact, they all extend javax.swing.JComponent.
Components can be nested
In Swing, virtually all components are capable of holding other
components. In other words, you can stick just about anything into anything
else. But most of the time, you’ll add user interactive components such as
buttons and lists into background components such as frames and panels.
Although it’s possible to put, say, a panel inside a button, that’s pretty
weird, and won’t win you any usability awards.
With the exception of JFrame, though, the distinction between interactive
components and background components is artificial. A JPanel, for
example, is usually used as a background for grouping other components,
but even a JPanel can be interactive. Just as with other components, you
can register for the JPanel’s events including mouse clicks and keystrokes.
Four steps to making a GUI (review)
1
 Make a window (a JFrame)
JFrame frame = new JFrame();
2
 Make a component (button, text field, etc.)
JButton button = new JButton(“click me”);
3
 Add the component to the frame
frame.getContentPane().add(BorderLayout.EAST, button);
4
 Display it (give it a size and make it visible)
frame.setSize(300,300);
frame.setVisible(true);
A widget is technically
a Swing Component.
Almost every thing
you can stick in a GUI
extends from javax.
swing.JComponent.
Put interactive components:
JButton
JCheckBox
JTextField
Into background components:
JFrame
JPanel
400
 chapter 13
using swing
Layout Managers
As a layout manager,
A layout manager is a Java object associated
with a particular component, almost always a
 I’m in charge of the size
background component. The layout manager
 and placement of your components.
controls the components contained within the
 In this GUI, I’m the one who decided
component the layout manager is associated
 how big these buttons should be, and
with. In other words, if a frame holds a panel,
where they are relative to each
and the panel holds a button, the panel’s layout
other and the frame.
manager controls the size and placement of
the button, while the frame’s layout manager
controls the size and placement of the
panel. The button, on the other hand,
doesn’t need a layout manager because the
button isn’t holding other components.
If a panel holds five things, even if those
five things each have their own layout
managers, the size and location of the five
things in the panel are all controlled by the
panel’s layout manager. If those five things,
in turn, hold other things, then those other
things are placed according to the layout
manager of the thing holding them.
When we say hold we really mean add as in, a
each myPanel.add(button);
 Layout panel added background to holds managers the a panel button come component using because in something several the can button flavors, have like:
 its was
 and
 own
 Panel contr of th
 B ols e ’s
three la
 the y
o
ut size but
 man a tons.
 n a d ger
 placement
 placement controls Panel A’s the layo
 of si Pane ut ze m an
 l anager
 B.
 d
button 1
layout manager. Layout managers have their
own policies to follow when building a layout.
 button 2
For example, one layout manager might insist
that all components in a panel must be the same
 button 3
size, arranged in a grid, while another layout
manager might let each component choose its
Panel B
own size, but stack them vertically. Here’s an
example of nested layouts:
JPanel panelA = new JPanel();
 Panel A
JPanel panelA.add(panelB);
 panelB.add(new panelB.add(new panelB.add(new panelB = JButton(“button JButton(“button JButton(“button new JPanel();
 1”));
 3”));
 2”));
 nested to manager of say Panel Panel control about A’s within A, controls layout the is and only those three does manager only one added not button
s.
 the level—Panel con
trol has things com
ponents.
 NOTH
ING Th
e
 add
ed anything
 A’s hie
rarchy
 layout
 directly
 to
you are here�
 401
layout managers
How does the layout manager decide?
Different layout managers have different policies for arranging
components (like, arrange in a grid, make them all the same size,
stack them vertically, etc.) but the components being layed out do
get at least some small say in the matter. In general, the process of
laying out a background component looks something like this:
A layout scenario:
1
 Make a panel and add three buttons to it.
2
 The panel’s layout manager asks each button how big
that button prefers to be.
Let’s see here... the
first button wants to be
30 pixels wide, and the text field
needs 50, and the frame is 200 pixels
wide and I’m supposed to arrange
everything vertically...
3
The panel’s layout manager uses its layout policies to decide
whether it should respect all, part, or none of the buttons’
preferences.
4
 Add the panel to a frame.
5
The frame’s layout manager asks the panel how big the panel
prefers to be.
6
The frame’s layout manager uses its layout policies to decide
whether it should respect all, part, or none of the panel’s
preferences.
Different layout managers have different policies
Some layout managers respect the size the component wants to
be. If the button wants to be 30 pixels by 50 pixels, that’s what the
layout manager allocates for that button. Other layout managers
respect only part of the component’s preferred size. If the button
wants to be 30 pixels by 50 pixels, it’ll be 30 pixels by however
wide the button’s background panel is. Still other layout managers
respect the preference of only the largest of the components
being layed out, and the rest of the components in that panel
are all made that same size. In some cases, the work of the layout
manager can get very complex, but most of the time you can
figure out what the layout manager will probably do, once you get
to know that layout manager’s policies.
402
 chapter 13
layout m
anager
The Big Three layout managers:
border, flow, and box.
BorderLayout
A BorderLayout manager divides a background
component into five regions. You can add only one
component per region to a background controlled
by a BorderLayout manager. Components laid out
by this manager usually don’t get to have their
preferred size. BorderLayout is the default layout
manager for a frame!
FlowLayout
A FlowLayout manager acts kind of like a word
processor, except with components instead of
words. Each component is the size it wants to be,
and they’re laid out left to right in the order that
they’re added, with “word-wrap” turned on. So
when a component won’t fit horizontally, it drops
to the next “line” in the layout. FlowLayout is the
default layout manager for a panel!
BoxLayout
A BoxLayout manager is like FlowLayout in that
each component gets to have its own size, and
the components are placed in the order in which
they’re added. But, unlike FlowLayout, a BoxLayout
manager can stack the components vertically (or
horizontally, but usually we’re just concerned with
vertically). It’s like a FlowLayout but instead of
having automatic ‘component wrapping’, you can
insert a sort of ‘component return key’ and force
the components to start a new line.
using swing
one compone
nt
per re
g
io
nto new components right, line when wrapp ad need de
 in
g
 d
 ed
 to
 left
 a
components to bottom, added on
e
 per top ‘line
’
you are here�
 403
border layout
BorderLayout cares
about five regions:
east, west, north,
south, and center
Let’s add a button to the east region:
import import javax.swing.*;
 java.awt.*;
 BorderLayout is in
 java.awt package
public class Button1 {
public static void main (String[] args) {
Button1 gui = new Button1();
gui.go();
}
public JFrame voidframe go() {
 = new JFrame();
 specify th
e region
JButton button = new JButton(“click me”);
frame.getContentPane().add(BorderLayout.EAST, button);
frame.setSize(200,200);
frame.setVisible(true);
}
}
br Brain
 ain barbell
 Barbell
How did the BorderLayout manager come up with
this size for the button?
What are the factors the layout manager has to
consider?
Why isn’t it wider or taller?
404
 chapter 13
using swing
Watch what happens when we give
the button more characters...
 We
 on the c
h
a
n
 bu
 g
e
d
 tton
 only the t
e
xt
public void go() {
JFrame frame = new JFrame();
JButton button = new JButton(“click like you mean it”);
frame.getContentPane().add(BorderLayout.EAST, button);
frame.setSize(200,200);
frame.setVisible(true);
}
First, I ask the
button for its
preferred size.
I have a lot of words
now, so I’d prefer to be
60 pixels wide and 25
pixels tall.
Bu
tton obje
 t
 cSince it’s in the east
region of a border layout,
I’ll respect its preferred width. But
I don’t care how tall it wants to be;
it’s gonna be as tall as the frame,
because that’s my policy.
Next time
I’m goin’ with flow
layout. Then I get
EVERYTHING I
want.
The its but pre but no
 f t t erred on height.
 gets
 width,
 Bu
tton obje
 t
 cyou are here�
 405
border layout
Let’s try a button in the NORTH region
public void go() {
JFrame frame = new JFrame();
JButton button = new JButton(“There is no spoon...”);
frame.getContentPane().add(BorderLayout.NORTH, button);
frame.setSize(200,200);
frame.setVisible(true);
}
 The wants b
u
t
 to t
 o
n
 be, is but as ta a
s ll wide as it
as
the fr
ame.
Now let’s make the button ask to be taller
How do we do that? The button is already as wide
as it can ever be—as wide as the frame. But we
can try to make it taller by giving it a bigger font.
public JFrame JButton Font void bigFont frame go() button {
 = = = new new new Font(“serif”, JFrame();
 JButton(“Click Font.BOLD, This!”);
28);
 A frame for bigge th
 r
 to e f
 button’s o
 allocate n
t
 w
ill heig
 forc mor ht.
 e e s t pace
 he
button.setFont(bigFont);
frame.getContentPane().add(BorderLayout.NORTH, button);
frame.setSize(200,200);
frame.setVisible(true);
}
 The the region the width button button’s stretche stay is ne
 ta s d w ller. the to preferred accomodate
 The same, north
 but height.
 now
406
 chapter 13
using swing
I think I’m getting it... if I’m in east or
west, I get my preferred width but the
height is up to the layout manager. And
if I’m in north or south, it’s just the
opposite—I get my preferred height, but
not width.
But what happens
in the center region?
Bu
tton obje
 t
 cThe center region gets whatever’s left!
(except in one special case we’ll look at later)
public void go() {
JFrame frame = new JFrame();
JButton east = new JButton(“East”);
JButton west = new JButton(“West”);
JButton north = new JButton(“North”);
JButton south = new JButton(“South”);
JButton center = new JButton(“Center”);
frame.getContentPane().add(BorderLayout.EAST, east);
frame.getContentPane().add(BorderLayout.WEST, west);
frame.getContentPane().add(BorderLayout.NORTH, north);
frame.getContentPane().add(BorderLayout.SOUTH, south);
frame.getContentPane().add(BorderLayout.CENTER, center);
frame.setSize(300,300);
frame.setVisible(true);
}
(300 based whatever Components
 x on 300 the spac in in fram e th
 is
 th e left is e center dimensions
 code).
 over,
 get
Components west get th
ei in r th preferred e east and
 width.
Components south get th
 in eir th preferred
 e north and
height.
300 pixels
3 0 0 x e l s
 i p south would won’t in the goes in When the the frame, regions all be be east you north the if as pu th
e
 and tall so way wer t or the so no
rt
 as w e so
 ac m
ething
 est
 empty.
 things
 they
 uth, ross
 h and
 it
you are here�
 407
flow layout
FlowLayout cares
about the flow of the
components:
left to right, top to bottom, in
the order they were added.
Let’s add a panel to the east region:
A JPanel’s layout manager is FlowLayout, by default. When we add
a panel to a frame, the size and placement of the panel is still
under the BorderLayout manager’s control. But anything inside the
panel (in other words, components added to the panel by calling
panel.add(aComponent)) are under the panel’s FlowLayout
manager’s control. We’ll start by putting an empty panel in the frame’s
east region, and on the next pages we’ll add things to the panel.
import javax.swing.*;
import java.awt.*;
public class Panel1 {
public static void main (String[] args) {
Panel1 gui = new Panel1();
gui.go();
}
The in width it, panel so in it the
 doe doe sn east sn ’t ’t have region.
 ask anything
 for much
public void go() {
JFrame frame = new JFrame();
 Make the panel gray so we can see
JPanel panel = new JPanel();
 where it is on the frame.
panel.setBackground(Color.darkGray);
frame.getContentPane().add(BorderLayout.EAST, panel);
frame.setSize(200,200);
frame.setVisible(true);
}
}
408
 chapter 13
using swing
Let’s add a button to the panel
public void go() {
JFrame frame = new JFrame();
panel.setBackground(Color.darkGray);
 JButton JPanel panel button = = new new JPanel();
 JButton(“shock me”);
 Add panel (flow) layout
 th
 to
 e
 controls m
 bu
 th
 an
 tt
 e
 ag
 fr
 on
 er
am
 to
 th
 (b
 e.
 or
 e th
 T
 button, d
 e er) he
 panel panel’s cont
 and and ro
layo ls
 th ad th
e e ut
 d fr
 the
 panel.
 man am
e’s
 ager
panel.add(button);
frame.getContentPane().add(BorderLayout.EAST, panel);
frame.setSize(250,200);
frame.setVisible(true);
}
The panel expanded !
panel
 panel
 And preferred the button
 size in go bo tth its
(not button uses dimensions, the flow is frame).
 part layout, beca of us an
d
 e the the
 th
 panel
 panel
 e
Ok... I need to
know how big the
panel wants to be...
I have a button now,
so my layout manager’s
gonna have to figure out
how big I need to be...
I need
to know how big the
button wants to
be...
Based on my font size
and the number of
characters, I want to be 70
pixels wide and 20 pixels tall.
controls
The frame’s
BorderLayout manager
Pa
nel objec
 t
controls
The panel’s
FlowLayout manager
Bu
tton obje
 t
 cyou are here�
409
flow layout
What happens if we add TWO buttons
to the panel?
public void go() {
JFrame frame = new JFrame();
JPanel panel = new JPanel();
panel.setBackground(Color.darkGray);
 JButton button = new JButton(“shock me”);
 make TW
 O
 buttons
JButton buttonTwo = new JButton(“bliss”);
panel.add(button);
 panel.add(buttonTwo);
 add BOTH to
 the panel
frame.getContentPane().add(BorderLayout.EAST, panel);
frame.setSize(250,200);
frame.setVisible(true);
}
what we wanted:
We stacke
 wan d t on the top butto
 of ns
 each
other
what we got:
by The fit side.
 both panel but
t
 exp
anded o
ns side
 to
Sharpen your pencil
If the code above were modified to the code below,
what would the GUI look like?
JButton button = new JButton(“shock me”);
JButton buttonTwo = new JButton(“bliss”);
JButton buttonThree = new JButton(“huh?”);
panel.add(button);
panel.add(buttonTwo);
panel.add(buttonThree);
410
 chapter 13
notice the layout needs ‘sho th (a
 w ck orks. at nd me’ the no The button... more).
 ‘b
li
button
 ss
’
 b
u
 tha t
t
 g o
 t e n ts ’s is how
 just smalle f
 what lo r w
 than
 it
Draw what you
think the GUI would
look like if you ran
the code to the left.
(Then try it!)
BoxLayout to the rescue!
It keeps components
stacked, even if there’s room
to put them side by side.
using swing
Unlike FlowLayout, BoxLayout can force a
‘new line’ to make the components wrap to
the next line, even if there’s room for them
to fit horizontally.
But now you’ll have to change the panel’s layout manager from the
default FlowLayout to BoxLayout.
public panel.setBackground(Color.darkGray);
 JPanel JFrame void frame panel go() = = {
 new new JFrame();
 JPanel();
 Change instance t
h
 o
 e
 f la
 BoxLayout.
 y
 o
u
t manager
 to be a
 new
panel.setLayout(new BoxLayout(panel, BoxLayout.Y_AXIS));
panel.add(buttonTwo);
 panel.add(button);
 JButton JButton buttonTwo button = new = new JButton(“shock JButton(“bliss”);
 me”);
 vertical and The the which component BoxLayout
 stack).
 axis to it co
 s
 us la
 nstructor e ying (weout use needs (i.e., Y_AXIS
 the to
 know
 panel)
 for a
frame.getContentPane().add(BorderLayout.EAST, panel);
frame.setSize(250,200);
frame.setVisible(true);
}
Notice because horizonta it button, needed how it ‘s
 lly. doesn’t hock enough the So p
 me’.
 a
 the n
 room need e
l
 panel is
 n
 to for a
r
told rower fit on
ly both
 the the aga frame
 b in largest
 uttons
 ,
you are here�
 411
layout managers
Dumb there are Questions
 no
Q:
 How come you can’t add directly to a frame the way
you can to a panel?
A:
 A JFrame is special because it’s where the rubber
meets the road in making something appear on the screen.
While all your Swing components are pure Java, a JFrame
has to connect to the underlying OS in order to access the
display. Think of the content pane as a 100% pure Java layer
that sits on top of the JFrame. Or think of it as though JFrame
is the window frame and the content pane is the... glass. You
know, the window pane. And you can even swap the content
pane with your own JPanel, to make your JPanel the frame’s
content pane, using,
myFrame.setContentPane(myPanel);
Q:
 Can I change the layout manager of the frame?
What if I want the frame to use flow instead of border?
A:
 The easiest way to do this is to make a panel, build
the GUI the way you want in the panel, and then make that
panel the frame’s content pane using the code in the previ-
ous answer (rather than using the default content pane).
Q:
 What if I want a different preferred size? Is there a
setSize() method for components?
A:
 Yes, there is a setSize(), but the layout managers will
ignore it. There’s a distinction between the preferred size of
the component and the size you want it to be. The preferred
size is based on the size the component actually needs
(the component makes that decision for itself ). The layout
manager calls the component’s getPreferredSize() method,
and that method doesn’t care if you’ve previously called
setSize() on the component.
Q:
Can’t I just put things where I want them? Can I turn
the layout managers off?
A:
 Yep. On a component by component basis, you can call
setLayout(null) and then it’s up to you to hard-code
the exact screen locations and dimensions. In the long run,
though, it’s almost always easier to use layout managers.
412
 chapter 13
��
�
�
��
�
�
�
�
�
BULLET POINTS
Layout managers control the size and location of
components nested within other components.
When you add a component to another component
(sometimes referred to as a background component,
but that’s not a technical distinction), the added
component is controlled by the layout manager of the
background component.
A layout manager asks components for their
preferred size, before making a decision about
the layout. Depending on the layout manager’s
policies, it might respect all, some, or none of the
component’s wishes.
The BorderLayout manager lets you add a
component to one of five regions. You must specify
the region when you add the component, using the
following syntax:
add(BorderLayout.EAST, panel);
With BorderLayout, components in the north and
south get their preferred height, but not width.
Components in the east and west get their preferred
width, but not height. The component in the center
gets whatever is left over (unless you use pack()).
The pack() method is like shrink-wrap for the
components; it uses the full preferred size of the
center component, then determines the size of the
frame using the center as a starting point, building
the rest based on what’s in the other regions.
FlowLayout places components left to right, top to
bottom, in the order they were added, wrapping to a
new line of components only when the components
won’t fit horizontally.
FlowLayout gives components their preferred size in
both dimensions.
BoxLayout lets you align components stacked
vertically, even if they could fit side-by-side. Like
FlowLayout, BoxLayout uses the preferred size of
the component in both dimensions.
BorderLayout is the default layout manager for a
frame; FlowLayout is the default for a panel.
If you want a panel to use something other than flow,
you have to call setLayout() on the panel.
Playing with Swing components
You’ve learned the basics of layout managers, so now let’s try out a
few of the most common components: a text field, scrolling text area,
checkbox, and list. We won’t show you the whole darn API for each of
these, just a few highlights to get you started.
JLabe
l
JTextField
JTextField
using swing
Constructors
 JTextField field = new JTextField(20);
 20 This the means tex
 def t infield.
 2 es 0
 the c
o
lu
 preferred m
n
s,
 n
ot 20 widt
 pix
 h els.
 of
JTextField field = new JTextField(“Your name”);
How to use it
1
 Get text out of it
System.out.println(field.getText());
2
3Put text in it
field.setText(“whatever”);
field.setText(““);
 This clears the f
ield
Get presses an ActionEvent return or enter
 when the user
 user really You can presses want also to a register
 key. hear ab for out key it every events time if you
 the
field.addActionListener(myActionListener);
4Select/Highlight the text in the field
field.selectAll();
5
Put the cursor back in the field (so the user
can just start typing)
field.requestFocus();
you are here�
 413
text area
JTextArea
Unlike JTextField, JTextArea can have more than one line of text. It
takes a little configuration to make one, because it doesn’t come out of
the box with scroll bars or line wrapping. To make a JTextArea scroll, you
have to stick it in a ScrollPane. A ScrollPane is an object that really loves
Constructor
 to scroll, and will take care of the text area’s 10
 scrolling m
e
a
n
 20 s
 10
 needs.
 means line
s
 (s
 2
0 e
t
s
 columns t
h
e
 preferred (s
et
s
 t
h
e
 h
eight)
 p
r
e
f
 e
rred width)
JTextArea text = new JTextArea(10,20);
How 1 Make to use it have it
 a vertical scrollbar only
 Make text a
 a r J ea ScrollPa
 that it’s n
e
 a
 going
 n
d
 g
ive to it scro
 th ll e for.
JScrollPane text.setLineWrap(true);
 scroller = new JScrollPane(text);
 Turn on line wrapping
 Tell a vertical the scroll scrollbar pane to
 use only
scroller.setVerticalScrollBarPolicy(ScrollPaneConstants.VERTICAL_SCROLLBAR_ALWAYS);
scroller.setHorizontalScrollBarPolicy(ScrollPaneConstants.HORIZONTAL_SCROLLBAR_NEVER);
2 Replace panel.add(scroller);
 the text that’s in it
 don’t scroll Important!!
 add panethe co Y ns ou te tr give xt uctor), area the then directly
 text add area to the to the the scroll panel! sc
roll pa
ne pane to the (through panel. the
 You
text.setText(“Not all who are lost are wandering”);
3Append to the text that’s in it
text.append(“button clicked”);
4Select/Highlight the text in the field
text.selectAll();
5
 Put the cursor back in the field (so the user
can just start typing)
text.requestFocus();
414
 chapter 13
JTextArea example
using swing
importimportimportjavax.swing.*;
java.awt.*;
java.awt.event.*;
public class TextArea1 implements ActionListener {
JTextArea text;
public static void main (String[] args) {
TextArea1 gui = new TextArea1();
gui.go();
}
}
public void go() {
JFrame frame = new JFrame();
JPanel panel = new JPanel();
JButton button = new JButton(“Just Click It”);
button.addActionListener(this);
text = new JTextArea(10,20);
text.setLineWrap(true);
JScrollPane scroller = new JScrollPane(text);
scroller.setVerticalScrollBarPolicy(ScrollPaneConstants.VERTICAL_SCROLLBAR_ALWAYS);
scroller.setHorizontalScrollBarPolicy(ScrollPaneConstants.HORIZONTAL_SCROLLBAR_NEVER);
panel.add(scroller);
frame.getContentPane().add(BorderLayout.CENTER, panel);
frame.getContentPane().add(BorderLayout.SOUTH, button);
frame.setSize(350,300);
frame.setVisible(true);
}
public void actionPerformed(ActionEvent ev) {
text.append(“button clicked \n ”);
}
clicked. separate Insert a Otherwise, new line each
 line so ti
me
 they th
e’ll the words run button together.
 go on is
 a
you are here�
 415
check box
JCheckBox
Constructor
JCheckBox check = new JCheckBox(“Goes to 11”);
How to use it
1
 Listen for an item event (when it’s selected or deselected)
check.addItemListener(this);
2
 Handle the event (and find out whether or not it’s selected)
public void itemStateChanged(ItemEvent ev) {
String onOrOff = “off”;
if (check.isSelected()) onOrOff = “on”;
System.out.println(“Check box is “ + onOrOff);
}
3Select or deselect it in code
check.setSelected(true);
check.setSelected(false);
416
 chapter 13
Dumb there are Questions
 no
Q:
 Aren’t the layout manag-
ers just more trouble than they’re
worth? If I have to go to all this
trouble, I might as well just hard-
code the size and coordinates for
where everything should go.
A:
 Getting the exact layout
you want from a layout man-
ager can be a challenge. But think
about what the layout manager
is really doing for you. Even the
seemingly simple task of figuring
out where things should go on
the screen can be complex. For
example, the layout manager takes
care of keeping your components
from overlapping one another.
In other words, it knows how to
manage the spacing between
components (and between the
edge of the frame). Sure you can
do that yourself, but what happens
if you want components to be
very tightly packed? You might get
them placed just right, by hand,
but that’s only good for your JVM!
Why? Because the components
can be slightly different from
platform to platform, especially if
they use the underlying platform’s
native ‘look and feel’. Subtle things
like the bevel of the buttons can
be different in such a way that
components that line up neatly
on one platform suddenly squish
together on another.
And we’re still not at the really Big
Thing that layout managers do.
Think about what happens when
the user resizes the window! Or
your GUI is dynamic, where com-
ponents come and go. If you had
to keep track of re-laying out all
the components every time there’s
a change in the size or contents of
a background component...yikes!
JList
using swing
Constructor
 String JList type. They constructor
 representati don’t ha takes on ve will toan be appear array Strings, in of th
e but any list.
 a
 object
String [] listEntries = {“alpha”, “beta”, “gamma”, “delta”,

 “epsilon”, “zeta”, “eta”, “theta “};
JList list = new JList(listEntries);
How 1
 Make to use it have it
 a vertical scrollbar
 This JScrollPan scroll is pane just e
 lik (N
 (a
 e nd OT w
it
 give the h
 J
 it T
 list) e
x
 the t
A
 to re
 list), a the -- then pan
 you e
l.
 a
 mak dd
 e the
 a
JScrollPane scroller = new JScrollPane(list);
scroller.setVerticalScrollBarPolicy(ScrollPaneConstants.VERTICAL_SCROLLBAR_ALWAYS);
scroller.setHorizontalScrollBarPolicy(ScrollPaneConstants.HORIZONTAL_SCROLLBAR_NEVER);
panel.add(scroller);
2 Set the number of lines to show before scrolling
list.setVisibleRowCount(4);
3
Restrict the user to selecting only ONE thing at a time
list.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);
4
5
Register for list selection events
list.addListSelectionListener(this);
 You’ll get the event
 TWICE if you don’
t
Handle events (find out which thing in the list was selected)
 put in this if test.
public void valueChanged(ListSelectionEvent lse) {
if( !lse.getValueIsAdjusting()) {
String selection = (String) list.getSelectedValue();
 getSelectedValue() actuall
 y
}
 }
 System.out.println(selection);
 limited returns to an only Object. String A list objects.
 isn’t
you are here�
 417
Code Kitchen
Code Kitchen
418
This part’s optional. We’re making the full BeatBox, GUI
and all. In the Saving Objects chapter, we’ll learn how to
save and restore drum patterns. Finally, in the networking
chapter (Make a Connection), we’ll turn the BeatBox into a
working chat client.
chapter 13
Making the BeatBox
This is the full code listing for this version of the BeatBox, with buttons for starting,
stopping, and changing the tempo. The code listing is complete, and fully-
annotated, but here’s the overview:
using swing
1
2
3
Build a GUI that has 256 checkboxes (JCheckBox) that start out
unchecked, 16 labels (JLabel) for the instrument names, and four
buttons.
Register an ActionListener for each of the four buttons. We don’t
need listeners for the individual checkboxes, because we aren’t
trying to change the pattern sound dynamically (i.e. as soon as the
user checks a box). Instead, we wait until the user hits the ‘start’
button, and then walk through all 256 checkboxes to get their state
and make a MIDI track.
Set-up the MIDI system (you’ve done this before) including getting
a Sequencer, making a Sequence, and creating a track. We are using
a sequencer method that’s new to Java 5.0, setLoopCount( ). This
method allows you to specify how many times you want a sequence
to loop. We’re also using the sequence’s tempo factor to adjust the
tempo up or down, and maintain the new tempo from one iteration of
the loop to the next.
4
When the user hits ‘start’, the real action begins. The event-handling
method for the ‘start’ button calls the buildTrackAndStart() method.
In that method, we walk through all 256 checkboxes (one row at
a time, a single instrument across all 16 beats) to get their state,
then use the information to build a MIDI track (using the handy
makeEvent() method we used in the previous chapter). Once the track
is built, we start the sequencer, which keeps playing (because we’re
looping it) until the user hits ‘stop’.
you are here�
 419
BeatBox code
import java.awt.*;
import javax.swing.*;
import javax.sound.midi.*;
import java.util.*;
import java.awt.event.*;
public class BeatBox {
JPanel mainPanel;
 We store the checkboxes in an
 ArrayList
ArrayList<JCheckBox> checkboxList;
Sequencer sequencer;
Sequence sequence;
Track JFrametrack;
 theFrame;
 These array, are for the building names the of GUI the labels instruments, (on each as row)
 a String
String[] instrumentNames = {“Bass Drum”, “Closed Hi-Hat”,
“Open Hi-Hat”,”Acoustic Snare”, “Crash Cymbal”, “Hand Clap”,
“High Tom”, “Hi Bongo”, “Maracas”, “Whistle”, “Low Conga”,
“Cowbell”, “Vibraslap”, “Low-mid Tom”, “High Agogo”,
“Open Hi Conga”};
int[] instruments = {35,42,46,38,49,39,50,60,70,72,64,56,58,47,67,63};
These represent the actual drum ‘keys’.
public static void main (String[] args) {
 The drum channel is like a piano, exc
 ept}
 new BeatBox().buildGUI();
 drum, So each the ‘key’ 42 number is onClosed the ‘35’ piano is Hi-Hat, the is a key
 different etc for
 .
 the dru
m.
 Bass
public void buildGUI() {
theFrame = new JFrame(“Cyber BeatBox”);
theFrame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
BorderLayout layout = new BorderLayout();
JPanel background = new JPanel(layout);
background.setBorder(BorderFactory.createEmptyBorder(10,10,10,10));
checkboxList start.addActionListener(new JButton Box buttonBox start== =new new newArrayList<JCheckBox>();
 JButton(“Start”);
 Box(BoxLayout.Y_AXIS);
 MyStartListener());
 where Purely between An ‘empty the aesthetic. the componen border edges ’ giv
 of
 tses
 are the us placed.
 panel a margin
 and
buttonBox.add(start);
JButton stop = new JButton(“Stop”);
stop.addActionListener(new MyStopListener());
JButton buttonBox.add(stop);
 upTempo = new JButton(“Tempo Up”);
 Nothing code. You’ve special seen here
 most , just of
 lots it before.
 of GUI
upTempo.addActionListener(new MyUpTempoListener());
buttonBox.add(upTempo);
JButton downTempo = new JButton(“Tempo Down”);
420
 chapter 13
using swing
downTempo.addActionListener(new MyDownTempoListener());
buttonBox.add(downTempo);
Box nameBox = new Box(BoxLayout.Y_AXIS);
for (int i = 0; i < 16; i++) {
nameBox.add(new Label(instrumentNames[i]));
}
background.add(BorderLayout.EAST, background.add(BorderLayout.WEST, buttonBox);
 nameBox);
 Still Nothing more remarkab
 GUI set le. -uptheFrame.getContentPane().add(background);
GridLayout grid = new GridLayout(16,16);
grid.setVgap(1);
grid.setHgap(2);
mainPanel = new JPanel(grid);
background.add(BorderLayout.CENTER, mainPanel);
for (int i = 0; i < 256; i++) {
JCheckBox c = new JCheckBox();
c.setSelected(false);
} //checkboxList.add(c);
 mainPanel.add(c);
 end loop
 the ‘false’ add Make GUI them (so the panel.
 they to checkboxe
 the aren’t ArrayList s, ch set eck them ed) AND and
 to
 to
setUpMidi();
theFrame.setBounds(50,50,300,300);
theFrame.pack();
theFrame.setVisible(true);
} // close method
co
de.
public void setUpMidi() {
try {
sequencer = MidiSystem.getSequencer();
sequencer.open();
sequence = new Sequence(Sequence.PPQ,4);
track = sequence.createTrack();
sequencer.setTempoInBPM(120);
} catch(Exception e) {e.printStackTrace();}
} // close method
and The getting the usual the Track. MIDI Sequencer, Again, set-up nothing
 the stuff
 Sequence,
 for
 special.
you are here�
 421
BeatBox code
This turn and public int[] add is checkbox where void them trackList it buildTrackAndStart() state to all the
 ha = in pp Track.
 to null;
 ens! MIDI Where events,
 we
 {
 We’ll one supposed will play instru be make on th t
 to m e hat a en key. 16 play t, -eleme
 beat, across If on that put that n
t
 all a
 in instrum r
 beat, 16 r
 a a
y
 bea ze
 t
 ro.
 t o
 t ent h s. hold e If value is the
 NOT the at in
 va st supposed
 that lu rument es for
 elem t is
 e o n
t
sequence.deleteTrack(track);
 track = sequence.createTrack();
 get rid of the old track,
 make a fresh one.
for (int i = 0; i < 16; i++) {
 do this for each of the
 16 ROWS
 (i.e. Bass, Congo, etc.)
trackList = new int[16];
int key = instruments[i];
 Set actual is (B
 the as
s,
 MIDI ‘key’. Hi
-H
 numbers at
 that ,
 etc. repr
 for
 Th
 esents e
 each instrum
 which instrument.)
 en
ts
 instr ar
ra
y ument holds this
 the
for (int j = 0; j < 16; j++ ) {
 Do this for each of the
 BEATS for this row
JCheckBox jc = checkboxList.get(j + 16*i);
if ( jc.isSelected()) {
} // }
 } close else trackList[j] trackList[j] {
 inner loop
 = = key;
 0;
 Is NOT the represents the key supposed checkbox value this in to beat). this at play this slot at Otherwise, beat in this the selected? beat, array the soinstrument (the set If yes, it slot to put
 that
 zero.
 is
track.add(makeEvent(176,1,127,0,16));
 makeTracks(trackList);
 For make this events instrument, and
 add
 and the
for m to all the 16 track.
 beats,
} // close outer
We always want to make sure that there IS an event
 at
track.add(makeEvent(192,9,1,0,15));
 beat 16 (it goes 0 to
 15).
 Other
wise,
 the
 BeatB
ox
 might
try {
 not go the full 16 beats before it starts over.
sequencer.setSequence(sequence);
 Lets you specify the number of
sequencer.setLoopCount(sequencer.LOOP_CONTINUOUSLY);
 sequencer.start();
 sequencer.setTempoInBPM(120);
 loop continuous iterations, looping.
 or in this case,
} catch(Exception e) {e.printStackTrace();}
} // close buildTrackAndStart method
NOW PLAY THE THING!!
public class MyStartListener implements ActionListener {
public void actionPerformed(ActionEvent a) {
buildTrackAndStart();
 First of the inner classes,
}
 listeners for the button
s.
} // close inner class
 Nothing special here.422
 chapter 13
public class MyStopListener implements ActionListener {
public void actionPerformed(ActionEvent a) {
sequencer.stop();
}
} // close inner class
public class MyUpTempoListener implements ActionListener {
public void actionPerformed(ActionEvent a) {
float tempoFactor = sequencer.getTempoFactor();
sequencer.setTempoFactor((float)(tempoFactor * 1.03));
}
} // close inner class
public class MyDownTempoListener implements ActionListener {
public void actionPerformed(ActionEvent a) {
float tempoFactor = sequencer.getTempoFactor();
sequencer.setTempoFactor((float)(tempoFactor * .97));
}
} // close inner class
using swing
The other inner class
listeners for the buttons
The Tempo Factor scales
the sequencer’s tempo by
the factor provided. The
default is 1.0, so we’re
adjusting +/- 3% per
click.
public forint void (int key i makeTracks(int[] = = list[i];
 0; i < 16; i++) list) {
 {
 This all drum, the the Otherwise, 16 key instrument makes beats. and of each events make that So it index isn’t instr an
 for mig event supposed ht
 ument in one th
 get instrument and e , array an
 or to add int
 a play zero. [
 will it ]
 at at to for
 hold
 If a that the tim
 th
 it’s eit
 e
 track.
 e, beat.
 a her
 Bass
 for
 zero,
if (key != 0) {
track.add(makeEvent(144,9,key, 100, i));
 Make the NOTE ON and
track.add(makeEvent(128,9,key, 100, i+1));
 NOTE OFF events, and}
 add them to the Track
.
}
}
public MidiEvent makeEvent(int comd, int chan, int one, int two, int tick) {
MidiEvent event = null;
try {
ShortMessage a = new ShortMessage();
} catch(Exception event a.setMessage(comd, = new MidiEvent(a, e) {e.printStackTrace(); chan, one, tick);
 two);
 }
 This chapter’s is the CodeKit
 utility
 chen. method
 Nothing from ne
w.
 last
return event;
}
} // close class
you are here�
 423
exercise: Which Layout?
3
Exercise
Which code goes with
which layout?
Five of the six screens below were made from one
of the code fragments on the opposite page. Match
each of the five code fragments with the layout that
fragment would produce.
1
5
2
6
?
4
424
 chapter 13
using swing
Code Fragments
D
 JFrame frame = new JFrame();
JPanel panel = new JPanel();
panel.setBackground(Color.darkGray);
JButton button = new JButton(“tesuji”);
JButton buttonTwo = new JButton(“watari”);
frame.getContentPane().add(BorderLayout.NORTH,panel);
panel.add(buttonTwo);
frame.getContentPane().add(BorderLayout.CENTER,button);
JFrame frame = new JFrame();
B
 JPanel panel = new JPanel();
panel.setBackground(Color.darkGray);
JButton button = new JButton(“tesuji”);
JButton buttonTwo = new JButton(“watari”);
panel.add(buttonTwo);
frame.getContentPane().add(BorderLayout.CENTER,button);
frame.getContentPane().add(BorderLayout.EAST, panel);
C
 JFrame frame = new JFrame();
JPanel panel = new JPanel();
panel.setBackground(Color.darkGray);
JButton button = new JButton(“tesuji”);
JButton buttonTwo = new JButton(“watari”);
panel.add(buttonTwo);
frame.getContentPane().add(BorderLayout.CENTER,button);
A
 JFrame frame = new JFrame();
JPanel panel = new JPanel();
panel.setBackground(Color.darkGray);
JButton button = new JButton(“tesuji”);
JButton buttonTwo = new JButton(“watari”);
panel.add(button);
frame.getContentPane().add(BorderLayout.NORTH,buttonTwo);
frame.getContentPane().add(BorderLayout.EAST, panel);
E
 JFrame frame = new JFrame();
JPanel panel = new JPanel();
panel.setBackground(Color.darkGray);
JButton button = new JButton(“tesuji”);
JButton buttonTwo = new JButton(“watari”);
frame.getContentPane().add(BorderLayout.SOUTH,panel);
panel.add(buttonTwo);
frame.getContentPane().add(BorderLayout.NORTH,button);
you are here�
 425
puzzle: crossword
GUI-Cross 7.0
1
 2
 3
4
5
 6
 7
8
 9
 10
11
 12
 You can do it.
13
14
 15
16
17
 18
 19
20
21
 22
23
26
24
25
Across
1. Artist’s sandbox
4. Border’s catchall
5. Java look
9. Generic waiter
11. A happening
12. Apply a widget
15. JPanel’s default
16. Polymorphic test
17. Shake it baby
21. Lots to say
23. Choose many
25. Button’s pal
26. Home of
actionPerformed
Down
2. Swing’s dad
3. Frame’s purview
5. Help’s home
6. More fun than text
7. Component slang
8. Romulin command
9. Arrange
10. Border’s top
13. Manager’s rules
14. Source’s behavior
15. Border by default
18. User’s behavior
19. Inner’s squeeze
20. Backstage widget
22. Mac look
24. Border’s right
426
 chapter 13
Exercise1
2
3
4
6
Solutions
using swing
JFrame frame = new JFrame();
C JPanel panel = new JPanel();
panel.setBackground(Color.darkGray);
JButton button = new JButton(“tesuji”);
JButton buttonTwo = new JButton(“watari”);
panel.add(buttonTwo);
frame.getContentPane().add(BorderLayout.CENTER,button);
JFrame frame = new JFrame();
D JPanel panel = new JPanel();
panel.setBackground(Color.darkGray);
JButton button = new JButton(“tesuji”);
JButton buttonTwo = new JButton(“watari”);
frame.getContentPane().add(BorderLayout.NORTH,panel);
panel.add(buttonTwo);
frame.getContentPane().add(BorderLayout.CENTER,button);
E JFrame frame = new JFrame();
JPanel panel = new JPanel();
panel.setBackground(Color.darkGray);
JButton button = new JButton(“tesuji”);
JButton buttonTwo = new JButton(“watari”);
frame.getContentPane().add(BorderLayout.SOUTH,panel);
panel.add(buttonTwo);
frame.getContentPane().add(BorderLayout.NORTH,button);
JFrame frame = new JFrame();
A JPanel panel = new JPanel();
panel.setBackground(Color.darkGray);
JButton button = new JButton(“tesuji”);
JButton buttonTwo = new JButton(“watari”);
panel.add(button);
frame.getContentPane().add(BorderLayout.NORTH,buttonTwo);
frame.getContentPane().add(BorderLayout.EAST, panel);
JFrame frame = new JFrame();
BJPanel panel = new JPanel();
panel.setBackground(Color.darkGray);
JButton button = new JButton(“tesuji”);
JButton buttonTwo = new JButton(“watari”);
panel.add(buttonTwo);
frame.getContentPane().add(BorderLayout.CENTER,button);
frame.getContentPane().add(BorderLayout.EAST, panel);
you are here�
 427
puzzle answers
Puzzle Answers
GUI-Cross 7.0
1
 D R A W P 2
 A N E L
 3
 S
W
 4
 C E N T E R
5
 M E T A L
 T
 6
 G W
 7
8
 S
 E
 9
 L I S T E 10
 N E R
 I
11
 E V E N T
 A
 I
 O 12
 A D D
T
 U
 13
 P
 Y
 Z
 R
 P
 G
V
 14
 C
 15
 F L O W O
 E
 T
 H E
16
 I S A
 R
 L
 U
 H
 I
 T
S
 L
 17
 A N I M 18
 A T I 19
 O N
 C
I
 L
 M C
 C
 U
 20
 P
B
 B
 E
 Y
 21
 T E X T A R E 22
 A
 A
L
 A
 I
 E
 Q
 N
E
 23
 C H 24
 E C K B O X
 R
 U
 E
K
 A
 N
 25
 L A B E L
S
26
 A C T I O N L I S T E N E R
428
 chapter 13
14 serialization and file I/O
Saving Objects
If I have to read
one more file full of
data, I think I’ll have to kill him. He
knows I can save whole objects, but
does he let me? NO, that would be
too easy. Well, we’ll just see how
he feels after I...
Objects can be flattened and inflated. Objects have state and behavior.
Behavior lives in the class, but state lives within each individual object. So what happens when
it’s time to save the state of an object? If you’re writing a game, you’re gonna need a Save/
Restore Game feature. If you’re writing an app that creates charts, you’re gonna need a Save/
Open File feature. If your program needs to save state, you can do it the hard way, interrogating
each object, then painstakingly writing the value of each instance variable to a file, in a
format you create. Or, you can do it the easy OO way—you simply freeze-dry/flatten/persist/
dehydrate the object itself, and reconstitute/inflate/restore/rehydrate it to get it back. But you’ll
still have to do it the hard way sometimes, especially when the file your app saves has to be read
by some other non-Java application, so we’ll look at both in this chapter.
this is a new chapter
 429
saving objects
Capture the Beat
You’ve made the perfect pattern. You want to save the pattern.
You could grab a piece of paper and start scribbling it down, but
instead you hit the Save button (or choose Save from the File
menu). Then you give it a name, pick a directory,
and exhale knowing that your masterpiece won’t go
out the window with the blue screen of death.
You have lots of options for how to save the state of
your Java program, and what you choose will probably
depend on how you plan to use the saved state. Here
are the options we’ll be looking at in this chapter.
If your data will be used by only the Java
program that generated it:
1
Use serialization
Write a file that holds flattened (serialized)
objects. Then have your program read the
serialized objects from the file and inflate them
back into living, breathing, heap-inhabiting objects.
If your data will be used by other programs:
2
Write a plain text file
Write a file, with delimiters that other programs can parse.
For example, a tab-delimited file that a spreadsheet or
database application can use.
These aren’t the only options, of course. You can save data in any
format you choose. Instead of writing characters, for example,
you can write your data as bytes. Or you can write out any kind
of Java primitive as a Java primitive—there are methods to write
ints, longs, booleans, etc. But regardless of the method you use,
the fundamental I/O techniques are pretty much the same:
write some data to something, and usually that something is either
a file on disk or a stream coming from a network connection.
Reading the data is the same process in reverse: read some data
from either a file on disk or a network connection. And of course
everything we talk about in this part is for times when you aren’t
using an actual database.
430
 chapter 14
Saving State
Imagine you have a program, say, a fantasy
adventure game, that takes more than one
session to complete. As the game progresses,
characters in the game become stronger, weaker,
smarter, etc., and gather and use (and lose)
weapons. You don’t want to start from scratch
each time you launch the game—it took you
forever to get your characters in top shape for
a spectacular battle. So, you need a way to save
the state of the characters, and a way to restore
the state when you resume the game. And since
you’re also the game programmer, you want the
whole save and restore thing to be as easy (and
foolproof) as possible.
1
Option one
Write the three serialized
character objects to a file
Create a file and write three serialized
character objects. The file won’t make
sense if you try to read it as text:
Ì ̈ srGameCharacter
%
 ̈ gê8MÛIpowerLjava/lang/
String;[weaponst[Ljava/lang/
String;xp2tlfur[Ljava.lang.String;≠“VÁ
È{Gxptbowtswordtdustsq~»tTrolluq~tb
are handstbig axsq~xtMagicianuq~tspe
llstinvisibility
2
Option two
Write a plain text file
Create a file and write three lines of text,
one per character, separating the pieces
of state with commas:
50,Elf,bow, sword,dust
200,Troll,bare hands,big ax
120,Magician,spells,invisibility
serialization and file I/O
Imagine you
have three
 g
ame
GameCharacter
 characters to save...
int power
String type
Weapon[] weapons
getWeapon()
useWeapon()
 power: 50
increasePower()
 // more
 type: Elf
weapons: bow,
sword, dust
o bject
power: 200
type: Troll
weapons: bare
hands, big ax
o bject
power: 120
type: Magician
weapons: spells,
invisibility
o bject
while order.! you to reading restore but The a could the text it’s serialized The in the Elf much acciden file. the typ three be ea e ob comes F f
 tally or might sier ject’s ile objects example, is (and a read much become weapon...
 variable safer) from back harder imagine “dust” values the serialization for for all values your instead that the humans progra in were way tha t of he s to
 n saved
 m in “Elf”,
 wrong
 read,
 from
 to
 which
you are here�
 431
saving objects
Writing a serialized object to a file
Here are the steps for serializing (saving) an object. Don’t bother
memorizing all this; we’ll go into more detail later in this chapter.
If exist, the it file will
 “M be yG cr am eated e.ser” automatically.
 doesn’t
1
 Make a FileOutputStream
FileOutputStream fileStream = new FileOutputStream(“MyGame.ser”);
knows Make a how F
 ileO t o utputStream connect to (a
nd object.
 create F
 il ) eO file.
autputStream
2
 Make an ObjectOutputStream
ObjectOutputStream os = new ObjectOutputStream(fileStream);
ObjectOutputS
 but need
 ca
lle
 it d
 s
 to
 can’t ‘c
ha
 be
 in
in
 fe
 dire
 g’
 tream d
 on
 ctly a
 e
 ‘h
stream el
 connec lets per’.
 yo T
 t u to hi
 to
 write s anot is a ac
 file. he
 objects,
 tually
 r.
 It
3
Write the object
os.writeObject(characterOne);
os.writeObject(characterTwo);
os.writeObject(characterThree);
serializ
 One, writes characte es
 them
 th
e
 to rT ob
je
 wo, th
 ct
 e and s
 file
 re
ferenced charac “M
yG
am
e.ser”.
 terThr
 by ch ee ar , ac and
 ter-
4
Close the ObjectOutputStream
os.close();
file) Closing underneath, will the close stre so automati th am e Fi
leO
 at
 the cally.
 utputStream top closes the (and ones
 the
432
 chapter 14
serialization and file I/O
Data moves in streams from one place to another.
Source
Destination
Connection
streams represent
a connection
to a source or
destination (file,
socket, etc.) while
chain streams
can’t connect on
their own and must
be chained to a
connection stream.
The Java I/O API has connection streams, that represent connections to destinations and
sources such as files or network sockets, and chain streams that work only if chained to
other streams.
Often, it takes at least two streams hooked together to do something useful—one to
represent the connection and another to call methods on. Why two? Because connection
streams are usually too low-level. FileOutputStream (a connection stream), for example,
has methods for writing bytes. But we don’t want to write bytes! We want to write objects, so
we need a higher-level chain stream.
OK, then why not have just a single stream that does exactly what you want? One that lets
you write objects but underneath converts them to bytes? Think good OO. Each class
does one thing well. FileOutputStreams write bytes to a file. ObjectOutputStreams turn
objects into data that can be written to a stream. So we make a FileOutputStream that lets
us write to a file, and we hook an ObjectOutputStream (a chain stream) on the end of it.
When we call writeObject() on the ObjectOutputStream, the object gets pumped into the
stream and then moves to the FileOutputStream where it ultimately gets written as bytes
to a file.
The ability to mix and match different combinations of connection and chain streams
gives you tremendous flexibility! If you were forced to use only a single stream class, you’d
be at the mercy of the API designers, hoping they’d thought of everything you might ever
want to do. But with chaining, you can patch together your own custom chains.
destination
01101001
object is flattened (serialized)
 object is written as bytes to
01101110
is written to
 is chained to
 011010010110111001
 01
Object
 ObjectOutputStream
 FileOutputStream
File
(a chain stream)
 (a connection stream)
you are here�
 433
serialized objects
What really happens to an object
when it’s serialized?
1 Object on the heap
 2 Object serialized
Objects on the heap have state—
 Serialized objects save the values
the value of the object’s instance
 of the instance variables, so that
variables. These values make one
 an identical instance (object) can be
instance of a class different from
 brought back to life on the heap.
another instance of the same class.
Object instan
ce w
it
 variables.
 h
 t
wo primit
 00
 ive
 10
0
10
1
 01
00
0
11
0
 The out the value and strea
 p s u m.
 a m r ped e sucked
 into
 00100101
 01000110
 the the along The saved for object JVM width instance with to the needs a (like and little file variable height wh
at
 to “foo.se more rest its
 va
 ar ore
 lues
 e
 info
 r”,
foo.ser
 class type is).
width
 height
 FileOutputStream fs = new FileOutputStream(“foo.ser”);
ObjectOutputStream os = new ObjectOutputStream(fs);
os.writeObject(myFoo);
Foo myFoo.setWidth(37);
 myFoo = new Foo();
 Make to the afile FileOutputStream “foo.ser”, then tha
 cha
in
 t
 connects
 an
myFoo.setHeight(70);
 ObjectOutputStream to it, and tell the
ObjectOutputStream to write the object.
434
 chapter 14
But what exactly IS an object’s state?
What needs to be saved?
Now it starts to get interesting. Easy enough to save the primitive
values 37 and 70. But what if an object has an instance variable
that’s an object reference? What about an object that has five
instance variables that are object references? What if those object
instance variables themselves have instance variables?
Think about it. What part of an object is potentially unique?
Imagine what needs to be restored in order to get an object that’s
identical to the one that was saved. It will have a different memory
location, of course, but we don’t care about that. All we care about
is that out there on the heap, we’ll get an object that has the same
state the object had when it was saved.
serialization and file I/O
br Brain
 ain barbell
 Barbell
What has to happen for the Car
object to be saved in such a way
that it can be restored back to its
original state?
Think of what—and how—you
might need to save the Car.
And what happens if an Engine
object has a reference to a
Carburetor? And what’s inside the
Tire [] array object?
The Car object has two
instance variables that
reference two other
objects.
eng
Engine
tires
Tire[ ]
Car
 objec
t
What does it take to
save a Car object?
En
gine o
bjec
 t
Ti
re
 [
 ] a rray e
 j ob
 c
 t
you are here�
 435
serialized objects
When an object is serialized, all the objects
it refers to from instance variables are also
serialized. And all the objects those objects
 Serialization saves the
objects refer to refer are serialized. to are serialized... And all the and objects the best
 those
 entire object graph.
part is, it happens automatically!
 All objects referenced
This Kennel object has a reference to a Dog [] array object. The
 by instance variables,
Dog [] holds references to two Dog objects. Each Dog object holds
 starting with the
have a reference a collection to a String of characters and a Collar and the object. Collar The objects String have objects
 an int.
 object being serialized.
When you save the Kennel, all of this is saved!
 “Fido”
St
 t
 cring o
bje
size
dogs
int
Dog [ ]
 name
 Co
 t
llar o
bjec
String
 col
Ke
nnel objec
t
 Collar
Everyth be restore to saved this ing the s
 in tate.
 has order Kennel t
 o
 to
 back
 foof
 barf
 Dog
 obje
ct
 St
rin “Spike”
 g o
bje
 t
 cDog
 Dog
size
Do
 g[ ]
 arr
ay ob
je
 t
 c
 name
 int
Co
 t
llar o
bjec
String
 col
Collar
Dog
 obje
ct
436
 chapter 14
serialization and file I/O
If you want your class to be serializable,
implement Serializable
The Serializable interface is known as a marker or tag interface,
because the interface doesn’t have any methods to implement. Its
sole purpose is to announce that the class implementing it is, well,
serializable. In other words, objects of that type are saveable through
the serialization mechanism. If any superclass of a class is serializable,
the subclass is automatically serializable even if the subclass doesn’t
explicitly declare implements Serializable. (This is how interfaces always
work. IfobjectOutputStream.writeObject(myBox);
 your superclass “IS-A” Serializable, you are too).
 Whatever goes
 o
r h
 it e
r
 e
 will M
U
 fail S
 T at implemen
 runtim
e.
 t
Serializable
import public java.io.*;
 class Box implements Serializable you need Serializable th
e is import.
 in t
h
e
 ja
v
 a
 { .i
o
 “implements “it’s package, No methods O
K
 so
 to
 se
 Serializab
 to ri
al
 impl iz
e
 ement, ob
 le”, je
cts it but says ofwhen this to the type.”
 yo
u JVM,
 say
private private int int width;
 height;
 these two values will be
 saved
public void setWidth(int w) {
width = w;
}
public void setHeight(int h) {
height = h;
}
public static void main (String[] args) {
myBox.setWidth(50);
 myBox.setHeight(20);
 Box myBox = new Box();
 I/O operations can th
row exceptions.
 Connect if new it file exis
 to t
 na
 s
. a
 med If f
il
 it e
 “foo.ser”.
 n
 doesn’t, a
m
e
d “foo.ser”
 make
 a
try {
FileOutputStream fs = new FileOutputStream(“foo.ser”);
ObjectOutputStream os = new ObjectOutputStream(fs);
os.writeObject(myBox);
 Make an Obje
}
 } catch(Exception os.close();
 ex.printStackTrace();
 ex) {
 T
 chained e
ll
 it to to write the ctOutputStream
 t
 connectio
 h
e
 object.
 n stream.
}
}
you are here�
 437
serialized objects
Serialization is all or nothing.
Can you imagine what would
happen if some of the object’s
state didn’t save correctly?
Eeewww! That
creeps me out just thinking
about it! Like, what if a Dog comes
back with no weight. Or no ears. Or
the collar comes back size 3 instead
of 30. That just can’t be allowed!
Either the entire
object graph is
serialized correctly
or serialization fails.
You can’t serialize
a Pond object if
import java.io.*;
 its Duck instance
Pond objects can be serialized.
 variable refuses to
public class Pond implements Serializable {
 be serialized (by
private Duck duck = new Duck();
 Class variable, Pond a D
 ha uc s k. on
e instance
 Serializable).
 not implementing
public static void main (String[] args) {
Pond myPond = new Pond();
try {
FileOutputStream fs = new FileOutputStream(“Pond.ser”);
ObjectOutputStream os = new ObjectOutputStream(fs);
os.close();
 os.writeObject(myPond);
 automatically object), When you its serial
 Duc gets iz k e in se my
 st
an
 rialized.
 Pond ce variable
 (a Pond
} catch(Exception ex.printStackTrace();
 ex) {
 When File Edit you Wind
 try ow He
 to lp Re ru gret
 n the main
 in class Pond:
}
}
 }
 Yikes!! Duck is not serializable!
 % java.io.No
 java Pond
 at tS
 Pond
 er
ia
li
 .main(Pond
 zableExcep
tion: .java:13)
 Duck
}
 public // duck class code Duck here
 {
 Pond’s Pon
d It so
 doesn’t when object, Duck you implement try instance it fails to serialize because variable
 Serializable,
 a
 the
can’t be saved.
438
 chapter 14
It’s hopeless,
then? I’m completely
screwed if the idiot who
wrote the class for my instance
variable forgot to make it
Serializable?
serialization and file I/O
Mark an instance variable as transient
if it can’t (or shouldn’t) be saved.
If you want an instance variable to be skipped by the
serialization process, mark the variable with the transient
keyword.
t
 save serializ
 r
a
n
s
ie
 this n
at t
 v says, ion, ariable
 just “do d n skip u ’t ring it.”
 import class transient Chat java.net.*;
 implements String currentID;
 Serializable {
String userName;
of will userName the be saved object’s variable
 as part
 st
ate
 }
 // more code
during serialization.If you have an instance variable that can’t be saved because
it isn’t serializable, you can mark that variable with the
transient keyword and the serialization process will skip right
over it.
So why would a variable not be serializable? It could be
that the class designer simply forgot to make the class
implement Serializable. Or it might be because the object
relies on runtime-specific information that simply can’t be
saved. Although most things in the Java class libraries are
serializable, you can’t save things like network connections,
threads, or file objects. They’re all dependent on (and
specific to) a particular runtime ‘experience’. In other words,
they’re instantiated in a way that’s unique to a particular run
of your program, on a particular platform, in a particular
JVM. Once the program shuts down, there’s no way to bring
those things back to life in any meaningful way; they have to
be created from scratch each time.
you are here�
439
serialized objects
Dumb there are Questions
 no
Q:
 If serialization is so important,
why isn’t it the default for all classes?
Why doesn’t class Object implement
Serializable, and then all subclasses
will be automatically Serializable.
A:
 Even though most classes will,
and should, implement Serializable,
you always have a choice. And you
must make a conscious decision on
a class-by-class basis, for each class
you design, to ‘enable’ serialization
by implementing Serializable.
First of all, if serialization were the
default, how would you turn it off?
Interfaces indicate functionality, not
a lack of functionality, so the model
of polymorphism wouldn’t work
correctly if you had to say, “implements
NonSerializable” to tell the world that
you cannot be saved.
Q:
 Why would I ever write a class
that wasn’t serializable?
A:
 There are very few reasons, but
you might, for example, have a security
issue where you don’t want a password
object stored. Or you might have an
object that makes no sense to save,
because its key instance variables are
themselves not serializable, so there’s
no useful way for you to make your
class serializable.
Q:
 If a class I’m using isn’t
serializable, but there’s no good
reason (except that the designer just
forgot or was stupid), can I subclass
the ‘bad’ class and make the subclass
serializable?
A:
 Yes! If the class itself is
extendable (i.e. not final), you can
make a serializable subclass, and just
substitute the subclass everywhere
your code is expecting the superclass
type. (Remember, polymorphism
allows this.) Which brings up another
interesting issue: what does it mean if
the superclass is not serializable?
Q:
 You brought it up: what does it
mean to have a serializable subclass
of a non-serializable superclass?
A:
 First we have to look at what
happens when a class is deserialized,
(we’ll talk about that on the next few
pages). In a nutshell, when an object
is deserialized and its superclass is not
serializable, the superclass constructor
will run just as though a new object of
that type were being created. If there’s
no decent reason for a class to not
be serializable, making a serializable
subclass might be a good solution.
Q:
 Whoa! I just realized
something big... if you make a
variable ‘transient’, this means the
variable’s value is skipped over
during serialization. Then what
happens to it? We solve the problem
of having a non-serializable instance
variable by making the instance
variable transient, but don’t we NEED
that variable when the object is
brought back to life? In other words,
isn’t the whole point of serialization
to preserve an object’s state?
A:
 Yes, this is an issue, but
fortunately there’s a solution. If you
serialize an object, a transient reference
440
 chapter 14
instance variable will be brought back
as null, regardless of the value it had
at the time it was saved. That means
the entire object graph connected to
that particular instance variable won’t
be saved. This could be bad, obviously,
because you probably need a non-null
value for that variable.
You have two options:
1) When the object is brought back,
reinitialize that null instance variable
back to some default state. This
works if your deserialized object isn’t
dependent on a particular value for
that transient variable. In other words,
it might be important that the Dog
have a Collar, but perhaps all Collar
objects are the same so it doesn’t
matter if you give the resurrected Dog
a brand new Collar; nobody will know
the difference.
2) If the value of the transient variable
does matter (say, if the color and design
of the transient Collar are unique for
each Dog) then you need to save the
key values of the Collar and use them
when the Dog is brought back to
essentially re-create a brand new Collar
that’s identical to the original.
Q:
 What happens if two objects in
the object graph are the same object?
Like, if you have two different Cat
objects in the Kennel, but both Cats
have a reference to the same Owner
object. Does the Owner get saved
twice? I’m hoping not.
A:
 Excellent question! Serialization
is smart enough to know when two
objects in the graph are the same. In
that case, only one of the objects is
saved, and during deserialization, any
references to that single object are
restored.
serialization and file I/O
Deserialization: restoring an object
 deserialized
serialized
The whole point of serializing an object is so that you can
restore it back to its original state at some later date, in a
different ‘run’ of the JVM (which might not even be the same
JVM that was running at the time the object was serialized).
Deserialization is a lot like serialization in reverse.
1
 Make a FileInputStream
 If exist, the you’ll file “M ge
t yG an am exception.
 e.ser” doesn’t
FileInputStream fileStream = new FileInputStream(“MyGame.ser”);
knows Make a how F
il t eIn
 o putStre
 connect a t m o object. an existing The f
ile.
 FileInput
Stream
2
 Make an ObjectInputStream
ObjectInputStream os = new ObjectInputStream(fileStream);
Obj
 but It stream, needs ec
 it tI
 can’t np
 in ut
 to th
 St
 be dire is re
 ch case am
 ct ained ly lets a connect FileInputStream.
 to you a read connection
 to
 a ob fi je le ct .
 s,
3
4
5
read the objects
Object Object Object two one three = = = os.readObject();
 os.readObject();
 os.readObject();
 objects get the object Each a same time big in than order fat the you you stream. exception say in wrote.
 which readObject(),
 So
 if the
y you’ll you
were read try you to written. them get read the back more
 You’ll
 nex
 in t
Cast the objects
GameCharacter GameCharacter GameCharacter elf magician troll = (GameCharacter) = (GameCharacter) = (GameCharacter) one;
 two;
 three; The (jus
 read you the t return have Object()
 type like to with yo valu u
 ca is kn
 st ArrayList), ety ow
 of it pe it back Object
 really
 to so
 is.
Close the ObjectInputStream
os.close();
file) Closing underneath, will the close stre so automati th am e Fi
leI
 at
 the cally.
 nputStream top closes (and the ones
 the
you are here�
 441
deserializing objects
What happens during deserialization?
When an object is deserialized, the JVM attempts to bring
the object back to life by making a new object on the heap
that has the same state the serialized object had at the time it
was serialized. Well, except for the transient variables, which
come back either null (for object references) or as default
primitive values.
This can’t step find
 will orth
 load r
o
w
 the a
n
 e
 class!
 x
ception if t
h
e JVM
01101001
01101110
01
File
is read by
object is read as bytes
011010010110111001
FileInputStream
(a connection stream)
class is found and loaded, saved
instance variables reassigned
is chained to
ObjectInputStream
(a chain stream)
Object
12The object is read from the stream.
The JVM determines (through info stored with
the serialized object) the object’s class type.
3The JVM attempts to find and load the ob-
ject’s class. If the JVM can’t find and/or load
the class, the JVM throws an exception and
the deserialization fails.
4
A new object is given space on the heap, but
the serialized object’s constructor does
NOT run! Obviously, if the constructor ran, it
would restore the state of the object back
to its original ‘new’ state, and that’s not what
we want. We want the object to be restored
to the state it had when it was serialized, not
when it was first created.
442
 chapter 14
serialization and file I/O
5
If the object has a non-serializable class
somewhere up its inheritance tree, the
constructor for that non-serializable class
will run along with any constructors above
that (even if they’re serializable). Once the
constructor chaining begins, you can’t stop it,
which means all superclasses, beginning with
the first non-serializable one, will reinitialize
their state.
6The object’s instance variables are given the
values from the serialized state. Transient
variables are given a value of null for object
references and defaults (0, false, etc.) for
primitives.
Dumb there are Questions
 no
Q:
Why doesn’t the class get saved as part of the ob-
ject? That way you don’t have the problem with whether
the class can be found.
A:
 Sure, they could have made serialization work that
way. But what a tremendous waste and overhead. And
while it might not be such a hardship when you’re using
serialization to write objects to a file on a local hard drive,
serialization is also used to send objects over a network
connection. If a class was bundled with each serialized
(shippable) object, bandwidth would become a much larger
problem than it already is.
For objects serialized to ship over a network, though, there
actually is a mechanism where the serialized object can be
‘stamped’ with a URL for where its class can be found. This
is used in Java’s Remote Method Invocation (RMI) so that
you can send a serialized object as part of, say, a method
argument, and if the JVM receiving the call doesn’t have
the class, it can use the URL to fetch the class from the
network and load it, all automatically. (We’ll talk about RMI
in chapter 17.)
Q:
 What about static variables? Are they serialized?
A:
 Nope. Remember, static means “one per class” not “one
per object”. Static variables are not saved, and when an
object is deserialized, it will have whatever static variable
its class currently has. The moral: don’t make serializable ob-
jects dependent on a dynamically-changing static variable!
It might not be the same when the object comes back.
you are here�
 443
serialization example
Saving and restoring the game characters
import java.io.*;
public class GameSaverTest {
 Make some characters...
public static void main(String[] args) {
GameCharacter one = new GameCharacter(50, “Elf”, new String[] {“bow”, “sword”, “dust”});
GameCharacter two = new GameCharacter(200, “Troll”, new String[] {“bare hands”, “big ax”});
GameCharacter three = new GameCharacter(120, “Magician”, new String[] {“spells”, “invisibility”});
// imagine code that does things with the characters that might change their state values
try {
ObjectOutputStream os = new ObjectOutputStream(new FileOutputStream(“Game.ser”));
os.writeObject(one);
os.writeObject(two);
os.writeObject(three);
os.close();
} catch(IOException ex) {
ex.printStackTrace();
two one }
 = = null;
 null;
 We access set the them objects to null on
 so th we e heap.
 can’t
three = null;
Now read them back in from the file...
try {
ObjectInputStream is = new ObjectInputStream(new FileInputStream(“Game.ser”));
GameCharacter oneRestore = (GameCharacter) is.readObject();
GameCharacter twoRestore = (GameCharacter) is.readObject();
GameCharacter threeRestore = (GameCharacter) is.readObject();
System.out.println(“One’s type: “ + oneRestore.getType());
 Check to see if
 it worked.
System.out.println(“Two’s type: “ + twoRestore.getType());
System.out.println(“Three’s type: “ + threeRestore.getType());
} catch(Exception ex) {
 power: 50
ex.printStackTrace();
 type: Elf
}
 File Edit Window Help Resuscitate
 weapons: bow,
}
 sword, dust
% java GameSaverTest
 power: 200
type: Troll
 o bject
One’s type: Elf
 weapons: bare
Two’s type: Troll
 hands, big ax
 power: 120
Three’s type: Magician
 o bject
 type: weapons: Magician
 spells,
invisibility
o bject
444
 chapter 14
serialization and file I/O
The GameCharacter class
import java.io.*;
public class GameCharacter implements Serializable {
int power;
String type;
String[] weapons;
public GameCharacter(int p, String t, String[] w) {
power = p;
type = t;
weapons = w;
}
public int getPower() {
return power;
}
public String getType() {
return type;
}
public String getWeapons() {
String weaponList = “”;
for (int i = 0; i < weapons.length; i++) {
weaponList += weapons[i] + “ “;
}
return weaponList;
}
}
This Serialization, actual is a game, basic but and class we’ll
 we jus do t leave n’t forhave that testing
 an
 to
you to experiment.
you are here�
 445
saving objects
Object Serialization
$$$
$
$
$
$
$
$
$
$
$
$
$
$
BULLET POINTS
You can save an object’s state by serializing the object.
To serialize an object, you need an ObjectOutputStream (from the
java.io package)
Streams are either connection streams or chain streams
Connection streams can represent a connection to a source or
destination, typically a file, network socket connection, or the
console.
Chain streams cannot connect to a source or destination and must
be chained to a connection (or other) stream.
To serialize an object to a file, make a FileOuputStream and chain it
into an ObjectOutputStream.
To serialize an object, call writeObject(theObject) on the
ObjectOutputStream. You do not need to call methods on the
FileOutputStream.
To be serialized, an object must implement the Serializable interface.
If a superclass of the class implements Serializable, the subclass will
automatically be serializable even if it does not specifically declare
implements Serializable.
When an object is serialized, its entire object graph is serialized. That
means any objects referenced by the serialized object’s instance
variables are serialized, and any objects referenced by those
objects...and so on.
If any object in the graph is not serializable, an exception will be
thrown at runtime, unless the instance variable referring to the object
is skipped.
Mark an instance variable with the transient keyword if you want
serialization to skip that variable. The variable will be restored as null
(for object references) or default values (for primitives).
During deserialization, the class of all objects in the graph must be
available to the JVM.
You read objects in (using readObject()) in the order in which they
were originally written.
The return type of readObject() is type Object, so deserialized
objects must be cast to their real type.
Static variables are not serialized! It doesn’t make sense to save
a static variable value as part of a specific object’s state, since all
objects of that type share only a single value—the one in the class.
446
 chapter 14
Writing a String to a Text File
Saving objects, through serialization, is the easiest way to save and
restore data between runnings of a Java program. But sometimes you
need to save data to a plain old text file. Imagine your Java program
has to write data to a simple text file that some other (perhaps non-
Java) program needs to read. You might, for example, have a servlet
(Java code running within your web server) that takes form data the
user typed into a browser, and writes it to a text file that somebody else
loads into a spreadsheet for analysis.
Writing text data (a String, actually) is similar to writing an object,
except you write a String instead of an object, and you use a
FileWriter instead of a FileOutputStream (and you don’t chain it to an
ObjectOutputStream).
serialization and file I/O
What the game character data
might look like if you wrote it
out as a human-readable text file.
50,Elf,bow,sword,dust
200,Troll,bare hands,big ax
120,Magician,spells,invisibility
To write a serialized object:
objectOutputStream.writeObject(someObject);
To write a String:
fileWriter.write(“My first String to save”);
We need
 the java.io p
ackage for
 FileWriter
import java.io.*;
class public WriteAFile static{
 void main (String[] args) {
 If exist, the F
 f il il e e Writer “
F
 o
o
.t
x
 will t
” does creat
 n e ot it.
try {
FileWriter writer = new FileWriter(“Foo.txt”);
A
 must Everyt IOExce
 L
L
 t
 be h
 h e
 ption!!
 ing in I/O a can try/
 stuf throw c f atch.
 an
 writer.close();
 writer.write(“hello Close foo!”);
 it when you’re The a done!
 String
 write() method takes
} catch(IOException ex) {
ex.printStackTrace();
}
}
}
you are here�
 447
writing a text file
Text File Example: e-Flashcards
Remember those flashcards you used in school? Where you
had a question on one side and the answer on the back?
They aren’t much help when you’re trying to understand
something, but nothing beats ‘em for raw drill-and-practice
and rote memorization. When you have to burn in a fact. And
they’re also great for trivia games.
We’re going to make an electronic version that has three
classes:
1) QuizCardBuilder, a simple authoring tool for creating and
saving a set of e-Flashcards.
2) QuizCardPlayer, a playback engine that can load a
flashcard set and play it for the user.
3) QuizCard, a simple class representing card data. We’ll
walk through the code for the builder and the player, and
have you make the QuizCard class yourself, using this
front
 index old-fa
 f s la h s ioned hcards
 3 x 5
What’s foreign so
u
t
h
 th o
 c f
 ountry e Detroit
 first
 due
 back
Michigan? Canada (Ontario)
QuizCard
QuizCard(q, a)
question
answer
getQuestion()
getAnswer()
.
QuizCardBuilder
Has a File menu with a “Save” option for saving
the current set of cards to a text file.
448
 chapter 14
QuizCardPlayer
Has a File menu with a “Load” option for loading a
set of cards from a text file.
serialization and file I/O
Quiz Card Builder (code outline)
public class QuizCardBuilder {
public // build void and go()display {
 gui
 Builds making and anddisp re
gi la st ys er th
 ing e event GUI, listene
 includin rs g .
}
Inner class
private public // add class void the NextCardListener actionPerformed(ActionEvent current card to implements the list and clear ActionListener ev) the {
 text areas
 {
 Triggere
 means the list the d
 and wh
 user en
 st
ar us
 wa t er
 nt a hi
 s new ts
 to ‘N
 store card.
 ext that Card’ card
 butt in on;
}
}
Inner class
private class SaveMenuListener implements ActionListener {
}
 public }
 // // let bring void the up actionPerformed(ActionEvent user a file name dialog and box
 save the set
 ev) {
 Java Quantum the Triggered File cards Rules, menu; Mechanics when in means etc.).
 the use curr the ch
 Se ent user ooses t, Hollywood list wants ‘Save’ as a to from ‘set’ Trivia,
 save (like,
 the
 all
Inner class
private }
 }
 public //class clear void NewMenuListener out actionPerformed(ActionEvent the card list, and implements clear out ActionListener the ev) text {
 areas
 {
 menu brand list and ;
 m
 ne e
 t
 w b n he y s set the c
 text h
o
 (so o
 user si
n
 areas).
 we g
 ‘N
 wants clea
 e
w
r
 ’ from t o
u o t st the ar
 th t e card
 a F
ile
Triggereda}
private void saveFile(File file) {
// iterate through the list of cards, and write each one out to a text file
// in a parseable way (in other words, with clear separations between parts)
}
Called by the SaveMenuLis tener;
does the actual file writin
g.you are here�
 449
Quiz Card Builder code
import java.util.*;
import java.awt.event.*;
import javax.swing.*;
import java.awt.*;
import java.io.*;
public class QuizCardBuilder {
private JTextArea question;
private JTextArea answer;
private ArrayList<QuizCard> cardList;
private JFrame frame;
public static void main (String[] args) {
QuizCardBuilder builder = new QuizCardBuilder();
builder.go();
}
public void go() {
Font frame // JPanel question build bigFont =mainPanel new = gui
 new JFrame(“Quiz = JTextArea(6,20);
 new = new Font(“sanserif”, JPanel();
 Card Builder”);
 Font.BOLD, 24);
 and to This special, look
 Men is all a althou u t It GU
 t e h m e s I gh MenuBar, code.
 code you here. might
 Me Nothing
 w n a u n , t
question.setLineWrap(true);
question.setWrapStyleWord(true);
question.setFont(bigFont);
JScrollPane qScroller = new JScrollPane(question);
qScroller.setVerticalScrollBarPolicy(ScrollPaneConstants.VERTICAL_SCROLLBAR_ALWAYS);
qScroller.setHorizontalScrollBarPolicy(ScrollPaneConstants.HORIZONTAL_SCROLLBAR_NEVER);
answer = new JTextArea(6,20);
answer.setLineWrap(true);
answer.setWrapStyleWord(true);
answer.setFont(bigFont);
JScrollPane aScroller = new JScrollPane(answer);
aScroller.setVerticalScrollBarPolicy(ScrollPaneConstants.VERTICAL_SCROLLBAR_ALWAYS);
aScroller.setHorizontalScrollBarPolicy(ScrollPaneConstants.HORIZONTAL_SCROLLBAR_NEVER);
JButton nextButton = new JButton(“Next Card”);
cardList = new ArrayList<QuizCard>();
JLabel qLabel = new JLabel(“Question:”);
JLabel aLabel = new JLabel(“Answer:”);
mainPanel.add(qLabel);
mainPanel.add(qScroller);
mainPanel.add(aLabel);
mainPanel.add(aScroller);
mainPanel.add(nextButton);
nextButton.addActionListener(new NextCardListener());
JMenuBar menuBar = new JMenuBar();
JMenu fileMenu = new JMenu(“File”);
JMenuItem newMenuItem = new JMenuItem(“New”);
450
 chapter 14
serialization and file I/O
JMenuItem frame.getContentPane().add(BorderLayout.CENTER, frame.setJMenuBar(menuBar);
 menuBar.add(fileMenu);
 fileMenu.add(saveMenuItem);
 fileMenu.add(newMenuItem);
 newMenuItem.addActionListener(new saveMenuItem.addActionListener(new saveMenuItem = new JMenuItem(“Save”);
 NewMenuListener());
 SaveMenuListener()); mainPanel);
 items frame menu items menu, We make can to to into then t
 us fire he a the e p m menu this ut
 e
nu an F ‘ne
 ile ActionEven
t
 bar, menu bar, w’ menu. and make
 then bar. We ‘save’ a Menu
 tell a
 File
 d
d menu
 t t he he
frame.setSize(500,600);
frame.setVisible(true);
}
public class NextCardListener implements ActionListener {
public void actionPerformed(ActionEvent ev) {
QuizCard card = new QuizCard(question.getText(), answer.getText());
cardList.add(card);
clearCard();
}
}
public class SaveMenuListener implements ActionListener {
public void actionPerformed(ActionEvent ev) {
}
 }
 saveFile(fileSave.getSelectedFile());
 fileSave.showSaveDialog(frame);
 QuizCard cardList.add(card);
 JFileChooser cardfileSave = new QuizCard(question.getText(), = new JFileChooser();
 answer.getText());
 B
 line dialog selecting JFileCho
 r
in
g
 until s
 u
 b p
 ox t a
 oser a . h file All e file, user ! It the dialo etc. really c
h file g oo is b ses o
 don dialo is x ‘Save’ t
 and e his g for navigatio
 easy.
 waits
 from you on by t n he this
 the
 and
public class NewMenuListener implements ActionListener {
public void actionPerformed(ActionEvent ev) {
cardList.clear();
clearCard();
}
}
 }
 private question.requestFocus();
 answer.setText(“”);
 question.setText(“”);
 void clearCard() {
 The (called The We’ll met arg loo
 b h ument y k od the at that
 the SaveMenuLis is the d
 File o
e
s
 ‘File’ class
 t
h
e
 t actual o on b ener’s ject the fil the e
 next v e ent writing
 user page
 handler).
 is .sav
ing.
private void saveFile(File file) {
try {
BufferedWriter writer = new BufferedWriter(new FileWriter(file));
}
 for(QuizCard writer.write(card.getAnswer() writer.write(card.getQuestion() card:cardList) {
 + “\n”);
 + “/”);
 (We’ll FileWriter We chain talka about to Buff ma er ke that ed wr
 W
riter it
 in ing a few more on pages).
 to efficient.
 a new
writer.close();
}
 }
 } }
 catch(IOException ex.printStackTrace();
 System.out.println(“couldn’t ex) {
 write the cardList out”);
 add swer per ca
rds Walk line, a separated through newline and with write characte the the by
 them ques Arra
 a
 “/
 tion r ou ”, yList (“\n”)
 t, and and one of
 then
 an-
 card
you are here�
 451
writing files
The java.io.File class
The java.io.File class represents a file on disk, but doesn’t
actually represent the contents of the file. What? Think of
a File object as something more like a pathname of a file
(or even a directory) rather than The Actual File Itself.
The File class does not, for example, have methods for
reading and writing. One VERY useful thing about a File
object is that it offers a much safer way to represent a
file than just using a String file name. For example, most
classes that take a String file name in their constructor
(like FileWriter or FileInputStream) can take a File
object instead. You can construct a File object, verify
that you’ve got a valid path, etc. and then give that File
object to the FileWriter or FileInputStream.
Some things you can do with a File object:
1
 Make a File object representing an
existing file
File f = new File(“MyCode.txt”);
2
 Make a new directory
File dir = new File(“Chapter7”);
dir.mkdir();
3
 List the contents of a directory
if (dir.isDirectory()) {
String[] dirContents = dir.list();
for (int i = 0; i < dirContents.length; i++)System.out.println(dirContents[i]);
}
}
4
 Get the absolute path of a file or directory
System.out.println(dir.getAbsolutePath());
5
 Delete a file or directory (returns true if
successful)
boolean isDeleted = f.delete();
452
 chapter 14
A File object represents the name
and path of a file or directory on
disk, for example:
/Users/Kathy/Data/GameFile.txt
But it does NOT represent, or give
you access to, the data in the file!
{
An address is NOT the
same house as ! A the File ac ob tu
 je al ct is
like it and ticular represents a location street file, ad bu
 of th t dress...
 e a it name
 par-
 isn’t
the file itself.
A File object
 represents th
e
filename “GameFile.txt”
 GameFile.txt
50,Elf,bow, sword,dust
200,Troll,bare hands,big ax
120,Magician,spells,invisibility
A represent direct data File insid
 obje access (o
 ct e r the does to) give file!
 the
 y
 N o O u T
The beauty of buffers
If there were no buffers, it would be like
shopping without a cart. You’d have to
carry each thing out to your car, one soup
can or toilet paper roll at a time.
serialization and file I/O
far (like place buffers fewer the to giv
 cart gro trip e up you ) s things is when a full. temporary you until You use
 get the a holding
 to buffer.
 holder
 mak
e
String is put into a buffer
 When the buffer is full, the
with other Strings
 Strings are all written to
“Boulder”
 “Boulder” “Aspen”
 “Aspen Denver Boulder”
is written to
 “Denver”
 is chained to
String
BufferedWriter
 FileWriter
(a chain stream that
 (a connection stream
works with characters)
 that writes characters
as opposed to bytes)
destination
Aspen
Denver
Boulder
File
BufferedWriter writer = new BufferedWriter(new FileWriter(aFile));
calling The data pass BufferedWriter need, working cool to in since write(someString), the memory. without thing file every each about will By them. trip chaining hold and buffers to You the every all but can the disk a is time. FileWriter BufferedWriter that write stuff is a they’re That’s Big you to a Deal write writes file overhead much using compared to onto each more it until FileWriter a you and FileWriter, efficient it’s to don’t every manipulating
 full. alone, want thing than
 Only the
 or
 by
 you
 when
 care BufferedWrit on, the BufferedWrit only the need Notice and object of FileWriter thing to the when th
 kee at we we’ re p w er, st ll er, e care w a e call object. of close reference because it don’t the about will methods
 the
 even
 chain
.
 The
 take
 that’s
 is to
 the
the buffer is full will the FileWriter actually be told to write to the file on disk.
If you do want to send data before the buffer is full, you do have control.
Just Flush It. Calls to writer.flush() say, “send whatever’s in the buffer, now!”
you are here�
 453
reading files
Reading from a Text File
Reading text from a file is simple, but this time we’ll use a File
object to represent the file, a FileReader to do the actual reading,
and a BufferedReader to make the reading more efficient.
The read happens by reading lines in a while loop, ending the loop
when the result of a readLine() is null. That’s the most common
style for reading data (pretty much anything that’s not a Serialized
object): read stuff in a while loop (actually a while loop test),
terminating when there’s nothing left to read (which we know
because the result of whatever read method we’re using is null).
A file with two lines of text.
What’s 2 + 2?/4
What’s 20+22/42
import java.io.*;
 Don’t forge
t the im
port.
 MyText.txt
class ReadAFile {
public static void main (String[] args) {
try {
 A characters, FileReader that is a connects connection
 to stream a text for
 file
File myFile = new File(“MyText.txt”);
FileReader fileReader = new FileReader(myFile);
Make each line a String
 as BufferedReader String the
 variable line is line read
 to hold
 = null;
 reader = new BufferedReader(fileReader);
 Cha Buffer efficie to the progra
 in
 the t
 buf edReader h
 nt e
 file m fer F
 reading. has il
to eReader is read empty read for It’ll every
 only m t (b o ore
 go ecause a
 thing when
 back
 the
 in it).
while ((line = reader.readLine()) != null) {
System.out.println(line);
} catch(Exception }
 reader.close();
 ex) {
 (because line This String that says, variable there was “Read just WA ‘line a
 re
 S line ’. ad.”
 so
m
et
hi
ng While of text, that to and variable read) assign print is it not ou
 to
 nu tth
 ll
 the
 e
}
 }
 ex.printStackTrace();
 to Or read, another read way them of
 saying and print it, “While them.”
 there are still
 lines
}
454
 chapter 14
Quiz Card Player (code outline)
public class QuizCardPlayer {
serialization and file I/O
public void go() {
// build and display gui
}
class NextCardListener implements ActionListener {
public void actionPerformed(ActionEvent ev) {
// if this is a question, show the answer, otherwise show next question
// set a flag for whether we’re viewing a question or answer
}
}
class OpenMenuListener implements ActionListener {
public void actionPerformed(ActionEvent ev) {
// bring up a file dialog box
// let the user navigate to and choose a card set to open
}
}
private void loadFile(File file) {
// must build an ArrayList of cards, by reading them from a text file
// called from the OpenMenuListener event handler, reads the file one line at a time
// and tells the makeCard() method to make a new card out of the line
// (one line in the file holds both the question and answer, separated by a “/”)
}
}
private void makeCard(String lineToParse) {
// called by the loadFile method, takes a line from the text file
// and parses into two pieces—question and answer—and creates a new QuizCard
// and adds it to the ArrayList called CardList
}
you are here�
 455
Quiz Card Player code
import java.util.*;
import java.awt.event.*;
import javax.swing.*;
import java.awt.*;
import java.io.*;
public class QuizCardPlayer {
private JTextArea display;
private JTextArea answer;
private ArrayList<QuizCard> cardList;
private private private private QuizCard JButton JFrame int currentCardIndex;
 frame;
 nextButton;
 currentCard;
 nothing Just GUI special code on
 this page;
private boolean isShowAnswer;
public static void main (String[] args) {
QuizCardPlayer reader = new QuizCardPlayer();
reader.go();
}
public void go() {
// build gui
frame = new JFrame(“Quiz Card Player”);
JPanel mainPanel = new JPanel();
Font bigFont = new Font(“sanserif”, Font.BOLD, 24);
display = new JTextArea(10,20);
display.setFont(bigFont);
display.setLineWrap(true);
display.setEditable(false);
JScrollPane qScroller = new JScrollPane(display);
qScroller.setVerticalScrollBarPolicy(ScrollPaneConstants.VERTICAL_SCROLLBAR_ALWAYS);
qScroller.setHorizontalScrollBarPolicy(ScrollPaneConstants.HORIZONTAL_SCROLLBAR_NEVER);
nextButton = new JButton(“Show Question”);
mainPanel.add(qScroller);
mainPanel.add(nextButton);
nextButton.addActionListener(new NextCardListener());
JMenuBar menuBar = new JMenuBar();
JMenu fileMenu = new JMenu(“File”);
JMenuItem loadMenuItem = new JMenuItem(“Load card set”);
loadMenuItem.addActionListener(new OpenMenuListener());
fileMenu.add(loadMenuItem);
menuBar.add(fileMenu);
frame.setJMenuBar(menuBar);
frame.getContentPane().add(BorderLayout.CENTER, mainPanel);
frame.setSize(640,500);
frame.setVisible(true);
} // close go
456
 chapter 14
serialization and file I/O
public class NextCardListener implements ActionListener {
public void actionPerformed(ActionEvent ev) {
if (isShowAnswer) {
// show the answer because they’ve seen the question
display.setText(currentCard.getAnswer());
nextButton.setText(“Next Card”);
} else if isShowAnswer //showNextCard();
 (currentCardIndex show {
 the next = false;
 question
 < cardList.size()) {
 thing or see Check an if depending answer, they’r the is
S e ho and cu on w rr Answer do ently the the answer.
 viewing boolean appropriate
 a flag question
 to
} else {
// there are no more cards!
display.setText(“That was last card”);
}
 }
 }
 nextButton.setEnabled(false);
}
public class OpenMenuListener implements ActionListener {
public void actionPerformed(ActionEvent ev) {
JFileChooser fileOpen = new JFileChooser();
}
 }
 loadFile(fileOpen.getSelectedFile());
 fileOpen.showOpenDialog(frame);
 Bring navi
gate up t
 to
 h
e
 and f
il
e
 choose d
ia
log the box file
 and to let open.
 them
private void loadFile(File file) {
cardList = new ArrayList<QuizCard>();
try {
BufferedReader reader = new BufferedReader(new FileReader(file));
showNextCard();
 // } }
 now catch(Exception String System.out.println(“couldn’t while }
 reader.close();
 ex.printStackTrace();
 time makeCard(line);
 ((line to line start ==null;
 ex) reader.readLine()) by {
showing the read first != the card
 null) card file”);
 {
 Mak
 to FileReader chose Read line that real ArrayList
 a e
 new to a
 from QuizC a parses B
 the line uf
 Fil f
 t eR .
 er
 th
 ard m he at it akeCard() eader, ed
 e File a and Reader open and time, object turns adds giving
 file passing
 chai method
 dialog.
 it it t the ne
 h
 to into e
 d
 the
 user
 the
 a
}
}
 private String[] QuizCard cardList.add(card);
 System.out.println(“made voidresult card makeCard(String = = new lineToParse.split(“/”);
 QuizCard(result[0], alineToParse) card”);
 {
 result[1]);
 Each flashcard, question use line and the into one line String and
 for two of but text the answ
 tokens we split() answer). er
 have
 corresponds as
 (one
 method to
 sepa
 for
 We’ll pars
 rate
 the
 to e to look
 out
 piec
 brea
 a question
 single
 at es.
 the
 k
 the
 We
 the
private void showNextCard() {
 split() method
 on
 the
 next
 page
.
currentCard = cardList.get(currentCardIndex);
currentCardIndex++;
display.setText(currentCard.getQuestion());
nextButton.setText(“Show Answer”);
isShowAnswer = true;
}
} // close class
you are here�
 457
parsing Strings with split()
Parsing with String split()
Imagine you have a flashcard like this:
question
What is blue + ye
llow?
answer
green
Saved in a question file like this:
What is blue + yellow?/green
What is red + blue?/purple
How do you separate the question and answer?
When you read the file, the question and answer are smooshed
together in one line, separated by a forward slash “/” (because
that’s how we wrote the file in the QuizCardBuilder code).
String split() lets you break a String into pieces.
The split() method says, “give me a separator, and I’ll break out all
the pieces of this String for you and put them in a String array.”
token 1
 separator
 token 2
In is when the what it’s QuizCar a read
 single dP in lin from la eyer looks app, the like
 file.
 this
String toTest = “What is blue + yellow?/green”;
}
 for String[] System.out.println(token);
 (String result token:result) = toTest.split(“/”);
 {
 tokens: (piece). Loop through “What In this
 is the
 ex bl ample, ue ar
ra
y
 what complex The pieces. break + yellow?” there split() an
d
 we’re apart (Note: parsing print are using method and
 the only split() each with “green”.
 it String two
 for takes token
 is fil FAR ter
 her
 into the e. s,
 mo wil
dc
ards, (in It “/”
 re can this powerful and do case) uses extremely
 etc.)
 two
 than
 it to
458
 chapter 14
serialization and file I/O
Dumb there are Questions
 no
Q:
 OK, I look in the API and there are about five
million classes in the java.io package. How the heck do
you know which ones to use?
A:
The I/O API uses the modular ‘chaining’ concept so
that you can hook together connection streams and chain
streams (also called ‘filter’ streams) in a wide range of
combinations to get just about anything you could want.
The chains don’t have to stop at two levels; you can hook
multiple chain streams to one another to get just the right
amount of processing you need.
Most of the time, though, you’ll use the same
small handful of classes. If you’re writing text files,
BufferedReader and BufferedWriter (chained to FileReader
and FileWriter) are probably all you need. If you’re writing
serialized objects, you can use ObjectOutputStream and
ObjectInputStream (chained to FileInputStream and
FileOutputStream).
In other words, 90% of what you might typically do with
Java I/O can use what we’ve already covered.
Q:
 What about the new I/O nio classes added in 1.4?
A:
The java.nio classes bring a big performance
improvement and take greater advantage of native
capabilities of the machine your program is running
on. One of the key new features of nio is that you have
direct control of buffers. Another new feature is non-
blocking I/O, which means your I/O code doesn’t just sit
there, waiting, if there’s nothing to read or write. Some
of the existing classes (including FileInputStream and
FileOutputStream) take advantage of some of the new
features, under the covers. The nio classes are more
complicated to use, however, so unless you really need the
new features, you might want to stick with the simpler
versions we’ve used here. Plus, if you’re not careful, nio can
lead to a performance loss. Non-nio I/O is probably right
for 90% of what you’ll normally do, especially if you’re just
getting started in Java.
But you can ease your way into the nio classes, by using
FileInputStream and accessing its channel through the
getChannel() method (added to FileInputStream as of
version 1.4).
Make it Stick
Roses Readers are and first,
 W
 violets riters
 are ar
e
next
 on
 ly
 .
 for text .
��
�
�
�
�
�
�
�
BULLET POINTS
To write a text file, start with a FileWriter
connection stream.
Chain the FileWriter to a BufferedWriter for
efficiency.
A File object represents a file at a particular
path, but does not represent the actual
contents of the file.
With a File object you can create, traverse,
and delete directories.
Most streams that can use a String filename
can use a File object as well, and a File object
can be safer to use.
To read a text file, start with a FileReader
connection stream.
Chain the FileReader to a BufferedReader for
efficiency.
To parse a text file, you need to be sure the
file is written with some way to recognize the
different elements. A common approach is to
use some kind of character to separate the
individual pieces.
Use the String split() method to split a String
up into individual tokens. A String with one
separator will have two tokens, one on each
side of the separator. The separator doesn’t
count as a token.
you are here�
 459
saving objects
Version ID: A Big Serialization Gotcha
Now you’ve seen that I/O in Java is actually pretty simple, especially if you
stick to the most common connection/chain combinations. But there’s one
issue you might really care about.
Version Control is crucial!
If you serialize an object, you must have the class in order to deserialize
and use the object. OK, that’s obvious. But what might be less obvious is
what happens if you change the class in the meantime? Yikes. Imagine
trying to bring back a Dog object when one of its instance variables
(non-transient) has changed from a double to a String. That violates
Java’s type-safe sensibilities in a Big Way. But that’s not the only change
that might hurt compatibility. Think about the following:
Changes to a class that can hurt deserialization:
Deleting an instance variable
Changing the declared type of an instance variable
Changing a non-transient instance variable to transient
Moving a class up or down the inheritance hierarchy
Changing a class (anywhere in the object graph) from Serializable to
not Serializable (by removing ‘implements Serializable’ from a class
declaration)
Changing an instance variable to static
Changes to a class that are usually OK:
Adding new instance variables to the class (existing objects will
deserialize with default values for the instance variables they didn’t have
when they were serialized)
Adding classes to the inheritance tree
Removing classes from the inheritance tree
Changing the access level of an instance variable has no effect on the
ability of deserialization to assign a value to the variable
Changing an instance variable from transient to non-transient
(previously-serialized objects will simply have a default value for the
previously-transient variables)
460
 chapter 14
1
You write a Dog class
101101
 101101
 10101000010
 1010 01010101
0
 class versio
n ID
1010101
 10101010
 1001010101
 #343
Dog.class
2
You serialize a Dog object
using that class
Dog
 obje
 ct
Dog object
 Object stamped is with
version
 #343
3
You change the Dog class
101101
 101101
 101000010
 1010 10 0
 class version ID
01010 100001 0 1 00110101
 0 1 10 1
 1010
 10
 #728
Dog.class
4
 You deserialize a Dog object
using the changed class
101101
101101
Dog
 obje
ct
 101000010
1010 10 0
01010 1
100001 1010
Object is
 0 1 00110101
 0 1 10 10
stamped with
 Dog.class
version #343
 class versio
n is
#728
5
Serialization fails!!
The JVM says, “you can’t
teach an old Dog new code”.
Using the serialVersionUID
Each time an object is serialized, the object (including
every object in its graph) is ‘stamped’ with a version
ID number for the object’s class. The ID is called
the serialVersionUID, and it’s computed based on
information about the class structure. As an object is
being deserialized, if the class has changed since the
object was serialized, the class could have a different
serialVersionUID, and deserialization will fail! But you
can control this.
If you think there is ANY possibility that
your class might evolve, put a serial version
ID in your class.
When Java tries to deserialize an object, it compares
the serialized object’s serialVersionUID with that of the
class the JVM is using for deserializing the object. For
example, if a Dog instance was serialized with an ID of,
say 23 (in reality a serialVersionUID is much longer),
when the JVM deserializes the Dog object it will first
compare the Dog object serialVersionUID with the
Dog class serialVersionUID. If the two numbers don’t
match, the JVM assumes the class is not compatible
with the previously-serialized object, and you’ll get an
exception during deserialization.
So, the solution is to put a serialVersionUID
in your class, and then as the class evolves, the
serialVersionUID will remain the same and the JVM
will say, “OK, cool, the class is compatible with this
serialized object.” even though the class has actually
changed.
This works only if you’re careful with your class
changes! In other words, you are taking responsibility
for any issues that come up when an older object is
brought back to life with a newer class.
To get a serialVersionUID for a class, use the serialver
tool that ships with your Java development kit.
File Edit Window Help serialKiller
% serialver Dog
Dog: static final long
serialVersionUID = -
5849794470654667210L;
serialization and file I/O
When you think your class
might evolve after someone has
serialized objects from it...
1Use the serialver command-line tool
to get the version ID for your class
File Edit Window Help serialKiller
% serialver Dog
Dog: static final long
serialVersionUID = -
5849794470654667210L;
Based on the version of
Java you’re using, this value
might be different.
2 Paste the output into your class
public class Dog {
static final long serialVersionUID =
-6849794470754667710L;
private String name;
private int size;
// method code here
}
3Be sure that when you make changes to
the class, you take responsibility in your
code for the consequences of the changes
you made to the class! For example, be
sure that your new Dog class can deal with
an old Dog being deserialized with default
values for instance variables added to the
class after the Dog was serialized.
you are here�
 461
Code Kitchen
Code Kitchen
Wh
 the e
n
 curre y
o
u
 nt
 c
li
c
 pattern k
 “serialize wil
l It”,
be saved
.“restore” the pattern checkboxes.
 back loads in, the and sa
ved
 resets
Let’s make the BeatBox save and
restore our favorite pattern
462
 chapter 14
serialization and file I/O
Saving a BeatBox pattern
Remember, in the BeatBox, a drum pattern is nothing more than a bunch of
checkboxes. When it’s time to play the sequence, the code walks through the
checkboxes to figure out which drums sounds are playing at each of the 16
beats. So to save a pattern, all we need to do is save the state of the checkboxes.
We can make a simple boolean array, holding the state of each of the 256
checkboxes. An array object is serializable as long as the things in the array are
serializable, so we’ll have no trouble saving an array of booleans.
To load a pattern back in, we read the single boolean array object (deserialize
it), and restore the checkboxes. Most of the code you’ve already seen, in the
Code Kitchen where we built the BeatBox GUI, so in this chapter, we look at
only the save and restore code.
This CodeKitchen gets us ready for the next chapter, where instead of writing
the pattern to a file, we send it over the network to the server. And instead of
loading a pattern in from a file, we get patterns from the server, each time a
participant sends one to the server.
Serializing a pattern
This is an inner class inside
the BeatBox code
.public public class void MySendListener actionPerformed(ActionEvent implements ActionListener a) {
 {
 It button all happens and the when Action the Eve
 user nt clicks fires.
the
for boolean[] (int icheckboxState = 0; i < 256;=i++) new boolean[256];
 {
 Make state of a boolea
 each n ch ar ec ra
 kbox.
 y to hold the
}
 JCheckBox }
 if checkboxState[i] (check.isSelected()) check = (JCheckBox) = true;
 {
 checkboxList.get(i); add
 (ArrayList get Walk the it th
rough
 to state the ofboolean th of ch e ec ea ch kb ch eckboxList
 oxes), ar
ray.
 one, and
 and
try {
FileOutputStream fileStream = new FileOutputStream(new File(“Checkbox.ser”));
ObjectOutputStream os = new ObjectOutputStream(fileStream);
} catch(Exception os.writeObject(checkboxState);
 ex.printStackTrace();
 ex) {
 This write/serialize part’s a piece the of
 one cake. boolean Just
 array!
}
} // close method
} // close inner class
you are here�
 463
deserializing the pattern
Restoring a BeatBox pattern
This is pretty much the save in reverse... read the boolean array and use it
to restore the state of the GUI checkboxes. It all happens when the user hits
the “restore” ‘button.
Restoring a pattern
This is another inner class
inside the BeatBo
x class.public class MyReadInListener implements ActionListener {
public void actionPerformed(ActionEvent a) {
boolean[] checkboxState = null;
try {
FileInputStream fileIn = new FileInputStream(new File(“Checkbox.ser”));
} catch(Exception checkboxState ObjectInputStream = ex) (boolean[]) is {ex.printStackTrace();}
 = new ObjectInputStream(fileIn);
 is.readObject();
 Read
 boolean boolean returns the array) array a single referenc
 (reme and object e ca
 mb
 of
 st er, in type it the readObject()
 back Object.
 fil
e to (the
 a
for (int i = 0; i < 256; i++) {
JCheckBox check = (JCheckBox) checkboxList.get(i);
if (checkboxState[i]) {
} else check.setSelected(false);
 check.setSelected(true);
 {
 JCheckBox Now checkboxes restore objects in the the state
 Array (checkboxLis of Lis eac
 t
 of
 h of t).
 actual
 the
}
}
sequencer.stop();
 buildTrackAndStart();
 Now and
 rebu
 stop ild
 whatever the
 sequ
 is ence
 currently usin
g
 the
 play new
 ing,
} // close method
 state of the checkboxes in the ArrayList.
} // close inner class
Sharpen your pencil
This version has a huge limitation! When you hit the “serializeIt” button, it
serializes automatically, to a file named “Checkbox.ser” (which gets created if it
doesn’t exist). But each time you save, you overwrite the previously-saved file.
Improve the save and restore feature, by incorporating a JFileChooser so that
you can name and save as many different patterns as you like, and load/restore
from any of your previously-saved pattern files.
464
 chapter 14
Sharpen your pencil
Can they be saved?
Which of these do you think are, or should be,
serializable? If not, why not? Not meaningful?
Security risk? Only works for the current
execution of the JVM? Make your best guess,
without looking it up in the API.
serialization and file I/O
Object type
Object
String
File
Date
OutputStream
JFrame
Integer
System
What’s Legal?
Circle the code fragments
that would compile (assuming
they’re within a legal class).
Serializable?
 If not, why not?
Yes / No
 ______________________________________
Yes / No
 ______________________________________
Yes / No
 ______________________________________
Yes / No
 ______________________________________
Yes / No
 ______________________________________
Yes / No
 ______________________________________
Yes / No
 ______________________________________
Yes / No
 ______________________________________
FileReader fileReader = new FileReader();
BufferedReader reader = new BufferedReader(fileReader);
FileOutputStream f = new FileOutputStream(new File(“Foo.ser”));
ObjectOutputStream os = new ObjectOutputStream(f);
BufferedReader reader = new BufferedReader(new FileReader(file));
String line = null;
while ((line = reader.readLine()) != null) {
makeCard(line);
}
ObjectInputStream is = new ObjectInputStream(new FileOutputStream(“Game.ser”));
GameCharacter oneAgain = (GameCharacter) is.readObject();
you are here�
 465
exercise: True or False
This chapter explored the wonerful world of
Java I/O. Your job is to decide whether each
of the following I/O-related statements is
true or false.
Exercise
CTrue or FalseD
1. Serialization is appropriate when saving data for non-Java programs to use.
2. Object state can be saved only by using serialization.
3. ObjectOutputStream is a class used to save serialized objects.
4. Chain streams can be used on their own or with connection streams.
5. A single call to writeObject() can cause many objects to be saved.
6. All classes are serializable by default.
7. The transient modifier allows you to make instance variables serializable.
8. If a superclass is not serializable then the subclass can’t be serializable.
9. When objects are deserialized, they are read back in last-in, first out sequence.
10. When an object is deserialized, its constructor does not run.
11. Both serialization and saving to a text file can throw exceptions.
12. BufferedWriters can be chained to FileWriters.
13. File objects represent files, but not directories.
14. You can’t force a buffer to send its data before it’s full.
15. Both file readers and file writers can be buffered.
16. The String split() method includes separators as tokens in the result array.
17. Any change to a class breaks previously serialized objects of that class.
466
 chapter 14
File Edit Window Help Torture
% java DungeonTest
12
8
Code Magnets
 serialization and file I/O
This one’s tricky, so we promoted it from an Exercise to full Puzzle status.
Reconstruct the code snippets to make a working Java program that pro-
duces the output listed below. (You might not need all of the magnets, and
you may reuse a magnet more than once.)
class DungeonG
ame im
plements Serial
izable {
try {
ream fos = ne
w
FileOutputSt
 short getZ() {
ream(“dg.ser
”);
FileOutputSt
 return z;
e.printStackTrace();
oos.close();
ObjectInputStream ois =
 new
int getX() {
ObjectInputStream(fis);
return x;
System.out.println(d.getX()+d.getY()+d.getZ());
FileInputStream fis = new
 public int x = 3;
FileInputStream(“dg.ser”
); transient long y = 4;
private short z = 5;
long getY() {
return y;
 class Du
ngeonTes
t {
ois.close();
import java.io.
*;
fos.writeObject(d);
} catch (Excepti
on e) {
d = (DungeonGame) ois.readObject();
ObjectOutputStream oos = new
ObjectOutputStream(fos);
oos.writeObject(
d);
public static void
 main(String [] ar
gs) {
DungeonGame d = ne
w DungeonGame();
you are here�
467
exercise solutions
Exercise Solutions
1. Serialization is appropriate when saving data for non-Java programs to use.
2. Object state can be saved only by using serialization.
3. ObjectOutputStream is a class used to save serialized objects.
4. Chain streams can be usedon their own or with connection streams.
5. A single call to writeObject() can cause many objects to be saved.
6. All classes are serializable by default.
7. The transient modifier allows you to make instance variables serializable.
8. If a superclass is not serializable then the subclass can’t be serializable.
9. When objects are deserialized they are read back in last-in, first out sequence.10. When an object is deserialized, its constructor does not run.
11. Both serialization and saving to a text file can throw exceptions.
12. BufferedWriters can be chained to FileWriters.
13. File objects represent files, but not directories.
14. You can’t force a buffer to send its data before it’s full.
15. Both file readers and file writers can optionally be buffered.
16. The String split() method includes separators as tokens in the result array.
17. Any change to a class breaks previously serialized objects of that class.
False
False
True
False
True
False
False
False
False
True
True
True
False
False
True
False
False
468
 chapter 14
Good thing we’re
finally at the answers.
I was gettin’ kind of
tired of this chapter.
serialization and file I/O
File Edit Window Help Escape
% java DungeonTest
12
8
import java.io.*;
class DungeonGame implements Serializable {
public int x = 3;
transient long y = 4;
private short z = 5;
int getX() {
return x;
}
long getY() {
return y;
}
short getZ() {
return z;
}
}
class DungeonTest {
public static void main(String [] args) {
DungeonGame d = new DungeonGame();
System.out.println(d.getX() + d.getY() + d.getZ());
try {
FileOutputStream fos = new FileOutputStream(“dg.ser”);
ObjectOutputStream oos = new ObjectOutputStream(fos);
oos.writeObject(d);
oos.close();
FileInputStream fis = new FileInputStream(“dg.ser”);
ObjectInputStream ois = new ObjectInputStream(fis);
d = (DungeonGame) ois.readObject();
ois.close();
} catch (Exception e) {
e.printStackTrace();
}
System.out.println(d.getX() + d.getY() + d.getZ());
}
}
you are here�
 469
15 networking and threads
Make a Connection
Connect with the outside world. Your Java program can reach out and touch a
program on another machine. It’s easy. All the low-level networking details are taken care of
by classes in the java.net library. One of Java’s big benefits is that sending and receiving data
over a network is just I/O with a slightly different connection stream at the end of the chain. If
you’ve got a BufferedReader, you can read. And the BufferedReader couldn’t care less if the data
came out of a file or flew down an ethernet cable. In this chapter we’ll connect to the outside
world with sockets. We’ll make client sockets. We’ll make server sockets. We’ll make clients and
servers. And we’ll make them talk to each other. Before the chapter’s done, you’ll have a fully-
functional, multithreaded chat client. Did we just say multithreaded? Yes, now you will learn
the secret of how to talk to Bob while simultaneously listening to Suzy.
this is a new chapter
 471
beat box chat
Real-time Beat Box Chat
You’re working on a computer game. You and your team
are doing the sound design for each part of the game.
Using a ‘chat’ version of the Beat Box, your team can
collaborate—you can send a beat pattern along with
your chat message, and everybody in the Beat Box Chat
gets it. So you don’t just get to read the other
participants’ messages, you get to load and
play a beat pattern simply by clicking the
message in the incoming messages area.
In this chapter we’re going to learn what it
takes to make a chat client like this. We’re
even going to learn a little about making a
chat server. We’ll save the full Beat Box Chat
for the Code Kitchen, but in this chapter you
will write a Ludicrously Simple Chat Client and
Very Simple Chat Server that send and receive
text messages.
You can have comp
letely
authentic, intellectually
stimulating chatconversations. Every
message is sent to all
participants.
472
 chapter 15
try this one...
 it’s better for
sequence 8
evster2: sequence skyler4: like fast
 12
 sk
yler2, and funky, but go
od mor
e
 for
 Type press to AND send a th yo
 m e your ess ur sendIt age current message
 and
 button
 beat
Oakenfoldish
 pattern
skyler5: you
 WISH! Too pe
rky
clicking loads the on patter a receive n th
 d at
 message
 went
with it
Send your message to the server
Chat Program Overview
The Client has to know
about the Server.
The Server has to know
about ALL the Clients.
networking and threads
Why am I
here? Don’t
expect ME to
answer that.
Client A
Why am I
here? Don’t
expect ME to
answer that.
Client B
there are currently
3 participants in
this chat session:
Client A, Client B
and Client C
Server
Why am I
here? Don’t
expect ME to
answer that.
How it Works:
1
 Client connects to the server
2The server makes a
connection and adds the client
to the list of participants
3
 Another client connects
4
Client A sends a message to
the chat service
5The server distributes the
message to ALL participants
(including the original sender)
Client C
Client A
Server, I’d like to connect
to the chat service
Waiting for
client requests
Server
Client A
Why am I here?
Don’t expect
ME to answer
that. So, why
Client B
OK, you’re in.
Server, I’d like to connect
to the chat service
OK, you’re in.
Why am I here?
Don’t expect
ME to answer
that. So, why
Client A
Why am I here?
Don’t expect
ME to answer
that. So, why
Client A
“Who took the lava lamp
from my dorm room?”
Why am I here?
Don’t expect
ME to answer
that. So, why
“Who took the lava lamp
from my dorm room?”
Client B
Participants:
1. Client A
Server
Participants:
1. Client A
2. Client B
Server
Message
received
Server
Message
distributed to
all participants
Server
you are here�
 473
socket connections
Connecting, Sending, and Receiving
The three things we have to learn to get the client working are :
1) How to establish the initial connection between the client and server
2) How to send messages to the server
3) How to receive messages from the server
There’s a lot of low-level stuff that has to happen for these things to work. But we’re
lucky, because the Java API networking package (java.net) makes it a piece of cake
for programmers. You’ll see a lot more GUI code than networking and I/O code.
And that’s not all.
Lurking within the simple chat client is a problem we haven’t faced so far in this
book: doing two things at the same time. Establishing a connection is a one-time
operation (that either works or fails). But after that, a chat participant wants to
send outgoing messages and simultaneously receive incoming messages from the other
participants (via the server). Hmmmm... that one’s going to take a little thought, but
we’ll get there in just a few pages.
1
2
Connect
Client connects to the server by
establishing a Socket connection.
Make a socket connection to
196.164.1.103 at port 5000
Client A
Send
Client sends a message to the server
3
writer.println(aMessage)
Client A
Receive
Client gets a message from the server
474
 chapter 15
Client A
String s = reader.readLine()
chat server at
196.164.1.103,
port 5000
Server
Server
machine at
196.164.1.103
Server
Server
machine at
196.164.1.103
Server
networking and threads
Make a net work Socket connection
To make a Socket
To connect to another machine, we need a Socket connection.
 connection, you need
A Socket ( java.net.Socket class) is an object that represents
a network connection between two machines. What’s a
 to know two things
pieces connection? of software A relationship know about between each two other. machines, Most importantly,
 where two
 about the server: who
those two pieces of software know how to communicate with
 it is, and which port
each other. In other words, how to send bits to each other.
 it’s running on.
We don’t care about the low-level details, thankfully, because
they’re stack’. If handled you don’t at a know much what lower the place ‘networking in the ‘networking
 stack’ is, don’t
 In other words,
information worry about it. (bits) It’s just must a travel way of through looking to at get the from layers a that
 Java
 IP address and TCP
program running in a JVM on some OS, to physical hardware
 port number.
(ethernet cables, for example), and back again on some other
machine. Somebody has to take care of all the dirty details.
But not you. That somebody is a combination of OS-specific
software and the Java networking API. The part that you have
to worry about is high-level—make that very high-level—and
shockingly simple. Ready?
 TCP port
 number
Socket chatSocket = new Socket(“196.164.1.103”, 5000);
IP address for the
 server
The chat server is at
 This client is at
When 196.164.1.103, that’s the I need message.
 where to port I’ll talk send
 5000.
 to him,
 on to Socket
 the
 port se c 5 r o 0 ver nnection
 00
 at
 S
 back 196.164
 o
c
k
e
t
 to c
 .1 o
 t nnectio .100, he clie
 port
 n n t at
 196.164.1.100, When him, that’s I need the where message.
 to port talk I’ll 4242.
 to
 send
196.164.1.103
 4242
Client
 Server
A Socket connection means the two machines have
information about each other, including network
location (IP address) and TCP port.
you are here�
475
well-known ports
A TCP port is just a number.
A 16-bit number that identifies
a specific program on the ser ver.
Your internet web (HTTP) server runs on port 80. That’s a
standard. If you’ve got a Telnet server, its running on port
23. FTP? 20. POP3 mail server? 110. SMTP? 25. The Time
server sits at 37. Think of port numbers as unique identifiers.
They represent a logical connection to a particular piece of
software running on the server. That’s it. You can’t spin your
hardware box around and find a TCP port. For one thing,
you have 65536 of them on a server (0 - 65535). So they
obviously don’t represent a place to plug in physical devices.
They’re just a number representing an application.
Without port numbers, the server would have no way of
knowing which application a client wanted to connect to.
And since each application might have its own unique
protocol, think of the trouble you’d have without these
identifiers. What if your web browser, for example, landed
at the POP3 mail server instead of the HTTP server? The
mail server won’t know how to parse an HTTP request! And
even if it did, the POP3 server doesn’t know anything about
servicing the HTTP request.
When you write a server program, you’ll include code that
tells the program which port number you want it to run on
(you’ll see how to do this in Java a little later in this chapter).
In the Chat program we’re writing in this chapter, we picked
5000. Just because we wanted to. And because it met the
criteria that it be a number between 1024 and 65535. Why
1024? Because 0 through 1023 are reserved for the well-
known services like the ones we just talked about.
And if you’re writing services (server programs) to run on
a company network, you should check with the sys-admins
to find out which ports are already taken. Your sys-admins
might tell you, for example, that you can’t use any port
number below, say, 3000. In any case, if you value your limbs,
you won’t assign port numbers with abandon. Unless it’s
your home network. In which case you just have to check with
your kids.
476
 chapter 15
Well-known TCP port numbers
for common server applications
Telnet
 SMTP
FTP
20 23 25
37
 Time
Server
443 110 80
HTTPS
 POP3
 HTTP
A server can have up to 65536
different server apps running,
one per port.
The TCP port
numbers from 0 to 1023
are reserved for well-
known services. Don’t
use them for your own
server programs!*
The chat server we’re
writing uses port
5000. We just picked a
number between 1024
and 65535.
*Well, you might be able to use one of
these, but the sys-admin where you
work will probably kill you.
Dumb there are Questions
 no
Q:
 How do you know the port
number of the server program you
want to talk to?
A:
 That depends on whether the
program is one of the well-known
services. If you’re trying to connect
to a well-known service, like the ones
on the opposite page (HTTP, SMTP,
FTP, etc.) you can look these up on
the internet (Google “Well-Known
TCP Port”). Or ask your friendly
neighborhood sys-admin.
But if the program isn’t one of the
well-known services, you need to find
out from whoever is deploying the
service. Ask him. Or her. Typically, if
someone writes a network service
and wants others to write clients for
it, they’ll publish the IP address, port
number, and protocol for the service.
For example, if you want to write a
client for a GO game server, you can
visit one of the GO server sites and
find information about how to write a
client for that particular server.
Q:
 Can there ever be more than
one program running on a single
port? In other words, can two
applications on the same server have
the same port number?
A:
 No! If you try to bind a program
to a port that is already in use,
you’ll get a BindException. To bind a
program to a port just means starting
up a server application and telling it to
run on a particular port. Again, you’ll
learn more about this when we get to
the server part of this chapter.
IP address is the mall
networking and threads
store Port number in the mall is
 the specific
IP address is like specifying a
particular shopping mall, say,
“Flatirons Marketplace”
Port number is like naming
a specific store, say,
“Bob’s CD Shop”
br Brain
 ain barbell
 Barbell
OK, you got a Socket connection. The client and the
server know the IP address and TCP port number for
each other. Now what? How do you communicate
over that connection? In other words, how do you
move bits from one to the other? Imagine the kinds of
messages your chat client needs to send and receive.
How do these two
actually talk
 toeach other?
Client
Chat server
program
Server
you are here�
 477
reading from a socket
To read data from a Socket, use a
 input and output streams
BufferedReader
 to connections
 and from the Socket
To communicate over a Socket connection, you use streams.
Regular old I/O streams, just like we used in the last chapter. One
of the coolest features in Java is that most of your I/O work won’t
care what your high-level chain stream is actually connected to. In
other words, you can use a BufferedReader just like you did when
you were writing to a file, the difference is that the underlying
connection 1
 Make stream a Socket is connected connection to a Socket to rather the server
 than a File!
 The because the port port we num num
 T O be ber L r, D for which you our that you chat know
 5000 server.
 is
Socket chatSocket = new Socket(“127.0.0.1”, 5000);
server can other 127.0.0.1
 use on word this a is si s, when ngle, the the IP stand-a
lone you’re one address this testing code for machin is y “
 ou
 localhost”, running r e c .lient on. and
 in
 You
2
 Make an InputStreamReader chained to the Socket’s
low-level (connection) input stream
InputStreamReader stream = new InputStreamReader(chatSocket.getInputStream());
InputStr
 level Sock
 the the Buffere et
 chain by
 )
 te
 ea
 an
 m
 st
 stre
 d
 R
 re
 dR
 a
 ea
 am
 am).
 hi
 ea
 de
 gh
 de
 r
 (l
 -
 ik
 is
 r
 le
 e
 we
 vel a
 th
 ’r
 ‘b
 e character ridge’ e
 one af
ter
 coming between as
st our re
 fr am
 om
 a to
 low-
 p
 (like
 the
 of
 something stream, an All input we have but stream! more to we’re do
 text-frie It’ just is s AS
 a
 gonn low
 K ndly.
 the -level a chain socket connection
 it for
 to
3
 Make a BufferedReader and read!
 Chain InputStr level connecti
 the ea
 Buf m
R
 on ea
 feredReader de
 stream r(
w
hi
ch
 wewa
 got to s th
 ch
 from ained e
 the to Sock
 the et low .) -
BufferedReader reader = new BufferedReader(stream);
String message = reader.readLine();
destinatio
n
 buffered characters
 converted to characters
 bytes from server
buffered
 characters
 011010011
characters
 chained to
 chained to
BufferedReader
 InputStreamReader
 Socket’s input stream
Client
 (we don’t need to know
the actual class)
478
 chapter 15
source
Data on the
server
Server
networking and threads
To write data to a Socket, use a
PrintWriter
We didn’t use PrintWriter in the last chapter, we used BufferedWriter. We have
a choice here, but when you’re writing one String at a time, PrintWriter is the
standard choice. And you’ll recognize the two key methods in PrintWriter,
print() and println()! Just like good ol’ System.out.
1
 Make a Socket connection to the server
 th
 opposite server, is
 pa
rt
 we page ’s
 st
 th
 ill e
 -- sa
 have
 m
 to e to as write it connect
 was to th on to e the
 it.
Socket chatSocket = new Socket(“127.0.0.1”, 5000);
2
 Make a PrintWriter chained to the Socket’s low-level
(connection) output stream
PrintWriter writer = new PrintWriter(chatSocket.getOutputStream());
PrintW
 character Socket’s PrintWriter can write ri
te
 low-leve r
 data String
 ac
 to ts
 an the s l as
 d output to it
 the Socket’s s the own bytes stream. Socket bridge output it gets
 connection.
 By betw stream, chaining fr ee om n
 the
 we
 a
 The giving stream Socket it and to the we gives chain PrintWrite us a it low
 to -level the r constructor.
 PrintWriter connection
 by
3
 writer.print(“another writer.println(“message Write (print) something
 message”);
 to send”);
 println(
 print() )
 doesn’t ad
ds
 a
 add
 ne
w
 the lin
e
 new at the line.
 end of wha
 t it send
s.
source
Client
characters
“message...”
PrintWriter
bytes to server
011010011
chained to
Socket’s output
stream (we don’t need
to know the actual class)
destination
Chat server
program
Server
you are here�
 479
writing a client
The DailyAdviceClient
Before we start building the Chat app,
let’s start with something a little smaller.
The Advice Guy is a server program that
offers up practical, inspirational tips
to get you through those long days of
coding.
We’re building a client for The Advice
Guy program, which pulls a message
from the server each time it connects.
What are you waiting for? Who knows
what opportunities you’ve missed
without this app.
Treat yourself to
a cold one! You
deserve it!
Tell your boss
the report will
have to wait. There’s
powder at Aspen!
That shade of
green isn’t really
workin’ for you...
1
The Advice Guy
Connect
Client connects to the server and gets an
input stream from it
Make a socket connection to
190.165.1.103 at port 4242
socket.getInputStream()
Client
advice server
at 190.165.1.103,
port 4242
Server
480
2
 Read
Client reads a message from the server
advice = reader.readLine()
Client A
chapter 15
advice server
composes
advice and
sends it
Server
networking and threads
DailyAdviceClient code
This program makes a Socket, makes a BufferedReader (with the
help of other streams), and reads a single line from the server
application (whatever is running at port 4242).
import java.io.*;
 class Socket is in
 java.net
import java.net.*;
public class DailyAdviceClient {
public tryvoid {
 go()a {
 lot can go wro
ng here
 mak
 running this e
 code a
 S
 on oc
 is
 ke
 po ru t
 rt nning co
 4242, nnection on. (The on to t
he wha ‘localhost’)
 same tever host
 is
Socket s = new Socket(“127.0.0.1”, 4242);
InputStreamReader streamReader = new InputStreamReader(s.getInputStream());
BufferedReader reader = new BufferedReader(streamReader);
 chain a BufferedReader to
an InputStreamReader to
the input stream from the
Socket.
} catch(IOException String reader.close();
 System.out.println(“Today advice = reader.readLine();
 ex) this {
 closes you ALL should: the streams
 “ + advice); the reader call In B
 the this other chara a same read
 Buffer do c e wor t L sn as ers ’t ine() d
s,
 edReader der came know you b
y is chained the EXACTLY
 were from
 or metho
 care time .
 usin
 to w a d g
 you h ,F a e the
 re
 IL
E..
ufferedReaif
ex.printStackTrace();
}
}
public static void main(String[] args) {
DailyAdviceClient client = new DailyAdviceClient();
client.go();
}
}
you are here�
 481
socket connections
Sharpen your pencil
Test your memory of the streams/classes for reading and writing from a
Socket. Try not to look at the opposite page!
To read text from a Socket:
Client
write/draw in the chain of streams the client
uses to read from the server
To send text to a Socket:
write/draw in the chain of streams the client
Client
 uses to send something to the server
Sharpen your pencil
Fill in the blanks:
What two pieces of information does the client need in order to make a
Socket connection with a server?
Which TCP port numbers are reserved for ‘well-known services’ like HTTP and FTP?
TRUE or FALSE: The range of valid TCP port numbers can be represented
by a short primitive?
482
 chapter 15
source
Server
destination
Server
Writing a simple ser ver
So what’s it take to write a server application? Just a
couple of Sockets. Yes, a couple as in two. A ServerSocket,
which waits for client requests (when a client makes a
new Socket()) and a plain old Socket socket to use for
communication with the client.
How it Works:
1
Server application makes a ServerSocket, on a specific port
ServerSocket serverSock = new ServerSocket(4242);
This starts the server application listening
for client requests coming in for port 4242.
networking and threads
ServerSock
et
4242
2
Client makes a Socket connection to the server application
Socket sock = new Socket(“190.165.1.103”, 4242);
Client knows the IP address and port number
(published or given to him by whomever
configures the server app to be on that port)
Socket
ServerSock
et
4242
3
Server makes a new Socket to communicate with this client
Socket sock = serverSock.accept();
The accept() method blocks (just sits there) while
it’s waiting for a client Socket connection. When a
 Socket
client finally tries to connect, the method returns
a plain old Socket (on a different port) that knows
how to communicate with the client (i.e., knows the
client’s IP address and port number). The Socket is on
a different port than the ServerSocket, so that the
ServerSocket can go back to waiting for other clients.
Se
 for r
v
e
 the r
S
o
c
 ne
 k
 xt e
t (waitin client
) g
4242
2789
Socket
you are here�
 483
writing a server
DailyAdviceSer ver code
This program makes a ServerSocket and waits for client requests. When it gets
a client request (i.e. client said new Socket() for this application), the server
makes a new Socket connection to that client. The server makes a PrintWriter
(using the Socket’s output stream) and sends a message to the client.
import import public java.net.*;
 java.io.*;
 class DailyAdviceServer remember t
he imports
 {
 daily advice comes
 fr
om
 th
is
 ar
ra
y
 (remember, were the hit of a return code
 word-w String ed
 th in it
 !)
 ra or. es th
 pp
 e e
 Strings
 ed
 N
 m
 ev
 id
 by
 er
 dle
String[] adviceList = {“Take smaller bites”, “Go for the tight jeans. No they do NOT
make you look fat.”, “One word: inappropriate”, “Just for today, be honest. Tell your
boss what you *really* think”, “You might want to rethink that haircut.”};
public tryvoid ServerSocket {
 go() The waiting {
 server for serverSock goes (and int
 servicing
 o a permanent = ) new client ServerSocket(4242);
 requ
 loop, ests
 the accept method code on application ServerSocke
 port is blocks runnin 42
 ‘li 4 (just t st g 2 m en on on akes ’ sits .
 for the this there) client machine server
 until reques
 this
 a
 ts
while(true) {
 request comes in, and then the method returns a
Socket (on some anonymous port) for communicating
Socket sock = serverSock.accept();
 with the client
PrintWriter writer = new PrintWriter(sock.getOutputStream());
String advice = getAdvice();
}
 System.out.println(advice);
 writer.close();
 writer.println(advice);
 we’re advice
 make now we done a message. PrintWrite use with the Th th So is en
 ck
 r
 an
d
 client.
 et weconn
ection close sen
d
 it the
 (println()) Socket to the because
 client a Strin
 to
g
} catch(IOException ex) {
ex.printStackTrace();
}
} // close go
private String getAdvice() {
int random = (int) (Math.random() * adviceList.length);
return adviceList[random];
}
public static void main(String[] args) {
DailyAdviceServer server = new DailyAdviceServer();
server.go();
}
}
484
 chapter 15
networking and threads
br Brain
 ain barbell
 Barbell
How does the server know how to
communicate with the client?
The client knows the IP address and port
number of the server, but how is the server
able to make a Socket connection with the
client (and make input and output streams)?
Think about how / when / where the server
gets knowledge about the client.
Dumb there are Questions
 no
Q:
 The advice server code on the opposite
page has a VERY serious limitation—it looks
like it can handle only one client at a time!
A:
 Yes, that’s right. It can’t accept a request
from a client until it has finished with the
current client and started the next iteration of
the infinite loop (where it sits at the accept()
call until a request comes in, at which time it
makes a Socket with the new client and starts
the process over again).
Q:
 Let me rephrase the problem: how can
you make a server that can handle multiple
clients concurrently??? This would never
work for a chat server, for instance.
A:
 Ah, that’s simple, really. Use separate
threads, and give each new client Socket to a
new thread. We’re just about to learn how to
do that!
������������BULLET POINTS
Client and server applications communicate over a Socket
connection.
A Socket represents a connection between two applications
which may (or may not) be running on two different physical
machines.
A client must know the IP address (or domain name) and
TCP port number of the server application.
A TCP port is a 16-bit unsigned number assigned to a
specific server application. TCP port numbers allow different
clients to connect to the same machine but communicate
with different applications running on that machine.
The port numbers from 0 through 1023 are reserved for
‘well-known services’ including HTTP, FTP, SMTP, etc.
A client connects to a server by making a Server socket
Socket s = new Socket(“127.0.0.1”, 4200);
Once connected, a client can get input and output streams
from the socket. These are low-level ‘connection’ streams.
sock.getInputStream();
sock.getOutputStream();
To read text data from the server, create a BufferedReader,
chained to an InputStreamReader, which is chained to the
input stream from the Socket.
InputStreamReader is a ‘bridge’ stream that takes in
bytes and converts them to text (character) data. It’s used
primarily to act as the middle chain between the high-level
BufferedReader and the low-level Socket input stream.
To write text data to the server, create a PrintWriter chained
directly to the Socket’s output stream. Call the print() or
println() methods to send Strings to the server.
Servers use a ServerSocket that waits for client requests on
a particular port number.
When a ServerSocket gets a request, it ‘accepts’ the request
by making a Socket connection with the client.
you are here�
 485
a simple chat client
Writing a Chat Client
We’ll write the Chat client application in two stages. First we’ll
make a send-only version that sends messages to the server but
doesn’t get to read any of the messages from other participants
(an exciting and mysterious twist to the whole chat room
concept).
Then we’ll go for the full chat monty and make one that both
sends and receives chat messages.
Version One: send-only
T
 to won’t server no yp
 send e
 scrolling
 a
 get m
 in it es
 t sa
 an to his ge
 y text t version, ,
 m he then essages server. area.
 press so FROM there’s
 W
 ‘Send
 e the
 ’
Code outline
public class SimpleChatClientA {
JTextField outgoing;
PrintWriter writer;
Socket sock;
public void go() {
// make gui and register a listener with the send button
// call the setUpNetworking() method
}
private void setUpNetworking() {
// make a Socket, then make a PrintWriter
// assign the PrintWriter to writer instance variable
}
public class SendButtonListener implements ActionListener {
public void actionPerformed(ActionEvent ev) {
// get the text from the text field and
// send it to the server using the writer (a PrintWriter)
}
} // close SendButtonListener inner class
} // close outer class
486
 chapter 15
networking and threads
import import import import java.io.*;
 java.awt.*;
 javax.swing.*;
 java.net.*;
 imports Socket (jav fora. t ne
 he t
streams ) and t
he
 (jav GU a. I
 io),
import java.awt.event.*;
 stuff
public class SimpleChatClientA {
JTextField outgoing;
PrintWriter writer;
Socket sock;
public void go() {
JFrame frame = new JFrame(“Ludicrously Simple Chat Client”);
JPanel mainPanel = new JPanel();
mainPanel.add(outgoing);
 sendButton.addActionListener(new outgoing JButton sendButton = new JTextField(20);
 = new JButton(“Send”);
 SendButtonListener());
 build here, networking
 the and GU no or t I, hi no
 I/
O.
 ng thing related new
 to
mainPanel.add(sendButton);
frame.getContentPane().add(BorderLayout.CENTER, mainPanel);
setUpNetworking();
frame.setSize(400,500);
frame.setVisible(true);
private } // close void go
 setUpNetworking() {
 and you we’re can server using test on localhos
 the one cl m t ient
 achine
 so
try {
sock = new Socket(“127.0.0.1”, 5000);
 This is
 where we make the Socket
writer = new PrintWriter(sock.getOutputStream()); and the PrintWriter (it’s called
System.out.println(“networking established”);
 from
 the
 go(
)
 me
tho
d
 rig
ht before
} catch(IOException ex) {
 displaying the app GU
I)
ex.printStackTrace();
}
} // close setUpNetworking
public class SendButtonListener implements ActionListener {
public void actionPerformed(ActionEvent ev) {
try {
writer.println(outgoing.getText()); Now we actually do the writing.
writer.flush();
 Remember, the writer is chained to
the output stream from the Socket,
} catch(Exception ex) {
 so whenever we do a println(), it goes
ex.printStackTrace();
 over the network to the server!
}
outgoing.setText(“”);
outgoing.requestFocus();
}
 If you want to try this now, type in
}
 // close SendButtonListener inner class
 the Ready-bake chat server code
listed at the end of this chapter .
public static void main(String[] args) {
 First, start the server in one terminal.
new SimpleChatClientA().go();
 Next, use another terminal to start
}
 this client.
} // close outer class
you are here�
 487
improving the chat client
Version Two: send
and receive
incoming
messages
The Server sends a message to all
client participants, as soon as the
message is received by the server.
When a client sends a message, it
doesn’t appear in the incoming
message display area until the
server sends it to everyone.
outgoing message
Big Question: HOW do you get messages from the server?
Should be easy; when you set up the networking make an input stream as well
(probably a BufferedReader). Then read messages using readLine().
Bigger Question: WHEN do you get messages from the server?
Think about that. What are the options?
1Option One: Poll the server every 20 seconds
Pros: Well, it’s do-able
Cons: How does the server know what you’ve seen and what you haven’t? The server
would have to store the messages, rather than just doing a distribute-and-forget each time
it gets one. And why 20 seconds? A delay like this affects usability, but as you reduce the
delay, you risk hitting your server needlessly. Inefficient.
2Option Two: Read something in from the server each time the user
sends a message.
Pros: Do-able, very easy
Cons: Stupid. Why choose such an arbitrary time to check for messages? What if a user is
a lurker and doesn’t send anything?
488
3 Option Three: Read messages as soon as they’re sent from the server
Pros: Most efficient, best usability
Cons: How do you do two things at the same time? Where would you put this code? You’d
need a loop somewhere that was always waiting to read from the server. But where would
that go? Once you launch the GUI, nothing happens until an event is fired by a GUI
component.
chapter 15
networking and threads
In Java you really CAN
walk and chew gum at
the same time.
You know by now that we’re
going with option three.
We want something to run continuously,
checking for messages from the server, but
without interrupting the user’s ability to interact
with the GUI! So while the user is happily
typing new messages or scrolling through
the incoming messages, we want something
behind the scenes to keep reading in new input
from the server.
That means we finally need a new thread. A
new, separate stack.
We want everything we did in the Send-Only
version (version one) to work the same way,
while a new process runs along side that reads
information from the server and displays it in
the incoming text area.
Well, not quite. Unless you have multiple
processors on your computer, each new Java
thread is not actually a separate process
running on the OS. But it almost feels as
though it is.
Multithreading in Java
Java has multiple threading built right into
the fabric of the language. And it’s a snap to
make a new thread of execution:
Thread t = new Thread();
t.start();
That’s it. By creating a new Thread object,
you’ve launched a separate thread of execution,
with its very own call stack.
Except for one problem.
That thread doesn’t actually do anything,
so the thread “dies” virtually the instant
it’s born. When a thread dies, its new stack
disappears again. End of story.
So we’re missing one key component—the
thread’s job. In other words, we need the
code that you want to have run by a separate
thread.
Multiple threading in Java means we have to
look at both the thread and the job that’s run
by the thread. And we’ll also have to look
at the Thread class in the java.lang package.
(Remember, java.lang is the package you get
imported for free, implicitly, and it’s where
the classes most fundamental to the language
live, including String and System.)
you are here�
 489
threads and Thread
Java has multiple threads but only
one Thread class
We can talk about thread with a lower-case ‘t’ and Thread
with a capital ‘T’. When you see thread, we’re talking
about a separate thread of execution. In other words,
a separate call stack. When you see Thread, think of
the Java naming convention. What, in Java, starts with a
capital letter? Classes and interfaces. In this case, Thread
is a class in the java.lang package. A Thread object
represents a thread of execution; you’ll create an instance of
class Thread each time you want to start up a new thread
of execution.
thread
x.baz(y.bar(x.foo(main()
)
)
)
doMore( )
go( )
doStuff( )
run( )
main thread
another thread
started by the code
A thread (lower-case ‘t’) is a separate thread of execution.
That means a separate call stack. Every Java application
starts up a main thread—the thread that puts the
main() method on the bottom of the stack. The JVM
is responsible for starting the main thread (and other
threads, as it chooses, including the garbage collection
thread). As a programmer, you can write code to start
other threads of your own.
A thread is a separate
‘thread of execution’.
In other words, a
separate call stack.
A Thread is a Java
class that represents
a thread.
To make a thread,
make a Thread.
Thread
voidvoidThread
join()
start()
static void sleep()
java.lang.Thread
class
Thread (capital ‘T’) is a class that
represents a thread of execution.
It has methods for starting a
thread, joining one thread with
another, and putting a thread to
sleep. (It has more methods; these
are just the crucial ones we need
to use now).
490
 chapter 15
networkingWhat does it mean to have more than
one call stack?
With more than one call stack, you get the appearance of having
multiple things happen at the same time. In reality, only a true
multiprocessor system can actually do more than one thing at a
time, but with Java threads, it can appear that you’re doing several
things simultaneously. In other words, execution can move back
and forth between stacks so rapidly that you feel as though all stacks
are executing at the same time. Remember, Java is just a process
running on your underlying OS. So first, Java itself has to be ‘the
currently executing process’ on the OS. But once Java gets its
turn to execute, exactly what does the JVM run? Which bytecodes
execute? Whatever is on the top of the currently-running stack!
And in 100 milliseconds, the currently executing code might switch
to a different method on a different stack.
One of the things a thread must do is keep track of which statement
(of which method) is currently executing on the thread’s stack.
It might look something like this:
the active thread
1
 The JVM calls the main() method.
public static void main(String[] args) {
...
}
main()
main thread
2
 main() starts a new thread. The main
thread is temporarily frozen while the new
t.start()
thread starts running.
main()
Runnable r = new MyThreadJob();
 main thread
Thread t = new Thread(r); you’ll lear
n wha
 t
t.start();
 this means in just
Dog d = new Dog();
 a moment...the activ
e thre
ad again
3
 The JVM switches between the new
 Dog()
thread (user thread A) and the original
 main()
main thread, until both threads complete.
main thread
and threads
a new thread starts
and becomes the active
thread
run( )
user thread A
x.go()
run( )
user thread A
you are here�
 491
launching a thread
How to launch a new thread:
1
Make a Runnable object (the thread’s job)
Runnable threadJob = new MyRunnable();
Runnable is an interface you’ll learn about on the next page.
You’ll write a class that implements the Runnable interface,
and that class is where you’ll define the work that a thread
will perform. In other words, the method that will be run
from the thread’s new call stack.
R
 un
nablet
ce
 j ob
2
Make a Thread object (the worker) and
give it a Runnable (the job)
Thread myThread = new Thread(threadJob);
Pass the new Runnable object to the Thread constructor.
This tells the new Thread object which method to put on
the bottom of the new stack—the Runnable’s run() method.
Th
readt
obje
 c3
Start the Thread
myThread.start();
Nothing happens until you call the Thread’s
start() method. That’s when you go from
having just a Thread instance to having a new
thread of execution. When the new thread
starts up, it takes the Runnable object’s
run() method and puts it on the bottom of
the new thread’s stack.
Th
readt
obje
 crun()
Ru
nna
ble c
 e j ob
 t
492
 chapter 15
Every Thread needs a job to do.
A method to put on the new thread stack.
networking and threads
All I need is a real job.
Just give me a Runnable
and I’ll get to work!
Runnable is to a
Thread what a job is to
a worker. A Runnable
is the job a thread is
supposed to run.
A Runnable holds the
method that goes on
the bottom of the new
thread’s stack: run().
Thread
A Thread object needs a job. A job the thread will run when the
like the thread new this:
 is public }
 thread’s started. // code void stack, That that job run() and is will it actually must {
 be always run the first by be a method the method new that thread
 that goes looks
 on
 The method, interface of Runn
 wheth
 able public er so inter the you void
 type method f
 a
 run(). c
 e
 it d
e
 in f
 is (Remember ines t
 p hat ublic only way.)
 re
 o g
 , ne
 ardless
 it’s an
How does the thread know which method to put at the bottom of
 .
the stack? Because Runnable defines a contract. Because Runnable
is an interface. A thread’s job can be defined in any class that
implements the Runnable interface. The thread cares only that you
pass the Thread constructor an object of a class that implements
Runnable.
When you pass a Runnable to a Thread constructor, you’re really
just giving the Thread a way to get to a run() method. You’re giving
the Thread its job to do.
you are here�
 493
Runnable interface
To make a job for your thread,
implement the Runnable interface
 Runna so you b
le
 don
 is
 ’t in
need t
h
e
 java.lang to import
 pa it ck . ag
e,
public class MyRunnable implements Runnable {
public void run() {
go();
 Runnable has only one met
hod to
}
 implement: public void run() (wi th
 no
2
 }
 public doMore();
 void go() {
 of JOB is arguments). the the the method new thread stack.
 This that is is supposed where goes atyou the to put
 run. bottom
 the
 This
public void doMore() {
System.out.println(“top o’ the stack”);
}
}
class public Thread ThreadTester Runnable static myThread threadJob void {
 main = new = (String[] new Thread(threadJob);
 MyRunnable();
 args) {
 Pass Thread what stack. the th
 new method e
 In constr ne
 thread
 othe w
 Ru
 uctor. to
 r nn
 wo will pu ab
 rd
s, t le
 This run.
 on
 in
 the th
 stance tells e
 first bott
om the into method
 thre
 the of ad
 the new
 that
 new
1
 myThread .start();
}
 }
 System.out.println(“back in main”);
 but it’s not You call just it really start() won’t won’t a Thread a get on thread have a the new any instanc Th
 un
til
 thre
 real read e, yo
u
 ad ‘threadness’.
 like instance. of start any execution it. other A Before thread until object,
 that,
 is
 you
1
myThread.start()
main( )
main thread
494
 chapter 15
2
doMore( )
go()
run( )
new thread
br Brain
 ain barbell
 Barbell
What do you think the output will be if you run the
ThreadTester class? (we’ll find out in a few pages)
networking and threads
The three states of a new thread
Thread t = new Thread(r);
NEW
t.start();
RUNNABLE
Selected to run
“I’m waiting to
get started.”
“I’m good to go!”
this is where a
 thread
wants to be
!RUNNING
“Can I
supersize
that for
you?”
Thread t = new Thread(r);
A Thread instance has been
created but not started.
In other words, there is a
Thread object, but no thread
of execution.
t.start();
When you start the thread, it
moves into the runnable state.
This means the thread is ready
to run and just waiting for its
Big Chance to be selected for
execution. At this point, there is
a new call stack for this thread.
This is the state all threads lust
after! To be The Chosen One.
The Currently Running Thread.
Only the JVM thread scheduler
can make that decision. You
can sometimes influence that
decision, but you cannot force a
thread to move from runnable
to running. In the running
state, a thread (and ONLY this
thread) has an active call stack,
and the method on the top of
the stack is executing.
But there’s more. Once the thread becomes
runnable, it can move back and forth between
runnable, running, and an additional state:
temporarily not runnable (also known as ‘blocked’).
you are here�
 495
thread states
Typical runnable/running loop
Typically, a thread moves back and
forth between runnable and running,
as the JVM thread scheduler selects a
thread to run and then kicks it back
out so another thread gets a chance.
RUNNABLE
Selected to run
RUNNING
Sent back to runnable
so another thread can
have a chance
A thread can be made
temporarily not-runnable
The thread scheduler can move a
 RUNNABLE
running thread into a blocked state,
for a variety of reasons. For example,
the thread might be executing code
to read from a Socket input stream,
but there isn’t any data to read. The
scheduler will move the thread out
of the running state until something
becomes available. Or the executing
code might have told the thread to
put itself to sleep (sleep()). Or the
thread might be waiting because it
tried to call a method on an object,
and that object was ‘locked’. In that
case, the thread can’t continue until
the object’s lock is freed by the thread
that has it.
All of those conditions (and more)
cause a thread to become temporarily
not-runnable.
496
 chapter 15
RUNNING
ar
y
BLOCKED
 Se
 no nt t
 til o
 a it t ab c em an le le p
 a
 b or s e
 ta in.
 t
e
 e
com
unnn-r ga
un nnab
ru
sleeping, waiting for another thread to finish,
waiting for data to be available on the stream,
waiting for an object’s lock...
The Thread Scheduler
The thread scheduler makes all the decisions about
who moves from runnable to running, and about when
(and under what circumstances) a thread leaves the
running state. The scheduler decides who runs, and for
how long, and where the threads go when the scheduler
decides to kick them out of the currently-running state.
You can’t control the scheduler. There is no API for
calling methods on the scheduler. Most importantly,
there are no guarantees about scheduling! (There are a
few almost-guarantees, but even those are a little fuzzy.)
The bottom line is this: do not base your program’s
correctness on the scheduler working in a particular way!
The scheduler implementations are different for
different JVM’s, and even running the same program
on the same machine can give you different results.
One of the worst mistakes new Java programmers
make is to test their multi-threaded program on a
single machine, and assume the thread scheduler will
always work that way, regardless of where the program
runs.
So what does this mean for write-once-run-anywhere?
It means that to write platform-independent Java code,
your multi-threaded program must work no matter how
the thread scheduler behaves. That means that you can’t
be dependent on, for example, the scheduler making
sure all the threads take nice, perfectly fair and equal
turns at the running state. Although highly unlikely
today, your program might end up running on a JVM
with a scheduler that says, “OK thread five, you’re up,
and as far as I’m concerned, you can stay here until
you’re done, when your run() method completes.”
The secret to almost everything is sleep. That’s
right, sleep. Putting a thread to sleep, even for a few
milliseconds, forces the currently-running thread to
leave the running state, thus giving another thread a
chance to run. The thread’s sleep() method does come
with one guarantee: a sleeping thread will not become
the currently-running thread before the the length of
its sleep time has expired. For example, if you tell your
thread to sleep for two seconds (2,000 milliseconds),
that thread can never become the running thread again
until sometime after the two seconds have passed.
networking and threads
Number four, you’ve had
enough time. Back to runnable.
Number two, looks like you’re up!
Oh, now it looks like you’re gonna have
to sleep. Number five, come take his
place. Number two, you’re still
sleeping...
The thread scheduler
makes all the
decisions about who
runs and who doesn’t.
He usually makes the
threads take turns,
nicely. But there’s
no guarantee about
that. He might let
one thread run to its
heart’s content while
the other threads
‘starve’.
you are here�
 497
thread scheduling
An example of how unpredictable the
scheduler can be...
Running this code on one machine:
public class MyRunnable implements Runnable {
public void run() {
go();
}
public void go() {
doMore();
}
public void doMore() {
System.out.println(“top o’ the stack”);
}
}
class ThreadTestDrive {
public static void main (String[] args) {
Runnable threadJob = new MyRunnable();
Thread myThread = new Thread(threadJob);
myThread.start();
System.out.println(“back in main”);
}
}
 Notice rando finishes thread m
 ho ly
 f
 f . w inishes irst, Sometimes the and order first.
 sometime
 the chan
 new ges
 s
 t
 t h h e re m aain
 d
Produced this output:
File Edit Window Help PickMe
% java ThreadTestDrive
back in main
top o’ the stack
% java ThreadTestDrive
top o’ the stack
back in main
% java ThreadTestDrive
top o’ the stack
back in main
% java ThreadTestDrive
top o’ the stack
back in main
% java ThreadTestDrive
top o’ the stack
back in main
% java ThreadTestDrive
top o’ the stack
back in main
% java ThreadTestDrive
back in main
top o’ the stack
498
 chapter 15
How did we end up with different results?
networking and threads
Sometimes it runs like this:
main() starts the
new thread
myThread.start()
main()
main thread
The scheduler sends
the main thread out
of running and back
to runnable, so that
the new thread can
run.
myThread.start()
main()
main thread
The scheduler lets
the new thread
run to completion,
printing out “top o’
the stack”
doMore( )
go()
run( )
new thread
The new thread goes
away, because its run()
completed. The main
thread once again
becomes the running
thread, and prints “back
in main”
main( )
main thread
time
And sometimes it runs like this:
main() starts the
new thread
The scheduler sends
the main thread out
of running and back
to runnable, so that
the new thread can
run.
The scheduler lets thenew thread run for a
little while, not long
enough for the run()
method to complete.
The scheduler
sends the new
thread back to
runnable.
myThread.start()
 myThread.start()
 go()
 go()
main()
 main()
 run()
 run( )
main thread
 main thread
 new thread
 new thread
The scheduler
selects the main
thread to be the
running thread
again. Main prints
out “back in main”
main( )
main thread
The new thread returns
to the running state
and prints out “top o’
the stack”.
doMore( )
go( )
run( )
new thread
time
you are here�
 499
socket connections
Dumb there are Questions
 no
Q:
 I’ve seen examples that don’t use a separate
Runnable implementation, but instead just make a
subclass of Thread and override the Thread’s run()
method. That way, you call the Thread’s no-arg
constructor when you make the new thread;
Thread t = new Thread(); // no Runnable
A:
 Yes, that is another way of making your own
thread, but think about it from an OO perspective.
What’s the purpose of subclassing? Remember that
we’re talking about two different things here—the
Thread and the thread’s job. From an OO view, those
two are very separate activities, and belong in separate
classes. The only time you want to subclass/extend
the Thread class, is if you are making a new and more
specific type of Thread. In other words, if you think of
the Thread as the worker, don’t extend the Thread class
unless you need more specific worker behaviors. But if
all you need is a new job to be run by a Thread/worker,
then implement Runnable in a separate, job-specific
(not worker-specific) class.
This is a design issue and not a performance or
language issue. It’s perfectly legal to subclass Thread
and override the run() method, but it’s rarely a good
idea.
Q:
 Can you reuse a Thread object? Can you give it
a new job to do and then restart it by calling start()
again?
A:
 No. Once a thread’s run() method has
completed, the thread can never be restarted. In fact,
at that point the thread moves into a state we haven’t
talked about—dead. In the dead state, the thread has
finished its run() method and can never be restarted.
The Thread object might still be on the heap, as a
living object that you can call other methods on (if
appropriate), but the Thread object has permanently lost
its ‘threadness’. In other words, there is no longer a separate
call stack, and the Thread object is no longer a thread. It’s
just an object, at that point, like all other objects.
But, there are design patterns for making a pool of
threads that you can keep using to perform different
jobs. But you don’t do it by restarting() a dead thread.
500
 chapter 15
�����������BULLET POINTS
A thread with a lower-case ‘t’ is a separate thread of
execution in Java.
Every thread in Java has its own call stack.
A Thread with a capital ‘T’ is the java.lang.Thread
class. A Thread object represents a thread of
execution.
A Thread needs a job to do. A Thread’s job is an
instance of something that implements the Runnable
interface.
The Runnable interface has just a single method, run().
This is the method that goes on the bottom of the new
call stack. In other words, it is the first method to run in
the new thread.
To launch a new thread, you need a Runnable to pass
to the Thread’s constructor.
A thread is in the NEW state when you have
instantiated a Thread object but have not yet called
start().
When you start a thread (by calling the Thread object’s
start() method), a new stack is created, with the
Runnable’s run() method on the bottom of the stack.
The thread is now in the RUNNABLE state, waiting to
be chosen to run.
A thread is said to be RUNNING when the JVM’s
thread scheduler has selected it to be the currently-
running thread. On a single-processor machine, there
can be only one currently-running thread.
Sometimes a thread can be moved from the RUNNING
state to a BLOCKED (temporarily non-runnable) state.
A thread might be blocked because it’s waiting for data
from a stream, or because it has gone to sleep, or
because it is waiting for an object’s lock.
Thread scheduling is not guaranteed to work in any
particular way, so you cannot be certain that threads
will take turns nicely. You can help influence turn-taking
by putting your threads to sleep periodically.
networking and threads
Putting a thread to sleep
One of the best ways to help your threads take turns is
to put them to sleep periodically. All you need to do
is call the static sleep() method, passing it the sleep
duration, in milliseconds.
For example:
Thread.sleep(2000);
will knock a thread out of the running state, and
keep it out of the runnable state for two seconds.
The thread can’t become the running thread again
until after at least two seconds have passed.
A bit unfortunately, the sleep method throws an
InterruptedException, a checked exception, so all
calls to sleep must be wrapped in a try/catch (or
declared). So a sleep call really looks like this:
try {
Thread.sleep(2000);
} catch(InterruptedException ex) {
ex.printStackTrace();
}
Your thread will probably never be interrupted from
sleep; the exception is in the API to support a thread
communication mechanism that almost nobody uses in
the Real World. But, you still have to obey the handle
or declare law, so you need to get used to wrapping your
sleep() calls in a try/catch.
Now you know that your thread won’t wake up before the
specified duration, but is it possible that it will wake up
some time after the ‘timer’ has expired? Yes and no. It
doesn’t matter, really, because when the thread wakes
up, it always goes back to the runnable state! The thread
won’t automatically wake up at the designated time
and become the currently-running thread. When a
thread wakes up, the thread is once again at the mercy
of the thread scheduler. Now, for applications that
don’t require perfect timing, and that have only a few
threads, it might appear as though the thread wakes up
and resumes running right on schedule (say, after the
2000 milliseconds). But don’t bet your program on it.
Put your thread to sleep
if you want to be sure
that other threads get a
chance to run.
When the thread wakes
up, it always goes back
to the runnable state
and waits for the thread
scheduler to choose it
to run again.
you are here�
 501
using Thread.sleep()
Using sleep to make our program
more predictable.
Remember our earlier example that kept giving us different
results each time we ran it? Look back and study the code
and the sample output. Sometimes main had to wait until the
new thread finished (and printed “top o’ the stack”), while
other times the new thread would be sent back to runnable
before it was finished, allowing the main thread to come back
in and print out “back in main”. How can we fix that? Stop
for a moment and answer this question: “Where can you put
a sleep() call, to make sure that “back in main” always prints
before “top o’ the stack”?
We’ll wait while you work out an answer (there’s more than
one answer that would work).
Figure it out?
public class MyRunnable implements Runnable {
public void run() {
go();
}
public void go() {
try {
Thread.sleep(2000);
} catch(InterruptedException ex) {
ex.printStackTrace();
}
doMore();
}
public void doMore() {
System.out.println(“top o’ the stack”);
}
}
class ThreadTestDrive {
public static void main (String[] args) {
Runnable theJob = new MyRunnable();
Thread t = new Thread(theJob);
t.start();
System.out.println(“back in main”);
}
}
502
 chapter 15
This is what we want—a consistent order
of print statements:
File Edit Window Help SnoozeButton
% java ThreadTestDrive
back in main
top o’ the stack
% java ThreadTestDrive
back in main
top o’ the stack
% java ThreadTestDrive
back in main
top o’ the stack
% java ThreadTestDrive
back in main
top o’ the stack
% java ThreadTestDrive
back in main
top o’ the stack
Call
 thread in
g
 sl
 to e
e
p
 lea
 h
e
 ve re
 the willcurre
 force n
t t ly h - e running
 new
state!
 Th
 currently-r out pause get prints e
 m
 “back to a
(fo in
 out
 th t
 r h
 in is re
 about un “top line, main”. a
 ning d will o’ which two threa
 Then the becom seconds) calls d
 stack”
 there a e gain, the
 doM
 b w and e ore() il f l ore be print
 and
 we
 a
networking and threads
Making and starting t wo threads
Threads have names. You can give your threads a name of
your choosing, or you can accept their default names. But the
cool thing about names is that you can use them to tell which
thread is running. The following example starts two threads.
Each thread has the same job: run in a loop, printing the
currently-running thread’s name with each iteration.
public class RunThreads implements Runnable {
public static void main(String[] args) {
 Make one R
unna
ble instance
.
RunThreads runner = new RunThreads();
Thread beta.setName(“Beta alpha.setName(“Alpha Thread alpha beta ==new newThread(runner);
 thread”);
 Thread(runner);
 thread”);
 Make and
 same one job--we’ll two Runnable” threads, talk in with more a few the about pages).
 same the Runnable “two threads
 (the
alpha.start();
 N
am
e the threads.
beta.start();
 Start the threads.
}
 public void run() {
 E
 prin
 ac
h
 ting th
read its name will run ea
ch th ti ro me.
 ugh this loop,
for (int i = 0; i < 25; i++) {String threadName = Thread.currentThread().getName();
System.out.println(threadName + “ is running”);
}
}
}
 File Edit Window Help Centauri
Part of the output when
 Alpha thread is running
the loop iterates 25
 Alpha thread is running
What will happen?
 times.
 Alpha thread is running
Beta thread is running
Will the threads take turns? Will you see the thread names
 Alpha thread is running
alternating? How often will they switch? With each iteration?
 Beta thread is running
After five iterations?
 Beta thread is running
You already know the answer: we don’t know! It’s up to the
 Beta thread is running
scheduler. And on your OS, with your particular JVM, on
 Beta thread is running
your CPU, you might get very different results.
 Beta thread is running
Beta thread is running
Running under OS X 10.2 (Jaguar), with five or fewer
 Beta thread is running
iterations, the Alpha thread runs to completion, then
 Beta thread is running
the Beta thread runs to completion. Very consistent. Not
 Beta thread is running
guaranteed, but very consistent.
 Beta thread is running
But when you up the loop to 25 or more iterations, things
 Beta thread is running
start to wobble. The Alpha thread might not get to complete
 Beta thread is running
all 25 iterations before the scheduler sends it back to
 Beta thread is running
runnable to let the Beta thread have a chance.
 Beta thread is running
Alpha thread is running
you are here�
 503
aren’t threads wonderful?
Wow! Threads are
the greatest thing since the
MINI Cooper! I can’t think
of a single downside to using
threads, can you?
Um, yes. There IS a dark side.
Threads can lead to concurrency ‘issues’.
Concurrency issues lead to race conditions. Race conditions
lead to data corruption. Data corruption leads to fear... you
know the rest.
It all comes down to one potentially deadly scenario: two or
more threads have access to a single object’s data. In other
words, methods executing on two different stacks are both
calling, say, getters or setters on a single object on the heap.
It’s a whole ‘left-hand-doesn’t-know-what-the-right-hand-
is-doing’ thing. Two threads, without a care in the world,
humming along executing their methods, each thread
thinking that he is the One True Thread. The only one
that matters. After all, when a thread is not running, and in
runnable (or blocked) it’s essentially knocked unconscious.
When it becomes the currently-running thread again, it doesn’t
know that it ever stopped.
504
 chapter 15
networking and threads
Marriage in Trouble.
Can this couple be saved?
Next, on a very special Dr.Steve Show
[Transcript from episode #42]
Welcome to the Dr. Steve show.
We’ve got a story today that’s centered around the top two reasons why
couples split up—finances and sleep.
Today’s troubled pair, Ryan and Monica, share a bed and a
bank account. But not for long if we can’t find a solution. The
problem? The classic “two people—one bank account” thing.
Here’s how Monica described it to me:
“Ryan and I agreed that neither of us will overdraw the checking account.
So the procedure is, whoever wants to withdraw money must check the
balance in the account before making the withdrawal. It all seemed so
simple. But suddenly we’re bouncing checks and getting hit with overdraft
fees!
I thought it wasn’t possible, I thought our procedure was safe. But then
this happened:
Ryan needed $50, so he checked the balance in the account,
saw that it was $100. No problem. So, he plans to withdraw
the money. But first he falls asleep!
and
And that’s where I come in, while Ryan’s still asleep,
and now I want to withdraw $100. I check the balance,
and it’s $100 (because Ryan’s still asleep and hasn’t
 yet
made his withdrawal), so I think, no problem. So I make
the withdrawal, and again no problem. But then Ryan wakes up,
completes his withdrawal, and we’re suddenly overdrawn! He didn’t
even know that he fell asleep, so he just went ahead and completed his
transaction without checking the balance again. You’ve got to help us Dr.
Steve!”
Is there a solution? Are they doomed? We can’t stop Ryan from falling
asleep, but can we make sure that Monica can’t get her hands on the bank
account until after he wakes up?
Take a moment and think about that while we go to a commercial break.
of Ryan the and “two Monica: people, vi
ctims
 one
account” problem.Ryan falls asleep after
he checks the balance
but withdrawal. before he When makes he the
 wakes
up, he immediately makes
the checking withdrawl the balanc
 witho e ut
 again.
you are here�
 505
Ryan and Monica code
The Ryan and Monica problem, in code
The following example shows what can happen when two
threads (Ryan and Monica) share a single object (the bank
account).
The code has two classes, BankAccount, and
MonicaAndRyanJob. The MonicaAndRyanJob class
implements Runnable, and represents the behavior that Ryan
and Monica both have—checking the balance and making
withdrawals. But of course, each thread falls asleep in between
checking the balance and actually making the withdrawal.
The MonicaAndRyanJob class has an instance variable of type
BankAccount., that represents their shared account.
The code works like this:
BankAccount
int balance
getBalance()
withdraw()
Runnable
RyanAndMonicaJob
BankAccount account
run()
makeWithdrawal()
1
2
3
4
506
Make one instance of RyanAndMonicaJob.
The RyanAndMonicaJob class is the Runnable (the job to do),
and since both Monica and Ryan do the same thing (check
balance and withdraw money), we need only one instance.
 In the run() method, do
RyanAndMonicaJob
 theJob = new RyanAndMonicaJob();
 exactly what Ryan and
Monica would do—check
the balance and, if
Make two threads with the same Runnable
 there’s enough money,
(the RyanAndMonicaJob instance)
 make the withdrawal.
Thread one = new Thread(theJob);
 This should protect
Thread two = new Thread(theJob);
 against overdrawing the
account.
Name and start the threads
Except... Ryan and
one.setName(“Ryan”);
 Monica always fall
two.setName(“Monica”);
 asleep after they
one.start();
two.start();
 check the balance but
before they finish the
withdrawal.
Watch both threads execute the run() method
(check the balance and make a withdrawal)
One thread represents Ryan, the other represents Monica.
Both threads continually check the balance and then make a
withdrawal, but only if it’s safe!
if (account.getBalance() >= amount) {
try {
Thread.sleep(500);
} catch(InterruptedException ex) {ex.printStackTrace(); }
}
chapter 15
networking and threads
The Ryan and Monica example
class private BankAccount int balance {
 = 100;
 The account starts with a
balance of $10
0.
public int getBalance() {
return balance;
}
public void withdraw(int amount) {
balance = balance - amount;
}
 public }
 private class BankAccount RyanAndMonicaJob account = implements new BankAccount();
 Runnable {
 There RyanAndMonic ONE threads instance willwill be
 o
 a
 n
 ccess aJob.That of ly
 the O
N
 this E
 bank in
 one stance means account. account.
 only
 of the
 Both
public static void main (String [] args) {
RyanAndMonicaJob Thread one = new Thread(theJob);
 theJob = new RyanAndMonicaJob();
 Instantiate the
 Runnable (job)
Thread two.start();
 one.start();
 two.setName(“Monica”);
 one.setName(“Ryan”);
 two = new Thread(theJob);
 account job. Make That two instance means threa
ds, both variable giving threads in each the
 will thread Runnable be accessin the class. sa
g
 me the Runnable
 one
}
public forif makeWithdrawal(10);
 (int System.out.println(“Overdrawn!”); (account.getBalance() void x = run() 0; x {
 < 10; x++)<{
0) {
 In
 to withdrawal, the th
 make e
 account ru
n( a withdra ) it method, is chec over
 wal ks drawn.
 a the with
 thre balance each ad lo
ops iteration. once through again After to and see tr
 the
 ies
 if
}
 }
 }
 Check enough
 enough, the money, we account go to we sleep, just balance, print then and a wake messag
 if up
 there’s e. and If complete
 there not
 IS
private if (account.getBalance() void makeWithdrawal(int >= amount) amount) {
 {
 the withdrawal, just like Ryan did.
System.out.println(Thread.currentThread().getName() + “ is about to withdraw”);
try {
System.out.println(Thread.currentThread().getName() + “ is going to sleep”);
Thread.sleep(500);
} catch(InterruptedException ex) {ex.printStackTrace(); }
System.out.println(Thread.currentThread().getName() + “ woke up.”);
account.withdraw(amount);
System.out.println(Thread.currentThread().getName() + “ completes the withdrawal”);
}
else {
System.out.println(“Sorry, not enough for “ + Thread.currentThread().getName());
}
 }
 }
 see We put what’s in a happening bunch of as print
 it
 runs.
 statements so
 we can
you are here�
 507
Ryan and Monica output
How did this
happen?
File Edit Window Help Visa
Ryan is about to withdraw
Ryan is going to sleep
Monica woke up.
Monica completes the withdrawl
Monica is about to withdraw
Monica is going to sleep
Ryan woke up.
Ryan completes the withdrawl
Ryan is about to withdraw
Ryan is going to sleep
Monica woke up.
Monica completes the withdrawl
Monica is about to withdraw
Monica is going to sleep
Ryan woke up.
Ryan completes the withdrawl
Ryan is about to withdraw
Ryan is going to sleep
Monica woke up.
Monica completes the withdrawl
Sorry, not enough for Monica
Sorry, not enough for Monica
Sorry, not enough for Monica
Sorry, not enough for Monica
Sorry, not enough for Monica
Ryan woke up.
Ryan completes the withdrawl
Overdrawn!
Sorry, not enough for Ryan
Overdrawn!
Sorry, not enough for Ryan
Overdrawn!
Sorry, not enough for Ryan
Overdrawn!
The makeWithdrawal() method
always checks the balance
before making a withdrawal,
but still we overdraw the
account.
Here’s one scenario:
Ryan checks the balance, sees that
there’s enough money, and then falls
asleep.
Meanwhile, Monica comes in and checks
the balance. She, too, sees that there’s
enough money. She has no idea that
Ryan is going to wake up and complete a
withdrawal.
Monica falls asleep.
Ryan wakes up and completes his
withdrawal.
Monica wakes up and completes her
withdrawal. Big Problem! In between the
time when she checked the balance and
made the withdrawal, Ryan woke up and
pulled money from the account.
Monica’s check of the account was
not valid, because Ryan had already
checked and was still in the middle of
making a withdrawal.
Monica must be stopped from getting
into the account until Ryan wakes up and
finishes his transaction. And vice-versa.
508
 chapter 15
They need a lock for account access!
The lock works like this:
1There’s a lock associated with the bank
account transaction (checking the balance
and withdrawing money). There’s only
one key, and it stays with the lock until
somebody wants to access the account.
2
When Ryan wants to access the bank
account (to check the balance and withdraw
money), he locks the lock and puts the key
in his pocket. Now nobody else can access
the account, since the key is gone.
3
Ryan keeps the key in his pocket until he
finishes the transaction. He has the only
key, so Monica can’t access the account
(or the checkbook) until Ryan unlocks the
account and returns the key.
Now, even if Ryan falls asleep after he
checks the balance, he has a guarantee
that the balance will be the same when he
wakes up, because he kept the key while he
was asleep!
networking and threads
The bank account
transaction is
unlocked when
nobody is using
the account.
When Ryan
wants to access
the account, he
secures the lock
and takes the key.
When Ryan is
finished, he
unlocks the lock
and returns the
key. Now the key
is available for
Monica (or Ryan
again) to access
the account.
you are here�
 509
using synchronized
We need the makeWithdrawal ( ) method
to run as one atomic thing.
We need to make sure that once a thread enters the
makeWithdrawal() method, it must be allowed to finish the method
before any other thread can enter.
In other words, we need to make sure that once a thread has
checked the account balance, that thread has a guarantee that it can
wake up and finish the withdrawal before any other thread can check the
account balance!
Use the synchronized keyword to modify a method so that only
one thread at a time can access it.
That’s how you protect the bank account! You don’t put a lock on
the bank account itself; you lock the method that does the banking
transaction. That way, one thread gets to complete the whole
transaction, start to finish, even if that thread falls asleep in the
middle of the method!
So if you don’t lock the bank account, then what exactly is locked? Is
it the method? The Runnable object? The thread itself?
We’ll look at that on the next page. In code, though, it’s quite
simple—just add the synchronized modifier to your method
declaration:
The synchronized
keyword means that
a thread needs a key
in order to access the
synchronized code.
To protect your data
(like the bank account),
synchronize the
methods that act on
that data.
private synchronized void makeWithdrawal(int amount) {
if (account.getBalance() >= amount) {
System.out.println(Thread.currentThread().getName() + “ is about to withdraw”);
try {
System.out.println(Thread.currentThread().getName() + “ is going to sleep”);
Thread.sleep(500);
} catch(InterruptedException ex) {ex.printStackTrace(); }
System.out.println(Thread.currentThread().getName() + “ woke up.”);
account.withdraw(amount);
System.out.println(Thread.currentThread().getName() + “ completes the withdrawl”);
} else {
System.out.println(“Sorry, not enough for “ + Thread.currentThread().getName());
}
}
510
(Note for you physics-savvy readers: yes, the convention of using the word ‘atomic’ here does not reflect
the whole subatomic particle thing. Think Newton, not Einstein, when you hear the word ‘atomic’ in the
context of threads or transactions. Hey, it’s not OUR convention. If WE were in charge, we’d apply
Heisenberg’s Uncertainty Principle to pretty much everything related to threads.)
chapter 15
Using an object’s lock
Every object has a lock. Most of the time, the
lock is unlocked, and you can imagine a virtual
key sitting with it. Object locks come into play
only when there are synchronized methods.
When an object has one or more synchronized
methods, a thread can enter a synchronized
method only if the thread can get the key to the
object’s lock!
The locks are not per method, they
are per object. If an object has two
synchronized methods, it does not
simply mean that you can’t have two
threads entering the same method. It
means you can’t have two threads entering
any of the synchronized methods.
Think about it. If you have multiple
methods that can potentially act on an
object’s instance variables, all those methods
need to be protected with synchronized.
The goal of synchronization is to protect
critical data. But remember, you don’t lock the
data itself, you synchronize the methods that
access that data.
So what happens when a thread is cranking
through its call stack (starting with the run()
method) and it suddenly hits a synchronized
method? The thread recognizes that it needs
a key for that object before it can enter the
method. It looks for the key (this is all handled
by the JVM; there’s no API in Java for accessing
object locks), and if the key is available, the
thread grabs the key and enters the method.
From that point forward, the thread hangs on
to that key like the thread’s life depends on
it. The thread won’t give up the key until it
completes the synchronized method. So while
that thread is holding the key, no other threads
can enter any of that object’s synchronized
methods, because the one key for that object
won’t be available.
networking and threads
Hey, this object’s
takeMoney() method is
synchronized. I need to get
this object’s key before I
can go in...
Every Java object has a lock.
A lock has only one key.
Most of the time, the lock is
unlocked and nobody cares.
But if an object has
synchronized methods, a
thread can enter one of the
synchronized methods ONLY
if the key for the object’s lock
is available. In other words,
only if another thread hasn’t
already grabbed the one key.
you are here�
 511
synchronization matters
The dreaded “Lost Update” problem
Here’s another classic concurrency problem, that comes from the database world. It’s
closely related to the Ryan and Monica story, but we’ll use this example to illustrate a few
more points.
The lost update revolves around one process:
Step 1: Get the balance in the account
Step int 2: i Add = 1 balance;
 to that balance
 not an at
o
mic proce
ss
Probably
balance = i + 1;
Even if we used the more common syntax: balance++; there is no guarantee that the
compiled bytecode will be an “atomic process”. In fact, it probably won’t.
In the “Lost Update” problem, we have two threads, both trying to increment the balance.
Take a look at the code, and then we’ll look at the real problem:
class TestSync implements Runnable {
private public for(int void int ibalance;
 run() = 0; i {
 < 50; i++) {
 each increm each t it
 h
 enting r
 eration
 e
a
d
 r
 the uns 50 bala
 t nce imes,
 on
increment();
System.out.println(“balance is “ + balance);
}
}
public void increment() {
int i = balance;
}
 }
 balance = i + 1;
 the TIME adding Here’s CURRENTva WE the 1 to READ crucia
 whate lue l IT ver part is)
 (rather the ! We value increment than of adding balance the 1was balanc
 to w A hatever
 T e THE
 by
public class TestSyncTest {
public static void main (String[] args) {
TestSync job = new TestSync();
Thread a = new Thread(job);
Thread b = new Thread(job);
a.start();
b.start();
}
}
512
 chapter 15
Let’s run this code...
1Thread A runs for awhile
Put the value of balance into variable i.
Balance is 0, so i is now 0.
Set the value of balance to the result of i + 1.
Now balance is 1.
A
 Put the value of balance into variable i.
Balance is 1, so i is now 1.
Set the value of balance to the result of i + 1.
Now balance is 2.
 B
2Thread B runs for awhile
Put the value of balance into variable i.
Balance is 2, so i is now 2.
Set the value of balance to the result of i + 1.
Now balance is 3.
B
 Put the value of balance into variable i.
Balance is 3, so i is now 3.
[now thread B is sent back to runnable,
before it sets the value of balance to 4]
3Thread A runs again, picking up where it left off
Put the value of balance into variable i.
Balance is 3, so i is now 3.
Set the value of balance to the result of i + 1.
Now balance is 4.
A
 Put the value of balance into variable i.
Balance is 4, so i is now 4.
Set the value of balance to the result of i + 1.
Now balance is 5.
4Thread B runs again, and picks up exactly where it left off!
Set the value of balance to the result of i + 1.
Now balance is 4.
 Yikes!!
B
 Thread A updated it to 5, but
now B came back and stepped
on top of the update A made,
as if A’s update never happened.
networking and threads
We lost the last updates
that Thread A made!
Thread B had previously
done a ‘read’ of the value
of balance, and when B
woke up, it just kept going
as if it never missed a beat.
you are here�
 513
synchronizing methods
Make the increment() method atomic.
Synchronize it!
Synchronizing the increment() method solves the “Lost
Update” problem, because it keeps the two steps in the method
as one unbreakable unit.
public synchronized void increment() {
int i = balance;
balance = i + 1;
}
Once a thread enters
the method, we have
to make sure that all
the steps in the method
complete (as one
atomic process) before
any other thread can
enter the method.
Dumb there are Questions
 no
Q:
 Sounds like it’s a good idea to synchronize
 B
everything, just to be thread-safe.
A:
 Nope, it’s not a good idea. Synchronization doesn’t
come for free. First, a synchronized method has a certain
amount of overhead. In other words, when code hits a
synchronized method, there’s going to be a performance hit
(although typically, you’d never notice it) while the matter of
“is the key available?” is resolved.
Second, a synchronized method can slow your program
down because synchronization restricts concurrency. In
other words, a synchronized method forces other threads to
get in line and wait their turn. This might not be a problem
in your code, but you have to consider it.
Third, and most frightening, synchronized methods can lead
to deadlock! (See page 516.)
A good rule of thumb is to synchronize only the bare
minimum that should be synchronized. And in fact, you
can synchronize at a granularity that’s even smaller than
a method. We don’t use it in the book, but you can use the
synchronized keyword to synchronize at the more fine-
grained level of one or more statements, rather than at the
whole-method level.
514
 chapter 15
public}
doStuff() doesn’t need to
be synchronized, so we don’tsynchronize the whole method
.
void go() {
doStuff();
synchronized(this) {
criticalStuff();
moreCriticalStuff();
}
object have rather synchronized into Now, to one only whose than provide atomic these
 in
 key keyword a an un tw the me it ar o th . gu me
 thread W
 od WITHIN ment he
 thod declaration, n you that needs calls a use method,
 is are to the
 the
 you
 get.
 grouped
lock object will Although almost if (this). the there always whole That’s are meth sync ot
 th hronize her od e same were ways on object synchronized.
 to the do current
 you’d
 it, you
networking and threads
1Thread A runs for awhile
Attempt to enter the increment() method.
A
The method is synchronized, so get the key for this object
Put the value of balance into variable i.
Balance is 0, so i is now 0.
Set the value of balance to the result of i + 1.
Now balance is 1.
Return the key (it completed the increment() method).
Re-enter the increment() method and get the key.
Put the value of balance into variable i.
Balance is 1, so i is now 1.
[now thread A is sent back to runnable, but since it has not
completed the synchronized method, Thread A keeps the key]
2 Thread B is selected to run
B
Attempt to enter the increment() method. The method is
synchronized, so we need to get the key.
The key is not available.
[now thread B is sent into a ‘object lock not available’ lounge]
3Thread A runs again, picking up where it left off
(remember, it still has the key)
Set the value of balance to the result of i + 1.
Now balance is 2.
A
 Return the key.
[now thread A is sent back to runnable, but since it
has completed the increment() method, the thread
does NOT hold on to the key]
4 Thread B is selected to run
B
Attempt to enter the increment() method. The method is
synchronized, so we need to get the key.
This time, the key IS available, get the key.
Put the value of balance into variable i.
[continues to run...]
you are here�
 515
thread deadlock
The deadly side of synchronization
Be careful when you use synchronized code, because nothing
will bring your program to its knees like thread deadlock.
Thread deadlock happens when you have two threads, both of
which are holding a key the other thread wants. There’s no way
out of this scenario, so the two threads will simply sit and wait.
And wait. And wait.
If you’re familiar with databases or other application servers,
you might recognize the problem; databases often have a
locking mechanism somewhat like synchronization. But a
real transaction management system can sometimes deal with
deadlock. It might assume, for example, that deadlock might
have occurred when two transactions are taking too long to
complete. But unlike Java, the application server can do a
“transaction rollback” that returns the state of the rolled-back
transaction to where it was before the transaction (the atomic
part) began.
Java has no mechanism to handle deadlock. It won’t even know
deadlock occurred. So it’s up to you to design carefully. If you
find yourself writing much multithreaded code, you might
want to study “Java Threads” by Scott Oaks and Henry Wong
for design tips on avoiding deadlock. One of the most common
tips is to pay attention to the order in which your threads are
started.
A simple deadlock scenario:
1
A
Thread A enters a
synchronized method
of object foo, and gets
the key.
foo
2
B
Thread B enters a
synchronized method
of object bar, and gets
the key.
bar
Thread A goes to
sleep, holding the
foo key.
A
foo
bar
B
Thread B tries to enter
a synchronized method
of object foo, but can’t
get that key (because
A has it). B goes to
the waiting lounge, until
the foo key is available.
B keeps the bar key.
516
 chapter 15
All it takes for
deadlock are two
objects and two
threads.
foo
bar
A
B
3
foo
A
Thread A wakes up (still
holding the foo key)
and tries to enter a
synchronized method on
object bar, but can’t get
that key because B has
it. A goes to the waiting
lounge, until the bar key is
available (it never will be!)
A
Thread A can’t run until
it can get the bar key,
but B is holding the bar
key and B can’t run until it
gets the foo key that A is
holding and...
networking and threads
�����������BULLET POINTS
The static Thread.sleep() method forces a thread to leave the
running state for at least the duration passed to the sleep method.
Thread.sleep(200) puts a thread to sleep for 200 milliseconds.
The sleep() method throws a checked exception (InterruptedException),
so all calls to sleep() must be wrapped in a try/catch, or declared.
You can use sleep() to help make sure all threads get a chance to run,
although there’s no guarantee that when a thread wakes up it’ll go to the
end of the runnable line. It might, for example, go right back to the front.
In most cases, appropriately-timed sleep() calls are all you need to keep
your threads switching nicely.
You can name a thread using the (yet another surprise) setName()
method. All threads get a default name, but giving them an explicit name
can help you keep track of threads, especially if you’re debugging with
print statements.
You can have serious problems with threads if two or more threads have
access to the same object on the heap.
Two or more threads accessing the same object can lead to data
corruption if one thread, for example, leaves the running state while still
in the middle of manipulating an object’s critical state.
To make your objects thread-safe, decide which statements should be
treated as one atomic process. In other words, decide which methods
must run to completion before another thread enters the same method
on the same object.
Use the keyword synchronized to modify a method declaration,
when you want to prevent two threads from entering that method.
Every object has a single lock, with a single key for that lock. Most of the
time we don’t care about that lock; locks come into play only when an
object has synchronized methods.
When a thread attempts to enter a synchronized method, the thread
must get the key for the object (the object whose method the thread
is trying to run). If the key is not available (because another thread
already has it), the thread goes into a kind of waiting lounge, until the key
becomes available.
Even if an object has more than one synchronized method, there is still
only one key. Once any thread has entered a synchronized method on
that object, no thread can enter any other synchronized method on the
same object. This restriction lets you protect your data by synchronizing
any method that manipulates the data.
you are here�
 517
final chat client
New and improved SimpleChatClient
Way back near the beginning of this chapter, we built the SimpleChatClient that could send
outgoing messages to the server but couldn’t receive anything. Remember? That’s how we
got onto this whole thread topic in the first place, because we needed a way to do two things
at once: send messages to the server (interacting with the GUI) while simultaneously reading
incoming messages from the server, displaying them in the scrolling text area.
import import import java.util.*;
 java.io.*;
 java.net.*;
 But Yes, end to not there this yet... really chap
ter.
 IS an
import javax.swing.*;
import java.awt.*;
import java.awt.event.*;
public class SimpleChatClient {
JTextArea incoming;
JTextField outgoing;
BufferedReader reader;
PrintWriter writer;
Socket sock;
public static void main(String[] args) {
public }
 SimpleChatClient client.go();
 void go() {
 client = new SimpleChatClient();
 Thi
 be
 highlighted new fo
 s
 is
 re
 ‘reader’ .
 m
 N
 os
 ot
 t
ly
 hi
 pa
 ng
 thr
 G
 rt U
 special I ead.
 where code exce you’ve
 we
 st pt ar seen
 t t he
 the
JFrame frame = new JFrame(“Ludicrously Simple Chat Client”);
JPanel mainPanel = new JPanel();
incoming = new JTextArea(15,50);
incoming.setLineWrap(true);
incoming.setWrapStyleWord(true);
incoming.setEditable(false);
JScrollPane qScroller = new JScrollPane(incoming);
qScroller.setVerticalScrollBarPolicy(ScrollPaneConstants.VERTICAL_SCROLLBAR_ALWAYS);
qScroller.setHorizontalScrollBarPolicy(ScrollPaneConstants.HORIZONTAL_SCROLLBAR_NEVER);
outgoing = new JTextField(20);
JButton sendButton = new JButton(“Send”);
sendButton.addActionListener(new SendButtonListener());
mainPanel.add(qScroller);
 frame.getContentPane().add(BorderLayout.CENTER, readerThread.start();
 mainPanel.add(outgoing);
 Thread setUpNetworking();
 mainPanel.add(sendButton);
 readerThread = new Thread(new IncomingReader());
 mainPanel); scrolling any so
ck
et
 to
 thre
ad. the using We’re re
ad incoming Runnable a starting stream, new The text from inner messa thread’s area.
 the (job) di
 a sp ge ne
 cla
ss
 server’s
 lay for
 s w ing
 in job th
read,
 as
 the
 the
 is
frame.setSize(800,500);
frame.setVisible(true);
} // close go
518
 chapter 15
networking and threads
private void setUpNetworking() {
try {
sock = new Socket(“127.0.0.1”, 5000);
InputStreamReader streamReader = new InputStreamReader(sock.getInputStream());
reader = new BufferedReader(streamReader);
} // } }
 close catch(IOException writer System.out.println(“networking ex.printStackTrace();
 setUpNetworking
 = new PrintWriter(sock.getOutputStream());
 ex) {
 established”);
 We’re and the but that messages output output
 now us the in
we’re
 g
 f
 ne t
 ro streams. stream he
 w m ‘r using so
 the eader’ ck
to et
 server.
 We the send to thr
 were input get ead to alre the the stream
 can ad
 in se get
 y pu rver,
 using
 t
 so
public class SendButtonListener implements ActionListener {
public void actionPerformed(ActionEvent ev) {
try {
writer.println(outgoing.getText());
} catch(Exception writer.flush();
 ex.printStackTrace();
 ex) {
 contents the Nothi
 send ng new button, of the here. text this When
 me
 field thod theto user sends the clic
ks
 server.
 the
}
outgoing.setText(“”);
outgoing.requestFocus();
}
}
 // close inner class
public public class try String void while } {
 IncomingReader // System.out.println(“read incoming.append(message run() message;
 close ((message {
while
 = implements reader.readLine()) +“ Runnable “\n”);
 + message);
 != {
 null) {
 This In
 loop the line to with th
 the is server (as e
 at a what ru
 a scrolli new
 long n(
 tim )
 is the
 m
 line as e not ng et
 and w ho
 thread character).
 text hat null), d, adding it it area reading stays does!!
 gets
 each (along
 fr
 in a
 om
 line
 a
} catch(Exception ex) {ex.printStackTrace();}
} // close run
} // close inner class
} // close outer class
you are here�
 519
chat server code
Ready-bake
Code
The really really simple Chat Ser ver
You can use this server code for both versions of the Chat Client. Every possible
disclaimer ever disclaimed is in effect here. To keep the code stripped down to the
bare essentials, we took out a lot of parts that you’d need to make this a real server.
In other words, it works, but there are at least a hundred ways to break it. If you
want a Really Good Sharpen Your Pencil for after you’ve finished this book, come
back and make this server code more robust.
Another possible Sharpen Your Pencil, that you could do right now, is to annotate
this code yourself. You’ll understand it much better if you work out what’s
happening than if we explained it to you. Then again, this is Ready-bake code,
so you really don’t have to understand it at all. It’s here just to support the two
versions of the Chat Client.
To run the chat client, you need two
terminals. First, launch this server
import java.io.*;
 from one terminal, then launch the
import java.net.*;
 client from another terminal
import java.util.*;
public class VerySimpleChatServer {
ArrayList clientOutputStreams;
public class ClientHandler implements Runnable {
BufferedReader reader;
Socket sock;
public ClientHandler(Socket clientSocket) {
try {
sock = clientSocket;
InputStreamReader isReader = new InputStreamReader(sock.getInputStream());
reader = new BufferedReader(isReader);
} catch(Exception ex) {ex.printStackTrace();}
} // close constructor
public void run() {
String message;
try {
while ((message = reader.readLine()) != null) {
System.out.println(“read “ + message);
tellEveryone(message);
} // close while
} catch(Exception ex) {ex.printStackTrace();}
} // close run
} // close inner class
520
 chapter 15
networking and threads
} //public static void main (String[] args) {
new VerySimpleChatServer().go();
}
public void go() {
clientOutputStreams = new ArrayList();
try {
ServerSocket serverSock = new ServerSocket(5000);
while(true) {
Socket clientSocket = serverSock.accept();
PrintWriter writer = new PrintWriter(clientSocket.getOutputStream());
clientOutputStreams.add(writer);
Thread t = new Thread(new ClientHandler(clientSocket));
t.start();
System.out.println(“got a connection”);
}
} catch(Exception ex) {
ex.printStackTrace();
}
} // close go
public void tellEveryone(String message) {
Iterator it = clientOutputStreams.iterator();
while(it.hasNext()) {
try {
PrintWriter writer = (PrintWriter) it.next();
writer.println(message);
writer.flush();
} catch(Exception ex) {
ex.printStackTrace();
}
} // end while
} // close tellEveryone
close class
you are here�
 521
synchronization questions
Dumb there are Questions
 no
Q:
 What about protecting static
variable state? If you have static
methods that change the static variable
state, can you still use synchronization?
A:
 Yes! Remember that static
methods run against the class and not
against an individual instance of the class.
So you might wonder whose object’s lock
would be used on a static method? After
all, there might not even be any instances
of that class. Fortunately, just as each
object has its own lock, each loaded class
has a lock. That means that if you have
three Dog objects on your heap, you have
a total of four Dog-related locks. Three
belonging to the three Dog instances,
and one belonging to the Dog class itself.
When you synchronize a static method,
Java uses the lock of the class itself. So if
you synchronize two static methods in a
single class, a thread will need the class
lock to enter either of the methods.
Q:
 What are thread priorities? I’ve
heard that’s a way you can control
scheduling.
A:
 Thread priorities might help
you influence the scheduler, but they
still don’t offer any guarantee. Thread
priorities are numerical values that tell
the scheduler (if it cares) how important a
thread is to you. In general, the scheduler
will kick a lower priority thread out of the
running state if a higher priority thread
suddenly becomes runnable. But... one
more time, say it with me now, “there
is no guarantee.” We recommend that
you use priorities only if you want to
influence performance, but never, ever
rely on them for program correctness.
522
 chapter 15
Q:
 Why don’t you just synchronize
all the getters and setters from the
class with the data you’re trying to
protect? Like, why couldn’t we have
synchronized just the checkBalance()
and withdraw() methods from class
BankAccount, instead of synchronizing
the makeWithdrawal() method from
the Runnable’s class?
A:
 Actually, we should have
synchronized those methods, to prevent
other threads from accessing those
methods in other ways. We didn’t
bother, because our example didn’t have
any other code accessing the account.
But synchronizing the getters
and setters (or in this case the
checkBalance() and withdraw()) isn’t
enough. Remember, the point of
synchronization is to make a specific
section of code work ATOMICALLY. In
other words, it’s not just the individual
methods we care about, it’s methods
that require more than one step to
complete! Think about it. If we had not
synchronized the makeWithdrawal()
method, Ryan would have checked the
balance (by calling the synchronized
checkBalance()), and then immediately
exited the method and returned the key!
Of course he would grab the key again,
after he wakes up, so that he can call
the synchronized withdraw() method,
but this still leaves us with the same
problem we had before synchronization!
Ryan can check the balance, go to sleep,
and Monica can come in and also check
the balance before Ryan has a chance to
wakes up and completes his withdrawal.
So synchronizing all the access methods
is probably a good idea, to prevent
other threads from getting in, but you
still need to synchronize the methods
that have statements that must execute
as one atomic unit.
Code Kitchen
networking and threads
dance beat
Andy: groove #2
Chris: groove2 revised
Nigel: dance beat
This is the last version of the BeatBox!
It connects to a simple MusicServer so that you can
send and receive beat patterns with other clients.
The code is really long, so the complete listing is
actually in Appendix A.
you
 the with pattern, r
 other m
 your e
s
s
a
g
 wh
 p c e
 layers, urrent en gets you sent along
 beat
 hit
 to
“sendIt”
‘Start’ with the players. incoming pattern it,to and Clic p me
 lay k then ssages that one it.
 click
 goes
 to from
 load
you are here�
 523
exercise: Code Magnets
Exercise
public class TestThreads {
Code Magnets
A working Java program is scrambled up on the fridge. Can
you add the code snippets on the next page to the empty
classes below, to make a working Java program that pro-
duces the output listed? Some of the curly braces fell on the
floor and they were too small to pick up, so feel free to add as
many of those as you need!
class ThreadOne
class Accum {
class ThreadTwo
Bonus Question: Why do you think we used the
modifiers we did in the Accum class?
524
 chapter 15
File Edit Window Help Sewing
% java TestThreads
one 98098
two 98099
networking and threads
Code Magnets, continued..
Accum a = Accum.getAccum();
(“two “+a.getCount
());
Thread one = new
 Thread(t1);
 System.out.println
dExcep
tion ex) { }
 ThreadTwo t2 = ne
w ThreadTwo();
 try {
} catch(Interrupte
return counter;
 counter += add;
Thread t
wo = new
 implements Runnable {
 one.start();
Thread(t
2);
Accum a = Accum.getAccum();
dException ex) {
 }
} catch(Interrupte
public static Accum getAccum() {
private static Ac
cum a = new Accum(
);
private int coun
ter = 0;
public void ru
n() {
a.updateCounter(
1);
 Thread.s
leep(50)
;
for(int x=0; x
 < 99; x++) {
 implements Runnable {
a.updateCounter(1000)
;
public i
nt return a;
getCo
unt() {
System.out.print
ln(“one “+a.getC
ount());
public void upda
teCounter(int ad
d) {
public static void main(String [] args) {
for(int x=0; x
 < 98; x++) {
 two.start();
try {
 public void ru
n() {
 private Accum() { }
 ThreadOne t1 = ne
w ThreadOne();
you are here�
 525
exercise solutions
public class TestThreads {
public static void main(String [] args) {
ThreadOne t1 = new ThreadOne();
ThreadTwo t2 = new ThreadTwo();
Thread one = new Thread(t1);
Thread two = new Thread(t2);
one.start();
two.start();
}
 }
 c
r
e
a
 t
e
 a
 st
atic instan
ce
of class A
ccum
class Accum {
private static Accum a = new Accum();
private int counter = 0;
private Accum() { }
 A private co
nstructor
public static Accum getAccum() {
return a;
}
public void updateCounter(int add) {
counter += add;
}
Exercise Solutions
Threads from two different classes are updating
the same object in a third class, because both
threads are accessing a single instance of Accum.
The line of code:
private static Accum a = new Accum( ); creates a
static instance of Accum (remember static means
one per class), and the private constructor in
Accum means that no one else can make an Accum
object. These two techniques (private constructor
and static getter method) used together, create
what’s known as a ‘Singleton’ - an OO pattern to
restrict the number of instances of an object
that can exist in an application. (Usually, there’s
just a single instance of a Singleton—hence the
name), but you can use the pattern to restrict the
instance creation in whatever way you choose.)
public int getCount() {
return counter;
}
}
class ThreadOne implements Runnable {
Accum a = Accum.getAccum();
public void run() {
for(int x=0; x < 98; x++) {
a.updateCounter(1000);
try {
Thread.sleep(50);
} catch(InterruptedException ex) { }
}
System.out.println(“one “+a.getCount());
}
}
class ThreadTwo implements Runnable {
Accum a = Accum.getAccum();
public void run() {
for(int x=0; x < 99; x++) {
a.updateCounter(1);
try {
Thread.sleep(50);
} catch(InterruptedException ex) { }
}
System.out.println(“two “+a.getCount());
}
}
526
 chapter 15
Five-Minute
Mystery
networking and threads
Near-miss at the Airlock
As Sarah joined the on-board development team’s design review meeting , she gazed out
the portal at sunrise over the Indian Ocean. Even though the ship’s conference room was
incredibly claustrophobic, the sight of the growing blue and white crescent overtaking night on
the planet below filled Sarah with awe and appreciation.
This morning’s meeting was focused on the control systems for the orbiter’s airlocks.
As the final construction phases were nearing their end, the number of spacewalks was
scheduled to increase dramatically, and traffic was high both in and out of the ship’s
airlocks. “Good morning Sarah”, said Tom, “Your timing is perfect, we’re just starting
the detailed design review.”
“As you all know”, said Tom, “Each airlock is outfitted with space-hardened GUI
terminals, both inside and out. Whenever spacewalkers are entering or exiting the orbiter
they will use these terminals to initiate the airlock sequences.” Sarah nodded, “Tom can
you tell us what the method sequences are for entry and exit?” Tom rose, and floated to the
whiteboard, “First, here’s the exit sequence method’s pseudocode”, Tom quickly wrote on the
board.
orbiterAirlockExitSequence()
verifyPortalStatus();
pressurizeAirlock();
openInnerHatch();
confirmAirlockOccupied();
closeInnerHatch();
decompressAirlock();
openOuterHatch();
confirmAirlockVacated();
closeOuterHatch();
“To ensure that the sequence is not interrupted, we have synchronized all of the
methods called by the orbiterAirlockExitSequence() method”, Tom explained. “We’d hate to
see a returning spacewalker inadvertently catch a buddy with his space pants down!”
Everyone chuckled as Tom erased the whiteboard, but something didn’t feel right
to Sarah and it finally clicked as Tom began to write the entry sequence pseudocode on the
whiteboard. “Wait a minute Tom!”, cried Sarah, “I think we’ve got a big flaw in the exit
sequence design, let’s go back and revisit it, it could be critical!”
Why did Sarah stop the meeting? What did she suspect?
you are here�
 527
puzzle answers
What did Sarah know?
Sarah realized that in order to ensure that the entire exit
sequence would run without interruption the
orbiterAirlockExitSequence( ) method needed to
be synchronized. As the design stood, it would be possible
for a returning spacewalker to interrupt the Exit Sequence!
The Exit Sequence thread couldn’t be interrupted in the
middle of any of the lower level method calls, but it could
be interrupted in between those calls. Sarah knew that the
entire sequence should be run as one atomic unit, and if
the orbiterAirlockExitSequence( ) method was
synchronized, it could not be interrupted at any point.
528
 chapter 15
16 collections and generics
Data
structures
Sheesh... and all
this time I could have just let
Java put things in alphabetical
order? Third grade really
sucks. We never learn
anything useful...
Sorting is a snap in Java. You have all the tools for collecting and manipulating
your data without having to write your own sort algorithms (unless you’re reading this right
now sitting in your Computer Science 101 class, in which case, trust us—you are SO going to be
writing sort code while the rest of us just call a method in the Java API). The Java Collections
Framework has a data structure that should work for virtually anything you’ll ever need to do.
Want to keep a list that you can easily keep adding to? Want to find something by name? Want
to create a list that automatically takes out all the duplicates? Sort your co-workers by the
number of times they’ve stabbed you in the back? Sort your pets by number of tricks learned?
It’s all here...
this is a new chapter
 529
sorting a list
Tracking song popularity on your jukebox
Congratulations on your new job—managing the automated
jukebox system at Lou’s Diner. There’s no Java inside the
jukebox itself, but each time someone plays a song, the song
data is appended to a simple text file.
Your job is to manage the data to track song popularity,
generate reports, and manipulate the playlists. You’re not
writing the entire app—some of the other software developer/
waiters are involved as well, but you’re responsible for managing
and sorting the data inside the Java app. And since Lou has a thing
against databases, this is strictly an in-memory data collection. All
you get is the file the jukebox keeps adding to. Your job is to take it
from there.
You’ve already figured out how to read and parse the file, and so far
you’ve been storing the data in an ArrayList.
SongList.txt
Pink Moon/Nick Drake
Somersault/Zero 7
Shiva Moon/Prem Joshua
Circles/BT
Deep Channel/Afro Celts
Passenger/Headmix
Listen/Tahiti 80
This writes. then is manipul the Your file
 at
 code e
 the the must jukebox song
 re
ad
 data.
 d t ev he ic e
 file,
Challenge #1
Sort the songs in alphabetical order
You have a list of songs in a file, where each line
represents one song, and the title and artist are
separated with a forward slash. So it should be simple
to parse the line, and put all the songs in an ArrayList.
Your boss cares only about the song titles, so for now
you can simply make a list that just has the song titles.
But you can see that the list is not in alphabetical
order... what can you do?
You know that with an ArrayList, the elements are
kept in the order in which they were inserted into the
list, so putting them in an ArrayList won’t take care of
alphabetizing them, unless... maybe there’s a sort()
method in the ArrayList class?
530
 chapter 16
collections with generics
Here’s what you have so far, without the sort:
import java.util.*;
import public java.io.*;
 class Jukebox1 {
 We’ll an ArrayLis
 store tt he of song Strings.
 titles in
ArrayList<String> songList = new ArrayList<String>();
public static void main(String[] args) {

 new Jukebox1().go();

 }
 public getSongs();
 void go() {
 The file
 the method an
 songList d
 t
he
n tha
 Arr
 pr
 t in
 ayList.
 ts starts the
 load
 co
nt
 ing ents the
 of

 System.out.println(songList);
}
 void getSongs() {
 Nothing call the sp
 ad
 ec
 dSong() ia
l
 he
re
...
 method just read for ea
 the ch f lin ile e.
 and

 try {
			 File file = new File(“SongList.txt”);
			 BufferedReader reader = new BufferedReader(new FileReader(file));
			 String line = null;
			 while ((line= reader.readLine()) != null) {
				 addSong(line);
			 }

 } catch(Exception ex) {


 void }
 String[] }
 addSong(String ex.printStackTrace();
 tokens = lineToParse.split(“/”);
 lineToParse) {
 pieces (that Card The add
 (token in has t S h bo o e ng s) th I/O method using the title the
 works split() andjust
 ar me t
 break is like t t h ) od.
 into the the Quiz-
 two
 line
chapter--you
 songList.add(tokens[0]);
}

 }
 SongL
ist add We only only wan
 the (the t f t ArrayL
ist).
 ir he st song token title, to the
 so
[Pink Deep %java Shiva File Edit Window Channel, Moon, Moon, Jukebox1
 Help Dance
 Circles,
 Somersault,
 Passenger,
 The were songs is within the songList added in same the the orig
 orde to or pr de inal the in r r ts the in text Arra ou wh
 t songs ic with yList file).
 h they
 are the
 (which
 in
Listen]
 This is definitely NOT al
phabetical !
you are here�
 531
ArrayList API
But the ArrayList class does NOT have a sort() method!
When you look in ArrayList, there doesn’t seem to be any method related to sorting.
Walking up the inheritance hierarchy didn’t help either—it’s clear that you can’t call a sort
method on the ArrayList.
ArrayList has a lot
 of methods,
but there’s nothing
 here that
looks like it would so
rt...
532
 chapter 16
collections with generics
I do see a collection class
called TreeSet... and the docs
say that it keeps your data
sorted. I wonder if I should be
using a TreeSet instead of an
ArrayList...
ArrayList is not the only collection
Although ArrayList is the one you’ll use most often,
there are others for special occasions. Some of the key
$ collection TreeSet
 classes include:
 Don
 to right more learn ’t details
 worry now. the W se e’ll about aot little go her t
 into
 rying
 ones
 later.
Keeps the elements sorted and prevents duplicates.
$HashMap
Lets you store and access elements as name/value pairs.
$LinkedList
Makes it easy to create structures like stacks or queues.
$HashSet
Prevents duplicates in the collection, and given an element, can
find that element in the collection quickly.
$LinkedHashMap
Like a regular HashMap, except it can remember the order in
which elements (name/value pairs) were inserted, or it can be
configured to remember the order in which elements were last
accessed.
you are here�
 533
Collections.sort()
You could use a TreeSet...
Or you could use the Collections.sort() method
If you put all the Strings (the song titles) into a TreeSet instead of
an ArrayList, the Strings would automatically land in the right place,
alphabetically sorted. Whenever you printed the list, the elements would
always come out in alphabetical order.
And that’s great when you need a set (we’ll
 java.util.Collections
you talk about know that sets the in alist few must minutes) always or stay
 when
 public static void copy(List
 destination, List source)
sorted alphabetically.
 public static List emptyList
()
On the other hand, if you don’t need the
 public static void fill(List listT
 oFill, Object objToFillItWith)
list to stay sorted, TreeSet might be more
 public static int frequency (Co
llection c, Object o)
expensive than you need—every time you
insert into a TreeSet, the TreeSet has to take
 public static void reverse(Lis
t list)
the time to figure out where in the tree the new
 public static void rotate(List
 list, int distance)
element must go. With ArrayList, inserts can
be blindingly fast because the new element
 public static void shuffle (Lis
t list)
just goes in at the end.
 public static void sort(List list)
public static boolean replac
eAll (List list, Object oldVal, Ob
ject newVal)
Q:
 ArrayList the that end—there’s takes But at you an a int specific CAN along anadd overloaded index with something instead the element add() to ofan
 method
 just toat
 add.
 // many more methods.
 a in Hmmm
 ..
 List, the ... Collections and there since IS Arr
ayList
 a clas
s. sort()
 It method
 takes
So wouldn’t it be slower than inserting at the end?
 implements the List
 interface,
somewhere A:
 Yes, it’s other slower than to at insert the end. something So using in the an overloaded
 ArrayList
 to ArrayList polymorphism, IS-A you
 List
. can Thanks
 pass an
add(index, the add(element)—which element) method puts doesn’t the added work element as quickly at as the calling
 end.
 ArrayList to a metho
d declared
But most of the time you use ArrayLists, you won’t need to put
 to take List.
something at a specific index.
Q:
 I see there’s a LinkedList class, so wouldn’t that be better for
doing inserts somewhere in the middle? At least if I remember my Data
Structures class from college...
remove A:
 to between care Yes, about something middle good unless inserts spot. from you’re The into the LinkedList dealing middle, a LinkedList with but can a for and be huge most quicker ArrayList number applications, when is of usually elements. you the insert not difference
 enough
 We’ll
 or
 Note: class out the API; this generic we is NOT simplifi type th ed e information real it here Collections
 by (which
 leaving
look more at LinkedList in a few minutes.
 you’ll see in a few pa
ges).
534
 chapter 16
collections with generics
Adding Collections.sort() to the Jukebox code
import java.util.*;
import java.io.*;
The Collections.sort()
public class Jukebox1 {
method sorts a list of
ArrayList<String> songList = new ArrayList<String>();
Strings alphabetically.
public static void main(String[] args) {

 new Jukebox1().go();
}
public void go() {




 System.out.println(songList);
 getSongs();
 Collections.sort(songList);
 System.out.println(songList);
 Call sort() list is in the again. alphab
 metho stat
 T etical he ic d, Collections se then cond order!
 prin
 print t the
 out
}
void getSongs() {

 try {
			 File file = new File(“SongList.txt”);
			 BufferedReader reader = new BufferedReader(new FileReader(file));
			 String line = null;
			 while ((line= reader.readLine()) != null) {
				 addSong(line);
			 }

 } catch(Exception ex) {
			 ex.printStackTrace();

 }
}
void addSong(String lineToParse) {

 String[] tokens = lineToParse.split(“/”);

 songList.add(tokens[0]);
}
}
File Edit Window Help Chill
%java Jukebox1
[Pink Moon, Somersault, Shiva Moon, Circles, Deep
 Before calling so
rt().
Channel, Passenger, Listen]
[Circles, Deep Channel, Listen, Passenger, Pink
 After calling sort
().
Moon, Shiva Moon, Somersault]
you are here�
 535
sorting your own objects
But now you need Song objects,
not just simple Strings.
Now your boss wants actual Song class instances in the list, not just
 SongListMore.txt
Strings, so that each Song can have more data. The new jukebox
device outputs more information, so this time the file will have four
pieces (tokens) instead of just two.
 Pink Moon/Nick Drake/5/80
Somersault/Zero 7/4/84
The Song class is really simple, with only one interesting feature—
 Shiva Moon/Prem Joshua/6/120
the overridden toString() method. Remember, the toString()
 Circles/BT/5/110
method is defined in class Object, so every class in Java inherits the
 Deep Channel/Afro Celts/4/120
method. And since the toString() method is called on an object
 Passenger/Headmix/4/100
when it’s printed (System.out.println(anObject)), you should
 Listen/Tahiti 80/5/90
override it to print something more readable than the default
unique method identifier will be called code. on When each you object.
 print a list, the toString()
 The new song file holds four
attributes instead of just two
.And we want ALL of them in our
class String String Song title;
 artist;
 {
 Four four instance song at
tri va butes riables infor the the
 file.
 four list, class so song with we attributes.
 instance need to make variables a Son
 for g all
String rating;
String bpm;
Song(String t, String a, String r, String b) {

 title = t;

 artist = a;
 The variables are all set in

 rating = r;
 the constructor when the

 bpm = b;
 new Song is created.
}
public String getTitle() {

 return title;
}
public String getArtist() {

 return artist;
}
 The getter methods for
the four attributes.
public String getRating() {

 return rating;
}
public String getBpm() {

 return bpm;
}
}


 }
 public return String title;
 toString() {
 the When out.println(aSongObje We override toString() you do toStrin
 a method
 System.out.p g() ct , of ), be we
 cause EACH rintln(aListOf want when element to you see in the do So th
e ngs), a title.
 System.
 list.
 it ca
lls
536
 chapter 16
Changing the Jukebox code to use Songs
instead of Strings
Your code changes only a little—the file I/O code is the same,
and the parsing is the same (String.split()), except this time
there will be four tokens for each song/line, and all four will be
used to create a new Song object. And of course the ArrayList
will be of type <Song> instead of <String>.
collections with generics
import import java.util.*;
 java.io.*;
 C
 objects ha
ng
e
 to
 instead an
 A
rr
 of
 ayList String.
 of Song
public class Jukebox3 {
ArrayList<Song> songList = new ArrayList<Song>();
public static void main(String[] args) {

 new Jukebox3().go();
}
public void go() {

 getSongs();

 System.out.println(songList);

 Collections.sort(songList);

 System.out.println(songList);
}
void getSongs() {

 try {
			 File file = new File(“SongListMore.txt”);
			 BufferedReader reader = new BufferedReader(new FileReader(file));
			 String line = null;
			 while ((line= reader.readLine()) != null) {
				 addSong(line);
			 }

 } catch(Exception ex) {
			 ex.printStackTrace();

 }
}
void addSong(String lineToParse) {

 String[] tokens = lineToParse.split(“/”);

 Song nextSong = new Song(tokens[0], tokens[1], tokens[2], tokens[3]);

 songList.add(nextSong);
}
 }
 for (which Create this means a line), newthe then So
ng fo add object ur pieces the using Song of the info to four th
e in the list.
 tokens
 song file
you are here�
 537
Collections.sort()
It won’t compile !
Something’s wrong... the Collections class clearly shows there’s a
sort() method, that takes a List.
ArrayList is-a List, because ArrayList implements the List interface,
so... it should work.
But it doesn’t!
The compiler says it can’t find a sort method that takes an
ArrayList<Song>, so maybe it doesn’t like an ArrayList of Song
objects? It didn’t mind an ArrayList<String>, so what’s the
important difference between Song and String? What’s the
difference that’s making the compiler fail?
File Edit Window Help Bummer
%javac Jukebox3.java
Jukebox3.java:15: cannot find symbol
symbol : method sort(java.util.ArrayList<Song>)
location: class java.util.Collections
Collections.sort(songList);
^
1 error
And of course you probably already asked yourself, “What would it
be sorting on?” How would the sort method even know what made
one Song greater or less than another Song? Obviously if you want
the song’s title to be the value that determines how the songs are
sorted, you’ll need some way to tell the sort method that it needs
to use the title and not, say, the beats per minute.
We’ll get into all that a few pages from now, but first, let’s find out
why the compiler won’t even let us pass a Song ArrayList to the
sort() method.
538
 chapter 16
WTF? I have no idea how to
read the method declaration
on this. It says that sort()
takes a List<T>, but what is
T? And what is that big thing
before the return type?
collections with generics
The sort() method declaration
From the API docs (looking up the java.util.Collections class, and scrolling to the sort()
method), it looks like the sort() method is declared... strangely. Or at least different from
anything we’ve seen so far.
That’s because the sort() method (along with other things in the whole collection framework in
Java) makes heavy use of generics. Anytime you see something with angle brackets in Java source
code or documentation, it means generics—a feature added to Java 5.0. So it looks like we’ll
have to learn how to interpret the documentation before we can figure out why we were able to
sort String objects in an ArrayList, but not an ArrayList of Song objects.
you are here�
 539
generic types
Generics means more type-safety
We’ll just say it right here—virtually all of the code you write that deals
with generics will be collection-related code. Although generics can be used
in other ways, the main point of generics is to let you write type-safe
collections. In other words, code that makes the compiler stop you
from putting a Dog into a list of Ducks.
Before generics (which means before Java 5.0), the compiler could
not care less what you put into a collection, because all collection
implementations were declared to hold type Object. You could put
anything in any ArrayList; it was like all ArrayLists were declared as
ArrayList<Object>.
 WITHOUT Objects go generics
 IN as a reference to
 Before way ArrayList, to generi decla so re cs, its the there add() type was method
 of no
 an
took type O
bject.
SoccerBall, Fish, Guitar, and
Car objects
ArrayList
Object
 Object
 Object
And come OUT as a reference of type Object
Object
WITH generics
Objects go IN as a reference to
only Fish objects
With generics, you can
create type-safe collections
where more problems are
caught at compile-time
instead of runtime.
Without generics, the
compiler would happily let
you put a Pumpkin into an
ArrayList that was supposed
to hold only Cat objects.
ArrayList<Fish>
And come out as a reference of type Fish
540
 chapter 16
able what sticking
 You objects objects Now to don’t you wit
 a a com Fis in g h e V ha t olkswagen t h generics, e ve he out reference.
 out to won’t as worry
 you Fish in really ther can ab
 refere o e u put
 , t b o e someone
 r nces.
 ,cast-
 only so that
 the
 Fish
ArrayList<Fish>Learning generics
Of the dozens of things you could learn about generics, there are
really only three that matter to most programmers:
1
2
Creating instances of generified classes (like ArrayList)
When you make an ArrayList, you have to tell it the type
of objects you’ll allow in the list, just as you do with plain
old arrays.
Declaring and assigning variables of generic types
How does polymorphism really work with generic
types? If you have an ArrayList<Animal> reference
variable, can you assign an ArrayList<Dog> to it? What
about a List<Animal> reference? Can you assign an
ArrayList<Animal> to it? You’ll see...
3
Declaring (and invoking) methods that take generic types
If you have a method that takes as a parameter, say, an
ArrayList of Animal objects, what does that really mean?
Can you also pass it an ArrayList of Dog objects? We’ll
look at some subtle and tricky polymorphism issues that
are very different from the way you write methods that
take plain old arrays.
(This is actually the same point as #2, but that shows you
how important we think it is.)
Q:
 But don’t I also need to learn how to create my OWN generic
classes? What if I want to make a class type that lets people
instantiating the class decide the type of things that class will use?
A:
 You probably won’t do much of that. Think about it—the API
designers made an entire library of collections classes covering most of
the data structures you’d need, and virtually the only type of classes that
really need to be generic are collection classes. In other words, classes
designed to hold other elements, and you want programmers using it to
specify what type those elements are when they declare and instantiate
the collection class.
Yes, it is possible that you might want to create generic classes, but that’s
the exception, so we won’t cover it here. (But you’ll figure it out from the
things we do cover, anyway.)
collections with generics
new ArrayList<Song>()
List<Song> songList =
new ArrayList<Song>()
void foo(List<Song> list)
x.foo(songList)
you are here�
 541
generic classes
Using generic CLASSES
Since ArrayList is our most-used generified type, we’ll
start by looking at its documentation. The two key areas
to look at in a generified class are:
1) The class declaration
2) The method declarations that let you add elements
Understanding ArrayList documentation
(Or, what’s the true meaning of “E”?)
Think of “E” as a stand-in for
“the type of element you want
this collection to hold and
return.” (E is for Element.)
The REAL declare
 “E”
 type and is
 a
 you p
 create la
c
 use e
holder an when Arra
 fo you r yList
 t
he
 type ArrayList so ArrayList
 whatever of the is is A t automatically bstractList.
 a ype subclass you specify of used Abstract
 for for th the
 List,
 e
public class ArrayList<E> extends AbstractList<E> implements List<E>
 ... {
public boolean add(E o)
 The type (the value of <E>)
determin
 Here’s the es
 important what
 kind
 part! of
 thin
 Whatever gs
 you’r
 e “E”
 allow
 is
 ed
 becomes interface the as well.
 type of the List
to add to the ArrayList.
// more code
}
The “E” represents the type used to create an instance
of ArrayList. When you see an “E” in the ArrayList
documentation, you can do a mental find/replace to
exchange it for whatever <type> you use to instantiate
ArrayList.
So, new ArrayList<Song> means that “E” becomes “Song”,
in any method or variable declaration that uses “E”.
542
 chapter 16
Using type parameters with ArrayList
collections with generics
THIS code:
ArrayList<String> thisList = new ArrayList<String>
Means ArrayList:
public class ArrayList<E> extends AbstractList<E> ... {
public boolean add(E o)
// more code
}
Is treated by the compiler as:
public class ArrayList<String> extends AbstractList<String> ... {
public boolean add(String o)
// more code
}
In other words, the “E” is replaced by the real type (also called the type parameter)
that you use when you create the ArrayList. And that’s why the add() method
for ArrayList won’t let you add anything except objects of a reference type that’s
compatible with the type of “E”. So if you make an ArrayList<String>, the add()
method suddenly becomes add(String o). If you make the ArrayList of type Dog,
suddenly the add() method becomes add(Dog o).
Q:
 Is “E” the only thing you can put there? Because the docs for sort used “T”....
A:
 You can use anything that’s a legal Java identifier. That means anything that you
could use for a method or variable name will work as a type parameter. But the conven-
tion is to use a single letter (so that’s what you should use), and a further convention is to
use “T” unless you’re specifically writing a collection class, where you’d use “E” to repre-
sent the “type of the Element the collection will hold”.
you are here�
 543
generic methods
Using generic METHODS
A generic class means that the class declaration includes a type
parameter. A generic method means that the method declaration
uses a type parameter in its signature.
You can use type parameters in a method in several different ways:
1
Using a type parameter defined in the class declaration
public class ArrayList<E> extends AbstractList<E> ... {
When you public declare boolean a type parameter add(E o)
 for the already You class, can be u
 you
 se enthe defined “E” here as pa
 ONLY rt of bec
 the ac use
 lass.
 it’s
can simply use that type any place that you’d use a
real class or interface type. The type declared in the
method argument is essentially replaced with the type
you use when you instantiate the class.
2
Using a type parameter that was NOT defined in the class declaration
public <T extends Animal> void takeThing(ArrayList<T> list)
(but If specify the available) class one itself for space—before a doesn’t method, use by the a declaring type return parameter, type. it in This a really you method can unusual
 still
 says
 “T” Here earlie we
 c r a in n use the <T> method because d
eclaration we de
c
la . red
that T can be “any type of Animal”.
544
 chapter 16
Wait... that can’t be right. If you can
take a list of Animal, why don’t you
just SAY that? What’s wrong with just
takeThing(ArrayList<Animal> list) ?
collections with generics
Here’s where it gets weird...
This:
public <T extends Animal> void takeThing(ArrayList<T> list)
Is NOT the same as this:
public void takeThing(ArrayList<Animal> list)
Both are legal, but they’re different!
The first one, where <T extends Animal> is part of the method
declaration, means that any ArrayList declared of a type that is
Animal, or one of Animal’s subtypes (like Dog or Cat), is legal.
So you could invoke the top method using an ArrayList<Dog>,
ArrayList<Cat>, or ArrayList<Animal>.
But... the one on the bottom, where the method argument is
(ArrayList<Animal> list) means that only an ArrayList<Animal>
is legal. In other words, while the first version takes an ArrayList
of any type that is a type of Animal (Animal, Dog, Cat, etc.),
the second version takes only an ArrayList of type Animal. Not
ArrayList<Dog>, or ArrayList<Cat> but only ArrayList<Animal>.
And yes, it does appear to violate the point of polymorphism.
but it will become clear when we revisit this in detail at the end
of the chapter. For now, remember that we’re only looking at
this because we’re still trying to figure out how to sort() that
SongList, and that led us into looking at the API for the sort()
method, which had this strange generic type declaration.
For now, all you need to know is that the syntax of the top version
is legal, and that it means you can pass in a ArrayList object
instantiated as Animal or any Animal subtype.
And now back to our sort() method...
you are here�
 545
sorting a Song
This still doesn’t
explain why the sort method
failed on an ArrayList of Songs
but worked for an ArrayList of
Strings...
Remember where we were...
File Edit Window Help Bummer
%javac Jukebox3.java
Jukebox3.java:15: cannot find symbol
symbol : method sort(java.util.ArrayList<Song>)
location: class java.util.Collections
Collections.sort(songList);
^
1 error
import java.util.*;
import java.io.*;
public class Jukebox3 {
ArrayList<Song> songList = new ArrayList<Song>();
public static void main(String[] args) {

 new Jukebox3().go();
}
public void go() {




 getSongs();
 System.out.println(songList);
 Collections.sort(songList);
 System.out.println(songList);
 This passe
 tried is d
 to where in
 sort an
 Ar
 it an ra
 br ArrayL
 yL
 eaks! ist
<S
 It ist<Song>, tr
 worked ing>,
 bu
 fi t it ne as failed.
 wh
 so
 en
 on as we
}
void getSongs() {

 try {
			 File file = new File(“SongListMore.txt”);
			 BufferedReader reader = new BufferedReader(new FileReader(file));
			 String line = null;
			 while ((line= reader.readLine()) != null) {
				 addSong(line);
			 }

 } catch(Exception ex) {
			 ex.printStackTrace();

 }
}
void addSong(String lineToParse) {

 String[] tokens = lineToParse.split(“/”);

 Song nextSong = new Song(tokens[0], tokens[1], tokens[2], tokens[3]);

 songList.add(nextSong);
}
}
546
 chapter 16
Revisiting the sort( ) method
So here we are, trying to read the sort() method docs to find
out why it was OK to sort a list of Strings, but not a
list of Song objects. And it looks like the answer is...
The sort() method can take only lists
of Comparable objects.
Song is NOT a subtype of
Comparable, so you cannot sort() the
list of Songs.
At least not yet...
collections with generics
public static <T extends Comparable<? super T>> void sort(List<T> list)
This be of says type “Whatever Comparable.”
 ‘T’ is must
 Comparable (Ignore that if you the can’t, this type must part it parameter just be for means
 of now. type for
 But
 T
 You that subtype that can uses “extends pass of a list, parameterized in Comparable”.
 only like a ArrayList)
 List (or
 type
or one of T’s supertypes.)
Um... I just checked the docs for
String, and String doesn’t EXTEND
Comparable--it IMPLEMENTS it.
Comparable is an interface. So it’s nonsense
to say <T extends Comparable>.
public final class String extends Object implements Serializable,
Comparable<String>, CharSequence
you are here�
 547
the sort() method
In generics, “extends” means
“extends or implements”
The Java engineers had to give you a way to put a constraint
on a parameterized type, so that you can restrict it to, say, only
subclasses of Animal. But you also need to constrain a type to
allow only classes that implement a particular interface. So
here’s a situation where we need one kind of syntax to work
for both situations—inheritance and implementation. In other
words, that works for both extends and implements.
And the winning word was... extends. But it really means “is-a”,
and works regardless of whether the type on the right is an
interface or a class.
Comparable is an interface, so thisREALLY reads, “T must be a type that
implements the Comparable interfac
e”.
In generics, the keyword
“extends” really means “is-a”,
and works for BOTH classes
and interfaces.
public static <T extends Comparable<? super T>> void sort(List<T> list)
It doesn’t matter whether the thing on the right is
a class or interface... you still say “extends”.
Q:
 Why didn’t they just make a new keyword,“is”?
A:
 Adding a new keyword to the language is a REALLY big deal because
it risks breaking Java code you wrote in an earlier version. Think about it—
you might be using a variable “is” (which we do use in this book to represent
input streams). And since you’re not allowed to use keywords as identifiers
in your code, that means any earlier code that used the keyword before it
was a reserved word, would break. So whenever there’s a chance for the
Sun engineers to reuse an existing keyword, as they did here with “extends”,
they’ll usually choose that. But sometimes they don’t have a choice...
A few (very few) new keywords have been added to the language, such
as assert in Java 1.4 and enum in Java 5.0 (we look at enum in the appen-
dix). And this does break people’s code, however you sometimes have the
option of compiling and running a newer version of Java so that it behaves
as though it were an older one. You do this by passing a special flag to the
compiler or JVM at the command-line, that says, “Yeah, yeah, I KNOW this is
Java 1.4, but please pretend it’s really 1.3, because I’m using a variable in my
code named assert that I wrote back when you guys said it would OK!#$%”.
(To see if you have a flag available, type javac (for the compiler) or java (for
the JVM) at the command-line, without anything else after it, and you should
see a list of available options. You’ll learn more about these flags in the chap-
ter on deployment.)
548
 chapter 16
Finally we know what’s wrong...
The Song class needs to implement Comparable
We can pass the ArrayList<Song> to the sort() method only if the
Song class implements Comparable, since that’s the way the sort()
method was declared. A quick check of the API docs shows the
Comparable interface is really simple, with only one method to
implement:
java.lang.Comparable
public interface Comparable<T> {
int compareTo(T o);
}
collections with generics
The big question is: what
makes one song less than,
equal to, or greater than
another song?
You can’t implement the
Comparable interface until you
make that decision.
And the method documentation for compareTo() says
Returns:
a negative integer, zero, or a
positive integer as this object
is less than, equal to, or greater
than the specified object.
Sharpen your pencil
Write in your idea and pseudo code (or
better, REAL code) for implementing the
compareTo() method in a way that will
sort() the Song objects by title.
Hint: if you’re on the right track, it should
take less than 3 lines of code!
It looks like the compareTo() method will be called on one
Song object, passing that Song a reference to a different
Song. The Song running the compareTo() method has to
figure out if the Song it was passed should be sorted higher,
lower, or the same in the list.
Your big job now is to decide what makes one song greater
than another, and then implement the compareTo() method
to reflect that. A negative number (any negative number)
means the Song you were passed is greater than the Song
running the method. Returning a positive number says
that the Song running the method is greater than the Song
passed to the compareTo() method. Returning zero means
the Songs are equal (at least for the purpose of sorting... it
doesn’t necessarily mean they’re the same object). You might,
for example, have two Songs with the same title.
(Which brings up a whole different can of worms we’ll look
at later...)
you are here�
 549
the Comparable interface
The new, improved, comparable Song class
We decided we want to sort by title, so we implement the compareTo()
method to compare the title of the Song passed to the method against
the title of the song on which the compareTo() method was invoked.
In other words, the song running the method has to decide how its
title compares to the title of the method parameter.
Hmmm... we know that the String class must know about alphabetical
order, because the sort() method worked on a list of Strings. We know
String has a compareTo() method, so why not just call it? That way, we
can simply let one title String compare itself to another, and we don’t
have to write the comparing/alphabetizing algorithm!
Usually these match...we’re specifying the type that
the implementing
 class
 can
 be
 comp
ared
 again
st.
class String Song title;
 implements
 Comparable<Song> {
 This other means Song that objects,
 Song for
 objects the
 purp
 can ose
 be of
 compared sorting.
 to
String artist;
String String rating;
 bpm;
 The to see sort() how
 method that
 Song
 sends comp
 a ares
 Song to
 to the
 compareTo()
 Song on
which the method was invoked.
public return int title.compareTo(s.getTitle());
 compareTo(Song s) {
 Simple! on to the We title just pass String objects,
 work
the}
 since we know Strings have a
Song(String t, String a, String r, String b) {
 compareTo() method.
title = t;
artist = a;
rating = r;
bpm = b;
}
public String getTitle() {
}
 return title;
 This which time puts it the
 worked. Song
s
 It in
 prints alpha
beti
 the cal
 list, orde
r then by calls title.
 sort
public String getArtist() {
 File Edit Window Help Ambient
return artist;
}
 %java Jukebox3
public return String rating;
 getRating() {
 [Pink Moon, Somersault, Shiva Moon, Circles, Deep
}
 Channel, Passenger, Listen]
public return String bpm;
 getBpm() {
 [Circles, Deep Channel, Listen, Passenger, Pink
}
 Moon, Shiva Moon, Somersault]
public String toString() {
return title;
}
}
550
 chapter 16
We can sort the list, but...
There’s a new problem—Lou wants two different views of the song list,
one by song title and one by artist!
But when you make a collection element comparable (by having it
implement Comparable), you get only one chance to implement the
compareTo() method. So what can you do?
The horrible way would be to use a flag variable in the Song class,
and then do an if test in compareTo() and give a different result
depending on whether the flag is set to use title or artist for the
comparison.
But that’s an awful and brittle solution, and there’s something much
better. Something built into the API for just this purpose—when you
want to sort the same thing in more than one way.
Look at the Collections class API again. There’s a
second sort() method—and it takes a Comparator.
collections with generics
That’s not good enough.
Sometimes I want it to sort
by artist instead of title.
The take so
 so
 r
m
 t
()
 e
t
h
 m
 in
 e
t
 g
h
 c
 o
 a
 d
 ll
e
 is d overloaded a Comparato
 t
 o
 r.
N
 get
 comp
 o
t
e
 /
m
 t
 a
o
 r
 a
 e
 k
 se
 e
 a
 lf
 n
 a
 d
 :
 C
 figure o
 o
 r
 m
 d
 parator e
r the out songs how that
 to
 by
 ca
 n
artist instead
 of title...
you are here�
 551
the Comparator interface
Using a custom Comparator
An element in a list can compare itself to another of
its own type in only one way, using its compareTo()
method. But a Comparator is external to the element
type you’re comparing—it’s a separate class. So you can
make as many of these as you like! Want to compare
songs by artist? Make an ArtistComparator. Sort by beats
per minute? Make a BPMComparator.
Then all you need to do is call the overloaded sort()
method that takes the List and the Comparator that will
help the sort() method put things in order.
The sort() method that takes a Comparator will use the
Comparator instead of the element’s own compareTo()
method, when it puts the elements in order. In other
words, if your sort() method gets a Comparator, it won’t
even call the compareTo() method of the elements
in the list. The sort() method will instead invoke the
compare() method on the Comparator.
So, the rules are:
$Invoking the one-argument sort(List o) method
means the list element’s compareTo() method
determines the order. So the elements in the list
MUST implement the Comparable interface.
java.util.Comparator
public interface Comparator<T> {
int compare(T o1, T o2);
}
If you pass a Comparator to the
sort() method, the sort order is
determined by the Comparator
rather than the element’s own
compareTo() method.
$Invoking sort(List o, Comparator c) means the
list element’s compareTo() method will NOT be
called, and the Comparator’s compare() method
will be used instead. That means the elements
in the list do NOT need to implement the
Comparable interface.
Q:
 So does this mean that if you have a class that
doesn’t implement Comparable, and you don’t have the
source code, you could still put the things in order by
creating a Comparator?
A:
 That’s right. The other option (if it’s possible) would beto subclass the element and make the subclass implement
Comparable.
552
 chapter 16
Q:
 But why doesn’t every class implement Comparable?
A:
 Do you really believe that everything can be ordered? If
you have element types that just don’t lend themselves to any kind
of natural ordering, then you’d be misleading other programmers
if you implement Comparable. And you aren’t taking a huge risk by
not implementing Comparable, since a programmer can compare
anything in any way that he chooses using his own custom
Comparator.
Updating the Jukebox to use a Comparator
We did three new things in this code:
1) Created an inner class that implements Comparator (and thus the compare()
method that does the work previously done by compareTo()).
2) Made an instance of the Comparator inner class.
3) Called the overloaded sort() method, giving it both the song list and the
instance of the Comparator inner class.
Note: we also updated the Song class toString() method to print both the song
title and the artist. (It prints title: artist regardless of how the list is sorted.)
collections with generics
import java.util.*;
import java.io.*;
public class Jukebox5 {
ArrayList<Song> songList = new ArrayList<Song>();
 Create a new inner class that implements
public static void main(String[] args) {
 Comparator (note that its typenew Jukebox5().go();
 parameter matches the type we’re
 going
}
 to compare—in this case Song objects.)
class ArtistCompare implements Comparator<Song> {
public int compare(Song one, Song two ) {
return one.getArtist().compareTo(two.getArtist());
}
public }
 void This go() becomes {
 a String (the artist)
 know do We’re the how letting actual to alphabetize the comparison, String
 variables themselves.
 since Strings (for artist)
 already
getSongs();
System.out.println(songList);
Collections.sort(songList);
System.out.println(songList);
Make an instance of the
ArtistCompare artistCompare = new ArtistCompare();
 Comparator inner class.
Collections.sort(songList, artistCompare);
}
 System.out.println(songList);
 custom and Invoke a reference sort(), Comparator passing to
 th
e
 obje it
 ne
 ct.
 the w
 list
void getSongs() {
// I/O code here
 Note: we’ve made sort-by-title the default sort, by
}
 keeping the compareTo() method in Song use the
titles. But another way to design this would be to
void addSong(String lineToParse) {
 implement both the title sorting and artist sorting as
// parse line and add to song list
 inner Comparator classes, and not have Song implement
}
 Comparable at all. That means we’d always use the two-
}
 arg version of Collections.sort().
you are here�
 553
collections exercise
import __________________;
public class SortMountains {
LinkedList______________ mtn = new LinkedList____________();
class NameCompare ___________________________________ {
public int compare(Mountain one, Mountain two) {
return ___________________________;
}
}
class HeightCompare _______________________________ {
public int compare(Mountain one, Mountain two) {
return (__________________________);
}
}
public static void main(String [] args) {
new SortMountains().go();
}
public void go() {
mtn.add(new Mountain(“Longs”, 14255));
mtn.add(new Mountain(“Elbert”, 14433));
mtn.add(new Mountain(“Maroon”, 14156));
mtn.add(new Mountain(“Castle”, 14265));
System.out.println(“as entered:\n” + mtn);
NameCompare nc = new NameCompare();
_________________________________;
System.out.println(“by name:\n” + mtn);
HeightCompare hc = new HeightCompare();
_________________________________;
System.out.println(“by height:\n” + mtn);
}
}
Sharpen your pencil
Reverse Engineer
Assume this code exists in
a single file. Your job is
to fill in the blanks so the
the program will create the
output shown.
Note: answers are at the end of
the chapter.
class Mountain {
__________________;
_________________;
}
______________________ {
_________________;
_________________;
}
_________________________ {
______________________________;
}
554
 chapter 16
Output:
File Edit Window Help ThisOne’sForBob
%java SortMountains
as entered:
[Longs 14255, Elbert 14433, Maroon 14156, Castle 14265]
by name:
[Castle 14265, Elbert 14433, Longs 14255, Maroon 14156]
by height:
[Elbert 14433, Castle 14265, Longs 14255, Maroon 14156]
Sharpen your pencil
collections with generics
Fill-in-the-blanks
For each of the questions below, fill in the blank
with one of the words from the “possible answers”
list, to correctly answer the question. Answers are
at the end of the chapter.
Possible Answers:
Comparator,
Comparable,
compareTo( ),
compare( ),
yes,
no
Given the following compilable statement:
Collections.sort(myArrayList);
1. What must the class of the objects stored in myArrayList implement?
 ________________
2. What method must the class of the objects stored in myArrayList implement? ________________
3. Can the class of the objects stored in myArrayList implement both
Comparator AND Comparable?
 ________________
Given the following compilable statement:
Collections.sort(myArrayList,myCompare);
4. Can the class of the objects stored in myArrayList implement Comparable? ________________
5. Can the class of the objects stored in myArrayList implement Comparator? ________________
6. Must the class of the objects stored in myArrayList implement Comparable? ________________
7. Must the class of the objects stored in myArrayList implement Comparator? ________________
8. What must the class of the myCompare object implement?
 ________________
9. What method must the class of the myCompare object implement?
 __________________
you are here�
 555
dealing with duplicates
Uh-oh. The sorting all works, but now we have duplicates...
The sorting works great, now we know how to sort on both title (using the Song object’s
compareTo() method) and artist (using the Comparator’s compare() method). But there’s
a new problem we didn’t notice with a test sample of the jukebox text file—the sorted list
contains duplicates.
It appears that the diner jukebox just keeps writing to the file regardless of whether the
same song has already been played (and thus written) to the text file. The SongListMore.txt
jukebox text file is a complete record of every song that was played, and might contain the
same song multiple times.
File Edit Window Help TooManyNotes
%java Jukebox4
[Pink Moon: Nick Drake, Somersault: Zero 7, Shiva Moon: Prem
 Before sorting.
Joshua, Circles: BT, Deep Channel: Afro Celts, Passenger:
Headmix, Listen: Tahiti 80, Listen: Tahiti 80, Listen: Tahiti
80, Circles: BT]
[Circles: BT, Circles: BT, Deep Channel: Afro Celts, Listen:
 After sorting using
Somersault: Headmix, Tahiti 80, Pink Listen: Zero Moon: 7]
Tahiti Nick Drake, 80, Listen: Shiva Moon: Tahiti Prem 80, Joshua,
 Passenger:
 the compareTo() (sort Song’s by title).
 own meth
od
Headmix, [Deep Channel: Pink Moon: Afro Nick Celts, Drake, Circles: Shiva BT, Moon: Circles: Prem BT, Joshua, Passenger:
 Listen:
 After the ArtistCompare sorting using
Tahiti 80, Listen: Tahiti 80, Listen: Tahiti 80, Somersault:
 Comparator (sort
 by
Zero 7]
 artist name).
SongListMore.txt
Listen/Tahiti Shiva Circles/BT/5/110
 Deep Passenger/Headmix/4/100
 Somersault/Zero Listen/Tahiti Listen/Tahiti Circles/BT/5/110
 Pink Moon/Nick Channel/Afro Moon/Prem 80/5/90
 80/5/90
 80/5/90
 Drake/5/80
 7/4/84
 Joshua/6/120
 Celts/4/120
 information. because We that three played, it, The can’t because had SongListMor
 times sometimes in change
 been or
 We the in der a pl ha . the ju ayed row, ve Somebody we’re kebox e text to way earlier.
 followed change going machine the file decided text to now the
 by need is
 “Circles”, has file java writi to all duplicat
 is pl code.
 ng that ay wri
 ev “L
 a tten
 ery song
 es isten”
 in
 so
ng
556
 chapter 16
We need a Set instead of a List
From the Collection API, we find three main interfaces, List, Set, and
Map. ArrayList is a List, but it looks like Set is exactly what we need.
$ LIST - when sequence matters
Collections that know about index position.
Lists know where something is in the list. You
can have more than one element referencing
the same object.
Duplicates OK.
0
collections with generics
1
 2
 3
List
$SET - when uniqueness matters
Collections that do not allow duplicates.
Sets know whether something is already in the collection.
You can never have more than one element referencing
the same object (or more than one element referencing
two objects that are considered equal—we’ll look at what
object equality means in a moment).
NO duplicates.
Set
$
MAP - when finding something by key matters
Collections that use key-value pairs.
Maps know the value associated with a given key. You
can have two keys that reference the same value, but you
cannot have duplicate keys. Although keys are typically
String names (so that you can make name/value property
lists, for example), a key can be any object.
Duplicate values OK, but NO
 duplicate keys.
“Ball”
 “Ball1” “Ball2”
 “Fish”
 “Fish” “Car”
 “Car”
Map
you are here�
 557
the collections API
The Collection API (part of it)
Notice that the Map interface doesn’t
actually extend the Collection interface,
but Map is still considered part of the
“Collection Framework” (also known
as the “Collection API”). So Maps are
still collections, even though they don’t
include java.util.Collection in their
inheritance tree.
(Note: this is not the complete
collection API; there are other
classes and interfaces, but
these are the ones we care
most about.)
Set
(interface)
Collection
(interface)
SortedSet
(interface)
List
(interface)
TreeSet
 LinkedHashSet
 HashSet
 ArrayList
 LinkedList
 Vector
HashSet
Set
KEY
extends
implements
implementationinterface
class
SortedMap
(interface)
Map
(interface)
Maps don’t extend from
java.util.Collection, but
they’re still considered
to be part of the
“collections framework”
in Java. So a Map is
still referred to as a
collection.
TreeMap
 HashMap
 LinkedHashMap
 Hashtable
558
 chapter 16
collections with generics
Using a HashSet instead of ArrayList
We added on to the Jukebox to put the songs in a HashSet. (Note: we left out some
of the Jukebox code, but you can copy it from earlier versions. And to make it easier
to read the output, we went back to the earlier version of the Song’s toString()
method, so that it prints only the title instead of title and artist.)
import java.util.*;
import java.io.*;
public class Jukebox6 {
ArrayList<Song> songList = new ArrayList<Song>();
// main method etc.
public void go() {
 We didn’t change getSongs(), so it still puts the songs in an Array
List
getSongs();
System.out.println(songList);
 Here we create a new HashSet
Collections.sort(songList);
 parameterized to hold Songs.
System.out.println(songList);
HashSet<Song> songSet = new HashSet<Song>();
// }
 getSongs() System.out.println(songSet);		 songSet.addAll(songList);
 and addSong() methods
 song the take HashSet HashSet. one another at has aa It’s collec
 time simple the tion (ex addAll() sam
 cept and e as much use if
 me
thod it we simpler). to added popula tha t eac
 te
 can
 h
}
File Edit Window Help GetBetterMusic
%java Jukebox6
 Before sorting
the ArrayList.
[Pink Moon, Somersault, Shiva Moon, Circles, Deep Channel,
Passenger, Listen, Listen, Listen, Circles]
After sorting
[Circles, Circles, Deep Channel, Listen, Listen, Listen,
 the ArrayList
Passenger, Pink Moon, Shiva Moon, Somersault]
 (by title).
[Pink Moon, Listen, Shiva Moon, Circles, Listen, Deep Channel,
 After putting it
Passenger, Circles, Listen, Somersault]
 into a HashSet,
and printing the
HashSet (we didn’t
The Set didn’t help!!
 (And it lost its sort order
 call sort() again).
We still have all the duplicates ! when HashSet, we put but the we’ll list worry
 into abo
ut
 a
that one later...)
you are here�
 559
object equality
What makes t wo objects equal ?
First, we have to ask—what makes two Song references
duplicates? They must be considered equal. Is it simply two
references to the very same object, or is it two separate objects
that both have the same title?
This brings up a key issue: reference equality vs. object equality.
$ Reference equality
Two
 references, one object on the heap.
Two references that refer to the same object on
the heap are equal. Period. If you call the hashCode() method on
both references, you’ll get the same result. If you don’t override the
hashCode() method, the default behavior (remember, you inherited
this from class Object) is that each object will get a unique number
(most versions of Java assign a hashcode based on the object’s
memory address on the heap, so no two objects will have the same
hashcode).
If you want to know if two references are really referring to the same
object, use the == operator, which (remember) compares the bits in
the variables. If both references point to the same object, the bits will
be identical.
If two objects foo and bar are
equal, foo.equals(bar) must be
true, and both foo and bar must
return the same value from
hashCode(). For a Set to treat
two objects as duplicates, you
must override the hashCode()
and equals() methods inherited
from class Object, so that you
can make two different objects
be viewed as equal.
foo
title: Circles
hashCode: 254
Song
bar
Song
if (foo == bar) {
// both references are referring
// to the same object on the heap
}
$ Object equality
Two
 references, two objects on the heap, but
 title: Circles
hashCode: 254
the objects are considered meaningfully equivalent.
If you want to treat two different Song objects as equal (for
title: Circles
example if you decided that two Songs are the same if they have
 hashCode: 254
matching title variables), you must override both the hashCode()
 foo
and equals() methods inherited from class Object.
As we said above, if you don’t override hashCode(), the default
 Song
behavior (from Object) is to give each object a unique hashcode
 bar
value. So you must override hashCode() to be sure that two
equivalent objects return the same hashcode. But you must also
Song
override equals() so that if you call it on either object, passing in
the other object, always returns true.
if (foo.equals(bar) && foo.hashCode() == bar.hashCode()) {
// both references are referring to either a
// a single object, or to two objects that are equal
}
560
 chapter 16
collections with generics
How a HashSet checks for duplicates: hashCode( ) and equals( )
When you put an object into a Hashset, it uses the
 HashSet finds a matching hashcode for two objects—
object’s hashcode value to determine where to put
 one you’re inserting and one already in the set—the
the object in the Set. But it also compares the object’s
 HashSet will then call one of the object’s equals()
hashcode to the hashcode of all the other objects in
 methods to see if these hashcode-matched objects
the HashSet, and if there’s no matching hashcode,
 really are equal.
the HashSet assumes that this new object is not a
And if they’re equal, the HashSet knows that the
duplicate.
object you’re attempting to add is a duplicate of
In other words, if the hashcodes are different, the
 something in the Set, so the add doesn’t happen.
HashSet assumes there’s no way the objects can be
You don’t get an exception, but the HashSet’s add()
equal!
method returns a boolean to tell you (if you care)
So you must override hashCode() to make sure the
 whether the new object was added. So if the add()
objects have the same value.
 method returns false, you know the new object was a
duplicate of something already in the set.
But two objects with the same hashCode() might not
be equal (more on this on the next page), so if the
I need to know
 Object you’re trying to
if your hashcode
 hashCode
()
 add to the HashSet.
values are the same.
title: Circles
hashCode: 742
foo
 742
Song
HashSet
 hashCo
de()
bar
 Object already IN
Song
 742
 title: hashCode: Circles
 742
 the HashSet.
Your hashcodes
are the same, but are
you REALLY equal?
HashSet
foo
Song
bar
Song
equals( ba
 r )
true
title: Circles
hashCode: 742
Object you’re trying
to add runs its equals()
method, comparing itself
to bar, and returns true.
title: Circles
hashCode: 742
Object already IN
the HashSet.
you are here�
 561
overriding hashCode() and equals()
The Song class with overridden
hashCode() and equals()
class Song implements Comparable<Song>{
String String String String title;
 bpm;
 rating;
 artist;
 The method)
 HashS sends e
t
 (o
r
 it a
 another ny
o
n
e else Song.
 calling
 this
public }
 return Song boolean s = getTitle().equals(s.getTitle());
 (Song) equals(Object aSong;
 aSong) {
 The and title method. GRE Strings if it’s AT
 So have al eq
 ne
 l w
 ual we s
 an is
 to have th
 overridden the at
 to title other do is is equals()
 song’s ask a String,
 one
 title.
public }
 public return int int compareTo(Song hashCode() title.hashCode();
 {
 s) {
 and calling hashCode() Same equals() deal hashCode() here... method, are using th
 on e so the the St yo ring u title. SAME can class just Notice instance has return an how overridden
 variable.
 the hashCode()
 result of
return title.compareTo(s.getTitle());
}
Song(String t, String a, String r, String b) {
title = t;
artist = a;
 Now it works! No duplicates when we
rating = r;
 print out the HashSet. But we didn’t
bpm = b;
 call sort() again, and when we put
}
 the ArrayList into the HashSet, the
HashSet didn’t preserve the sort order.
public String getTitle() {
return title;
 File Edit Window Help RebootWindows
}
%java Jukebox6
public String getArtist() {
return artist;
 [Pink Moon, Somersault, Shiva Moon, Circles,
}
 Deep Channel, Passenger, Listen, Listen,
public String getRating() {
 Listen, Circles]
return rating;
}
 [Circles, Circles, Deep Channel, Listen,
public String getBpm() {
 Listen, Listen, Passenger, Pink Moon, Shiva
return bpm;
 Moon, Somersault]
}
[Pink Moon, Listen, Shiva Moon, Circles,
public String toString() {
 Deep Channel, Passenger, Somersault]
return title;
}
}
562
 chapter 16
Java Object Law For HashCode()
and equals()
The API docs for class Object state the
rules you MUST follow:
$
 have If matching two objects hashcodes.
 are equal, they MUST
$
 equals() If two on objects either object are equal, MUST calling
 return
true. In other words, if (a.equals(b)) then
(b.equals(a)).
$
 value, If two they objects are NOT have required the same to be hashcode
 equal.
But if they’re equal, they MUST have the
same hashcode value.
$
 override So, if hashCode().
 you override equals(), you MUST
$
 is to The generate default abehavior unique integer of hashCode()
 for each
object on the heap. So if you don’t override
hashCode() in a class, no two objects of
that type can EVER be considered equal.
$
 do an The == default comparison. behavior In other of equals() words, is to
 to
test whether the two references refer to a
single object on the heap. So if you don’t
override equals() in a class, no two objects
can EVER be considered equal since
references to two different objects will
always contain a different bit pattern.
a.equals(b) must also mean that
a.hashCode() == b.hashCode()
But a.hashCode() == b.hashCode()
does NOT have to mean a.equals(b)
collections with generics
Dumb there are Questions
 no
Q:
 How come hashcodes can be the same
even if objects aren’t equal?
A:
 HashSets use hashcodes to store the ele-
ments in a way that makes it much faster to access.
If you try to find an object in an ArrayList by giving
the ArrayList a copy of the object (as opposed to
an index value), the ArrayList has to start searching
from the beginning, looking at each element in
the list to see if it matches. But a HashSet can find
an object much more quickly, because it uses the
hashcode as a kind of label on the “bucket” where
it stored the element. So if you say, “I want you
to find an object in the set that’s exactly like this
one...” the HashSet gets the hashcode value from
the copy of the Song you give it (say, 742), and
then the HashSet says, “Oh, I know exactly where
the object with hashcode #742 is stored...”, and it
goes right to the #742 bucket.
This isn’t the whole story you get in a computer
science class, but it’s enough for you to use Hash-
Sets effectively. In reality, developing a good hash-
code algorithm is the subject of many a PhD thesis,
and more than we want to cover in this book.
The point is that hashcodes can be the same
without necessarily guaranteeing that the objects
are equal, because the “hashing algorithm” used in
the hashCode() method might happen to return
the same value for multiple objects. And yes, that
means that multiple objects would all land in the
same bucket in the HashSet (because each bucket
represents a single hashcode value), but that’s not
the end of the world. It might mean that the Hash-
Set is just a little less efficient (or that it’s filled
with an extremely large number of elements), but
if the HashSet finds more than one object in the
same hashcode bucket, the HashSet will simply
use the equals() method to see if there’s a perfect
match. In other words, hashcode values are some-
times used to narrow down the search, but to find
the one exact match, the HashSet still has to take
all the objects in that one bucket (the bucket for
all objects with the same hashcode) and then call
equals() on them to see if the object it’s looking for
is in that bucket.
you are here�
 563
TreeSets and sorting
And if we want the set to stay
sorted, we’ve got TreeSet
TreeSet is similar to HashSet in that it prevents duplicates. But it also keeps the list sorted. It works
just like the sort() method in that if you make a TreeSet using the set’s no-arg constructor, the
TreeSet uses each object’s compareTo() method for the sort. But you have the option of passing
a Comparator to the TreeSet constructor, to have the TreeSet use that instead. The downside to
TreeSet is that if you don’t need sorting, you’re still paying for it with a small performance hit. But
you’ll probably find that the hit is almost impossible to notice for most apps.
import java.util.*;
import java.io.*;
public class Jukebox8 {
ArrayList<Song> songList = new ArrayList<Song>();
int val;
public static void main(String[] args) {
new Jukebox8().go();
}
 public System.out.println(songList);
 getSongs();
 Collections.sort(songList);
 System.out.println(songList);
 void go() {
 Instanti Calling means compareTo(
 (We
 co
 the the at
 ul
d
 e
 ha
 se a
 no-arg ) t ve
 T
 method re
 will pa
 eS
 ss
 use et
 TreeSet ed for instead the in a the Song Comparato
 constr of sort.
 object
 H uc as to
 hSet.
 r r.)
 ’s
TreeSet<Song> songSet = new TreeSet<Song>();
void }
 System.out.println(songSet);
 songSet.addAll(songList);
 getSongs() {
 the songs using We can way addAll(). individually add we added all (Or the using songs we songs songSet.add( cou to fro
 ld
 the hav
 m the
 e
 Arr add
 ) ayList.)
 HashSet
 just
 ed the
try {
File file = new File(“SongListMore.txt”);
BufferedReader reader = new BufferedReader(new FileReader(file));
String line = null;
while ((line= reader.readLine()) != null) {

 addSong(line);
}
} catch(Exception ex) {
ex.printStackTrace();
}
}
void addSong(String lineToParse) {
String[] tokens = lineToParse.split(“/”);
Song nextSong = new Song(tokens[0], tokens[1], tokens[2], tokens[3]);
songList.add(nextSong);
}
}
564
 chapter 16
collections with generics
What you MUST know about TreeSet...
TreeSet looks easy, but make sure you really understand what you need to
do to use it. We thought it was so important that we made it an exercise so
you’d have to think about it. Do NOT turn the page until you’ve done this.
We mean it.
Sharpen your pencil
 import java.util.*;
public class TestTree {
Look at this code.
 public static void main (String[] args) {
Read it carefully, then
 new TestTree().go();
answer the questions
 }
below. (Note: there
are no syntax errors in
 public void go() {
this code.)
 Book b1 = new Book(“How Cats Work”);
Book b2 = new Book(“Remix your Body”);
Book b3 = new Book(“Finding Emo”);
TreeSet<Book> tree = new TreeSet<Book>();
tree.add(b1);
tree.add(b2);
tree.add(b3);
System.out.println(tree);
}
}
class Book {
String title;
public Book(String t) {
title = t;
}
}
1). What is the result when you compile this code?
2). If it compiles, what is the result when you run the TestTree class?
3). If there is a problem (either compile-time or runtime) with this code, how would you fix it?
you are here�
 565
how TreeSets sort
TreeSet elements MUST be comparable
TreeSet can’t read the programmer’s mind to figure out how the
objects should be sorted. You have to tell the TreeSet how.
To use a TreeSet, one of these
things must be true:
$ The
 elements in
the list must be of a type that
implements Comparable
The Book class on the previous
 class Book implements Comparable {
page didn’t implement Comparable, so it
 String title;
wouldn’t work at runtime. Think about it, the
 public Book(String t) {
poor TreeSet’s sole purpose in life is to keep
 title = t;
your elements sorted, and once again—it had
 }
no idea how to sort Book objects! It doesn’t fail
 public int compareTo(Object b) {
at compile-time, because the TreeSet add()
 Book book = (Book) b;
method doesn’t take a Comparable type,The
 return (title.compareTo(book.title));
TreeSet add() method takes whatever type
 }
}
you used when you created the TreeSet. In
other words, if you say new TreeSet<Book>()
the add() method is essentially add(Book). And
there’s no requirement that the Book class
implement Comparable! But it fails at runtime
when you add the second element to the set.
That’s the first time the set tries to call one of
the object’s compareTo() methods and... can’t.
OR
public class BookCompare implements Comparator<Book> {
$
 You use the TreeSet’s
 public int compare(Book one, Book two) {
overloaded constructor
 return (one.title.compareTo(two.title));
that takes a Comparator
 }
}
TreeSet works a lot like the sort()
 class Test {
method—you have a choice of using the
 public void go() {
element’s compareTo() method, assuming
 Book b1 = new Book(“How Cats Work”);
the element type implemented the
 Book b2 = new Book(“Remix your Body”);
Comparable interface, OR you can use
 Book b3 = new Book(“Finding Emo”);
a custom Comparator that knows how
 BookCompare bCompare = new BookCompare();
TreeSet<Book> tree = new TreeSet<Book>(bCompare);
to sort the elements in the set. To use a
 tree.add(new Book(“How Cats Work”);
custom Comparator, you call the TreeSet
 tree.add(new Book(“Finding Emo”);
constructor that takes a Comparator.
 tree.add(new Book(“Remix your Body”);
System.out.println(tree);
}
}
566
 chapter 16
collections with generics
We’ve seen Lists and Sets, now we’ll use a Map
Lists and Sets are great, but sometimes a Map is the best collection (not Collection
with a capital “C”—remember that Maps are part of Java collections but they don’t
implement the Collection interface).
Imagine you want a collection that acts like a property list, where you give it a name
and it gives you back the value associated with that name. Although keys will often be
Strings, they can be any Java object (or, through autoboxing, a primitive).
value
key
“Ball”
 “Ball1”“Ball2”
 “Fish”
 “Ball3”
 “Car”
 “Ball4”
Map
Each element in a Map is actually
TWO objects—a key and a value.
You can have duplicate values, but
NOT duplicate keys.
Map example
import java.util.*;
public class TestMap {
 one HashMap for the needs key TWO and one typ
 for e par the
 ameters—
 value.
public static void main(String[] args) {
HashMap<String,
 Integer> scores = new HashMap <String, Integer>();
scores.put(“Kathy”, scores.put(“Bert”, 343);
 42);
 it Use takes put() two instead arguments of add(), (ke
y, and value).
 now
 of course
scores.put(“Skyler”, 420);
System.out.println(scores);
}
 }
 System.out.println(scores.get(“Bert”));
 The get() the method value (in takes thisa case, key, an
 and
 Integer).
returnsFile Edit Window Help WhereAmI
{Skyler=420, %java TestMap
 Bert=343, when in When braces you youprint { print } instead lists a Map, and of it sets.
 the gives bracke you
 the ts [ key=value,
 ] you see
Kathy=42}
343
you are here�
 567
generic types
Finally, back to generics
If a method argument is an array
Remember earlier in the chapter we talked about how methods
 of Animals, it will also take an
that take arguments with generic types can be... weird. And we
 array of any Animal subtype.
mean weird in the polymorphic sense. If things start to feel
strange here, just keep going—it takes a few pages to really tell
 In other words, if a method is
the whole story.
 declared as:
We’ll start with a reminder on how array arguments work,
 void foo(Animal[] a) { }
polymorphically, and then look at doing the same thing with
generic lists. The code below compiles and runs without errors:
 Assuming Dog extends Animal,
you are free to call both:
Here’s how it works with regular arrays:
foo(anAnimalArray);
foo(aDogArray);
import java.util.*;
public class TestGenerics1 {
public static void main(String[] args) {
new TestGenerics1().go();
}
 Declare and create an Animal
 array,
that holds both dogs and cats.public void go() {
Animal[] animals = {new Dog(), new Cat(), new Dog()};
Dog[] dogs = {new Dog(), new Dog(), new Dog()};
takeAnimals(animals);
 Declare and create a Dog array,
takeAnimals(dogs);
 Cal
l
 tak
eAn
imals(), using both
 that holds only Dogs (the compiler
}
 arr
ay
 typ
 es
 as arguments...
 won’t let you put a Cat in).
}
 public }
 for(Animal }
a.eat();
 void takeAnimals(Animal[] That a: and animal, Remember, we animals) array since didn’t we might the do can animals any {
 hold call casting. ONLY both param
 animals) Dogs the
 ete
 (W
hat
 r and met
hod
s
 is would of Cats.)
 type {
dec
lar
 weAnimal cast ed
 The Dog method it in
 array,
 IS-A crucial to?
 typ
 can e
 Animal. point take is an Polymorphism that Animal[] the takeAn
 or in a action.
 Dog[], imals()
 since
abstract class Animal {
void eat() {
System.out.println(“animal eating”);
}
}
class Dog extends Animal {
}
 void bark() { }
 The simplified Animal cla
ss hierarchy.
class Cat extends Animal {
void meow() { }
}
568
 chapter 16
Using polymorphic arguments and generics
So we saw how the whole thing worked with arrays, but will it work
the same way when we switch from an array to an ArrayList? Sounds
reasonable, doesn’t it?
First, let’s try it with only the Animal ArrayList. We made just a few
changes to the go() method:
collections with generics
Passing in just ArrayList<Animal>
A simple change from Animal[] to
ArrayList<Animal>.
public void go() {
ArrayList<Animal> animals = new ArrayList<Animal>();
animals.add(new Dog());
animals.add(new animals.add(new Cat());
 Dog());
 shortcut We have to syntax add one like at there a time is for
 since
 array there’s creation.
 no
}
 takeAnimals(animals);
 This variable is the refers same to code, an ArrayList except now
 inst the ead “animals”
 of array.
public void takeAnimals(ArrayList<Animal> animals) {
for(Animal a: animals) {
a.eat();
 The method now takes an ArrayL
ist
}
 }
 works the instead same.
 for of Remember, both an array, arrays but that and everythin for collect
ions.
 loop gsyntax
 else is
Compiles and runs just fine
File Edit Window Help CatFoodIsBetter
%java TestGenerics2
animal eating
animal eating
animal eating
animal eating
animal eating
animal eating
you are here�
 569
polymorphism and generics
But will it work with ArrayList<Dog> ?
Because of polymorphism, the compiler let us pass a Dog array
to a method with an Animal array argument. No problem. And
an ArrayList<Animal> can be passed to a method with an
ArrayList<Animal> argument. So the big question is, will the
ArrayList<Animal> argument accept an ArrayList<Dog>? If it works
with arrays, shouldn’t it work here too?
Passing in just ArrayList<Dog>
public void go() {
ArrayList<Animal> animals = new ArrayList<Animal>();
animals.add(new Dog());
animals.add(new Cat());
animals.add(new Dog());
takeAnimals(animals);
 We know this line worked fine.
ArrayList<Dog> dogs = new ArrayList<Dog>();
dogs.add(new dogs.add(new Dog());
 Dog());
 Make a Dog ArrayList and put a
 couple dogs in.
}
 takeAnimals(dogs);
 Will this an array work now to an that ArrayList? we cha
nged
frompublic void takeAnimals(ArrayList<Animal> animals) {
for(Animal a: animals) {
a.eat();
}
}
When we compile it:
File Edit Window Help CatsAreSmarter
%java TestGenerics3
TestGenerics3.java:21: takeAnimals(java.util.
ArrayList<Animal>) in TestGenerics3 cannot be applied to
(java.util.ArrayList<Dog>)
takeAnimals(dogs);
^
1 error
It looked so right,
but went so wrong...
570
 chapter 16
And I’m supposed to be OK with this? That
totally screws my animal simulation where the
veterinary program takes a list of any type of
animal, so that a dog kennel can send a list of dogs,
and a cat kennel can send a list of cats... now
you’re saying I can’t do that if I use collections
instead of arrays?
collections with generics
What could happen if it were allowed...
Imagine the compiler let you get away with that. It let you pass an
ArrayList<Dog> to a method declared as:
public void takeAnimals( ArrayList<Animal> animals) {
for(Animal a: animals) {
a.eat();
}
}
There’s nothing in that method that looks harmful, right? After all,
the whole point of polymorphism is that anything an Animal can
do (in this case, the eat() method), a Dog can do as well. So what’s
the problem with having the method call eat() on each of the Dog
references?
Nothing. Nothing at all.
There’s nothing wrong with that code. But imagine this code instead:
public void takeAnimals(ArrayList<Animal> animals) {
animals.add(new Cat());
 Yikes!! We just stuck a Cat in what
}
 might be a Dogs-only ArrayList.
So that’s the problem. There’s certainly nothing wrong with adding a
Cat to an ArrayList<Animal>, and that’s the whole point of having an
ArrayList of a supertype like Animal—so that you can put all types of
animals in a single Animal ArrayList.
But if you passed a Dog ArrayList—one meant to hold ONLY Dogs—
to this method that takes an Animal ArrayList, then suddenly you’d
end up with a Cat in the Dog list. The compiler knows that if it lets
you pass a Dog ArrayList into the method like that, someone could,
at runtime, add a Cat to your Dog list. So instead, the compiler just
won’t let you take the risk.
If you declare a method to take ArrayList<Animal> it can take ONLY an
ArrayList<Animal>, not ArrayList<Dog> or ArrayList<Cat>.
you are here�
571
arrays vs. ArrayLists
Wait a minute... if this is why they won’t let
you pass a Dog ArrayList into a method that
takes an Animal ArrayList—to stop you from
possibly putting a Cat in what was actually a Dog list,
then why does it work with arrays? Don’t you have
the same problem with arrays? Can’t you still add
a Cat object to a Dog[] ?
Array types are checked again at
runtime, but collection type checks
happen only when you compile
Whew ! At least the
JVM stopped it.
Let’s say you do add a Cat to an array declared as Dog[] (an array that
was passed into a method argument declared as Animal[], which is a
perfectly legal assignment for arrays).
public void go() {
Dog[] dogs = {new Dog(), new Dog(), new Dog()};
takeAnimals(dogs);
}
public void takeAnimals(Animal[] animals) {
animals[0] = new Cat();
}
 We put a new Cat into a Dog array. The
compiler allowed it, because it knows that
you might have passed a Cat array or Animal
array to the method, so to the compiler it
was possible that this was OK.
It compiles, but when we run it:
File Edit Window Help CatsAreSmarter
%java TestGenerics1
Exception in thread “main” java.lang.ArrayStoreException:
Cat
at TestGenerics1.takeAnimals(TestGenerics1.java:16)
at TestGenerics1.go(TestGenerics1.java:12)
at TestGenerics1.main(TestGenerics1.java:5)
572
 chapter 16
collections with generics
Wouldn’t it be dreamy if there were
a way to still use polymorphic collection
types as method arguments, so that my
veterinary program could take Dog lists
and Cat lists? That way I could loop through
the lists and call their immunize() method,
but it would still have to be safe so that you
couldn’t add a Cat in to the Dog list. But I
guess that’s just a fantasy...
you are here�
 573
generic wildcards
Wildcards to the rescue
It looks unusual, but there is a way to create a method argument that
can accept an ArrayList of any Animal subtype. The simplest way is to
use a wildcard—added to the Java language explicitly for this reason.
public void takeAnimals(ArrayList<?
 extends Animal> animals) {
for(Animal a: animals) {
a.eat();
 Remember, the keyword “extends”
}
 here means either extends OR
}
 implements depending on the
So now you’re wondering, “What’s the difference? Don’t you have
 type. So if you want to take
the same problem as before? The method above isn’t doing
 an ArrayList of types that
anything guaranteed dangerous—calling to have—but can’t a someone method any still Animal change subtype this to add is
 a
 implement the Pet interface,
Cat to the animals list, even though it’s really an ArrayList<Dog>?
 you’d declare it as:
And different since from it’s not declaring checked it again without at the runtime, wildcard?”
 how is this any
 ArrayList<? extends Pet>
And you’d be right for wondering. The answer is NO. When you
use the wildcard <?> in your declaration, the compiler won’t let
you do anything that adds to the list!
When you use a wildcard in your method
argument, the compiler will STOP you from doing
anything that could hurt the list referenced by the
method parameter.
You can still invoke methods on the elements in
the list, but you cannot add elements to the list.
In other words, you can do things with the list
elements, but you can’t put new things in the list.
So you’re safe at runtime, because the compiler
won’t let you do anything that might be horrible at
runtime.
So, this is OK inside takeAnimals():
for(Animal a: animals) {
a.eat();
}
But THIS would not compile:
animals.add(new Cat());
574
 chapter 16
Alternate syntax for doing the same thing
You probably remember that when we looked at the sort() method,
it used a generic type, but with an unusual format where the type
parameter was declared before the return type. It’s just a different way
of declaring the type parameter, but the results are the same:
collections with generics
This:
public <T extends Animal> void takeThing(ArrayList<T> list)
Does the same thing as this:
public void takeThing(ArrayList<? extends Animal> list)
Dumb there are Questions
 no
Q:
 If they both do the same thing, why would you use
one over the other?
A:
 It all depends on whether you want to use “T” some-
where else. For example, what if you want the method to
have two arguments—both of which are lists of a type that
extend Animal? In that case, it’s more efficient to just declare
the type parameter once:
public <T extends Animal> void takeThing(ArrayList <T> one,ArrayList<T> two)
Instead of typing:
public void takeThing(ArrayList<? extends Animal> one,
ArrayList<? extends Animal> two)
you are here�
 575
be the compiler exercise
Exercise
BE the compiler, advanced
Your job is to play compiler and determine which of
these statements would compile. But some of this
code wasn’t covered in the chapter, so you need to
work out the answers based on what you DID learn,
applying the “rules” to these new situations. In
some cases, you might have to guess, but the
point is to come up with a reasonable answer
based on what you know so far.
(Note: assume that this code is within a legal class
and method.)
Compiles?
❑ ArrayList<Dog> dogs1 = new ArrayList<Animal>();
❑ ArrayList<Animal> animals1 = new ArrayList<Dog>();
❑ List<Animal> list = new ArrayList<Animal>();
❑ ArrayList<Dog> dogs = new ArrayList<Dog>();
❑ ArrayList<Animal> animals = dogs;
❑ List<Dog> dogList = dogs;
❑ ArrayList<Object> objects = new ArrayList<Object>();
❑ List<Object> objList = objects;
❑ ArrayList<Object> objs = new ArrayList<Dog>();
576
 chapter 16
collections with generics
import java.util.*;
 Solution to the “Reverse
public class SortMountains {
 Engineer” sharpen exercise
LinkedList<Mountain> mtn = new LinkedList<Mountain>();
class NameCompare implements Comparator <Mountain> {
public int compare(Mountain one, Mountain two) {
return one.name.compareTo(two.name);
}
}
class HeightCompare implements Comparator <Mountain> {
public int compare(Mountain one, Mountain two) {
return (two.height - one.height);
}
}
 public new SortMountains().go();
 static void main(String [] args) {
 Did in DESCENDING you notice that
 sequen
 thece height ? : list )
 is
}
public void go() {
mtn.add(new Mountain(“Longs”, 14255));
mtn.add(new Mountain(“Elbert”, 14433));
mtn.add(new Mountain(“Maroon”, 14156));
mtn.add(new Mountain(“Castle”, 14265));
System.out.println(“as entered:\n” + mtn);
NameCompare nc = new NameCompare();
Collections.sort(mtn, nc);
System.out.println(“by name:\n” + mtn);
HeightCompare hc = new HeightCompare();
Collections.sort(mtn, hc);
System.out.println(“by height:\n” + mtn);
}
}
class Mountain {
String name;
int height;
Mountain(String n, int h) {
name = n;
height = h;
}
public String toString( ) {
return name + “ “ + height;
}
Output:
File Edit Window Help ThisOne’sForBob
%java SortMountains
as entered:
[Longs 14255, Elbert 14433, Maroon 14156, Castle 14265]
by name:
[Castle 14265, Elbert 14433, Longs 14255, Maroon 14156]
by height:
[Elbert 14433, Castle 14265, Longs 14255, Maroon 14156]
you are here�
 577
fill-in-the-blank solution
Exercise Solution
Possible Answers:
Comparator,
Comparable,
compareTo( ),
compare( ),
yes,
no
Given the following compilable statement:
Collections.sort(myArrayList);
1. What must the class of the objects stored in myArrayList implement?
 Comparable
2. What method must the class of the objects stored in myArrayList implement? compareTo( )
3. Can the class of the objects stored in myArrayList implement both
Comparator AND Comparable?
 yes
Given the following compilable statement:
Collections.sort(myArrayList,myCompare);
4. Can the class of the objects stored in myArrayList implement Comparable?
5. Can the class of the objects stored in myArrayList implement Comparator?
6. Must the class of the objects stored in myArrayList implement Comparable?7. Must the class of the objects stored in myArrayList implement Comparator?8. What must the class of the myCompare object implement?
9. What method must the class of the myCompare object implement?
yes
yes
no
no
Comparator
compare( )
578
 chapter 16
BE the compiler solution
collections with generics
Compiles?
❑ ArrayList<Dog> dogs1 = new ArrayList<Animal>();
❑ ArrayList<Animal> animals1 = new ArrayList<Dog>();
❑ List<Animal> list = new ArrayList<Animal>();
❑ ArrayList<Dog> dogs = new ArrayList<Dog>();
❑ ArrayList<Animal> animals = dogs;
❑ List<Dog> dogList = dogs;
❑ ArrayList<Object> objects = new ArrayList<Object>();
❑ List<Object> objList = objects;
❑ ArrayList<Object> objs = new ArrayList<Dog>();
you are here�
 579
17 package, jars and deployment
Release Your Code
It’s time to let go. You wrote your code. You tested your code. You refined your code.
You told everyone you know that if you never saw a line of code again, that’d be fine. But in the
end, you’ve created a work of art. The thing actually runs! But now what? How do you give it to
end users? What exactly do you give to end users? What if you don’t even know who your end
users are? In these final two chapters, we’ll explore how to organize, package, and deploy your
Java code. We’ll look at local, semi-local, and remote deployment options including executable
jars, Java Web Start, RMI, and Servlets. In this chapter, we’ll spend most of our time on organizing
and packaging your code—things you’ll need to know regardless of your ultimate deployment
choice. In the final chapter, we’ll finish with one of the coolest things you can do in Java. Relax.
Releasing your code is not saying goodbye. There’s always maintenance...
this is a new chapter
 581
Java deployment
Deploying your application
What exactly is a Java application? In other words,
once you’re done with development, what is it that you
deliver? Chances are, your end-users don’t have a system
identical to yours. More importantly, they don’t have your
application. So now it’s time to get your program in shape
for deployment into The Outside World. In this chapter,
we’ll look at local deployments, including Executable Jars
and the part-local/part-remote technology called Java Web
Start. In the next chapter, we’ll look at the more remote
deployment options, including RMI and Servlets.
Deployment options
File Edit View
 File Edit View
 File Edit View
 HTTP
HTTP
 RMI
 Servlets
Executable
Jar
 Web Start
 RMI app
100% Local
 Combination
 100% Remote
1
Local
The entire application runs on the
end-user’s computer, as a stand-alone,
probably GUI, program, deployed as
an executable JAR (we’ll look at JAR
in a few pages.)
2
Combination of local and remote
The application is distributed with a
client portion running on the user’s
local system, connected to a server
where other parts of the application
are running.
3
Remote
The entire Java application runs on a
server system, with the client accessing
the system through some non-Java
means, probably a web browser.
But before we really get into the whole deployment thing,
let’s take a step back and look at what happens when you’ve
finished programming your app and you simply want to pull
out the class files to give them to an end-user. What’s really
in that working directory?
582
 chapter 17
A Java program is a bunch
of classes. That’s the
output of your development.
The real question is what
to do with those classes
when you’re done.
br Brain
 ain barbell
 Barbell
What are the advantages and
disadvantages of delivering your
Java program as a local, stand-
alone application running on
the end-user’s computer?
What are the advantages and
disadvantages of delivering your
Java program as web-based
system where the user interacts
with a web browser, and the
Java code runs as servlets on the
server?
It’s
finallydone!
package, jars and deployment
Imagine this scenario...
Bob’s happily at work on the final pieces of his cool new
Java program. After weeks of being in the “I’m-just-one-
compile-away” mode, this time he’s really done.
The program is a fairly sophisticated GUI app,
but since the bulk of it is Swing code, he’s
made only nine classes of his own.
At last, it’s time to deliver the program to the
client. He figures all he has to do is copy the
nine class files, since the client already has the
Java API installed. He starts by doing an ls
on the directory where all his files are...
What the... ?
Whoa! Something strange has happened. Instead of 18
files (nine source code files and nine compiled class
files), he sees 31 files, many of which have very strange
names like:
Account$FileListener.class
Chart$SaveListener.class
and on it goes. He had completely forgotten
that the compiler has to generate class files
for all those inner class GUI event listeners
he made, and that’s what all the strangely-
named classes are.
Now he has to carefully extract all the class
files he needs. If he leaves even one of them out,
his program won’t work. But it’s tricky since he
doesn’t want to accidentally send the client
one of his source code files, yet everything is
in the same directory in one big mess.
you are here�
 583
organizing your classes
Separate source code and class files
A single directory with a pile of source code and class files is a
mess. It turns out, Bob should have been organizing his files
 But I thought I didn’t have
a choice about putting the class
from the beginning, keeping the source code and compiled
files in with the source files.
code separate. In other words, making sure his compiled class
When you compile, they just go
files didn’t land in the same directory as his source code.
there, so what do I do?
The key is a combination of directory structure organization and the
-d compiler option.
There are dozens of ways you can organize your files, and your
company might have a specific way they want you to do it. We
recommend an organizational scheme that’s become almost
standard, though.
With this scheme, you create a project directory, and inside
that you create a directory called source and a directory called
classes. You start by saving your source code (.java files) into
the source directory. Then the trick is to compile your code
in such a way that the output (the .class files) ends up in the
classes directory.
And there’s a nice compiler flag, -d, that lets you do that.
Compiling with the -d (directory) flag
%cd MyProject/source
%javac -d ../classes
 MyApp.java
MyProject
current back that’s into compiled tells the down the one working “classes: code compiler directory again (clas
 direct from di to s re up fi ct pu
t ory.
 the
 les)
 ory
 and
 the
 the the file last name to com
 thing of
 pile
 th is e still
 java
 compiled lands here
 code
 THIS compile directo
 from ry
By using the -d flag, you get to decide which directory the
 classes
 source
compiled code lands in, rather than accepting the default of
To class compile files landing all the in .java the files same in directory the source as directory, the source use:
 code.
 f
 run rom your here
 main()
 101101
 Lorper
%javac
 -d ../classes
 *.java
 101101
 10101000010
 1010 10 0
 iure tat conse
 vero
 eugue
*.java compiles ALL
 1010101
 10101010
 01010 1
 eugueroLore
 do do eliquis
 del dip
Running your code
 source files in the
 1001010101
current directory
 MyApp.class
 MyApp.java
%cd MyProject/classes
 (troubleshooting note: everything in this chapter assumes that the
%java MyApp
 the run ‘classes’ your program
 direct ory.
 from
 that have current explicitly it contains working set directory the a classpath “.” )
 (i.e. environment the “.”) is in variable, your classpath. be certain
 If you
584
 chapter 17
PutJAR
yourpackage, jars and deployment
Java in a JAR
A JAR file is a Java ARchive. It’s based on the pkzip file format, and it lets you bundle
all your classes so that instead of presenting your client with 28 class files, you hand
over just a single JAR file. If you’re familiar with the tar command on UNIX, you’ll
recognize the jar tool commands. (Note: when we say JAR in all caps, we’re referring
to the archive file. When we use lowercase, we’re referring to the jar tool you use to
create JAR files.)
The question is, what does the client do with the JAR? How do you get it to run?
You make the JAR executable.
An executable JAR means the end-user doesn’t have to pull the class files out before
running the program. The user can run the app while the class files are still in the
JAR. The trick is to create a manifest file, that goes in the JAR and holds information
about the files in the JAR. To make a JAR executable, the manifest must tell the JVM
which class has the main() method!
Making an executable JAR
1
Make sure all of your class files are in
the classes directory
We’re going to refine this in a few pages, but
for now, keep all your class files sitting in the
directory named ‘classes’.
MyProject
classes
2
Create a manifest.txt file that states
which class has the main() method
Make a text file named manifest.txt that has
one line:
 don’t put the .class
Main-Class: MyApp
 on the end
Press the return key after typing the Main-
Class line, or your manifest may not work
correctly. Put the manifest file into the “classes”
directory.
Main-Class: MyiApp
manifest.txt
3
 Run the jar tool to create a JAR file
that contains everything in the classes
directory, plus the manifest.
%cd MyProject/classes
%jar -cvmf manifest.txt app1.jar *.class
OR
%jar -cvmf manifest.txt app1.jar MyApp.class
classes
app1.jar
101101
101101
10101000010
 101101
1010
 101101
 10 0
01010
 10101000010
 1
101101
 1010101
 1010 10 0
101101
 10101010
 01010
 1
10101000010
 1001010101
 1010101
1010 10101010
 10 0
01010
 1001010101
 1
1010101
10101010
1001010101
MyApp.class
MyProject
classes
101101
101101
 101101
101101
101101
101101
Main-Class: MiniApp
manifest.txt
101101
101101
 101101
101101
101101
101101
Main-Class: MiniApp
no source
code (.java)
in the JAR
you are here�
 585
executable JAR
File
 File Edit
 EditView
 View
File EditView
 HTTP
 File Edit View
 RMI
 HTTP
Executable
GUI Jar
 client
 Web Start
 RMI app
 Servlets
100% Local
 Combination
 100% Remote
Most 100% local Java
apps are deployed as
executable JAR files.
Running (executing) the JAR
Java (the JVM) is capable of loading a class from a JAR, and calling
the main() method of that class. In fact, the entire application can
stay in the JAR. Once the ball is rolling (i.e., the main() method
starts running), the JVM doesn’t care where your classes come
from, as long as it can find them. And one of the places the JVM
looks is within any JAR files in the classpath. If it can see a JAR, the
JVM will look in that JAR when it needs to find and load a class.
 The it easiest is to must JVM make way behas in
 your to you
 to make worki r
 ‘see’ cla
 th ssp
ath. ng the e dir
 JAR JAR, ectory
 The
 visible
 so
the place where the JA
R is.classes
 %cd MyProject/classes
101101
101101
 101101
app1.jar
 Main-Class: 101101
 101101
 MiniApp
 101101
 JAR JVM The %java instead -jar
 you’r f e la -jar g of g iving tells a class.
 it app1.jar
 the
 a
 The a Class. amanif runtim
 JVM If est e it looks
 with exception.
 doesn’t in
 an s
id
 entry e
 find
 t
h
iso f JAR ne, or you M
 f a o in r get
 -
Depending on how your operating system is configured, you
might even be able to simply double-click the JAR file to launch
it. This works on most flavors of Windows, and Mac OS X. You
can usually make this happen by selecting the JAR and telling
the OS to “Open with...” (or whatever the equivalent is on your
operating system).
Dumb there are Questions
 no
Q:
 Why can’t I just JAR up an entire directory?
 Q:
 What did you just say?
A:
 The JVM looks inside the JAR and expects to find
 A:
 You can’t put your class files into some arbitrary
what it needs right there. It won’t go digging into other
 directory and JAR them up that way. But if your classes
directories, unless the class is part of a package, and even
 belong to packages, you can JAR up the entire package
then the JVM looks only in the directories that match the
 directory structure. In fact, you must. We’ll explain all this on
package statement.
 the next page, so you can relax.
586
 chapter 17
Put your classes in packages!
So you’ve written some nicely reusable class files, and you’ve
posted them in your internal development library for other
programmers to use. While basking in the glow of having
just delivered some of the (in your humble opinion) best
examples of OO ever conceived, you get a phone call. A
frantic one. Two of your classes have the same name as
the classes Fred just delivered to the library. And all hell is
breaking loose out there, as naming collisions and ambiguities
bring development to its knees.
And all because you didn’t use packages! Well, you did use
packages, in the sense of using classes in the Java API that are,
of course, in packages. But you didn’t put your own classes
into packages, and in the Real World, that’s Really Bad.
We’re going to modify the organizational structure from the
previous pages, just a little, to put classes into a package, and
to JAR the entire package. Pay very close attention to the
subtle and picky details. Even the tiniest deviation can stop
your code from compiling and/or running.
Packages prevent class name conflicts
Although packages aren’t just for preventing name collisions,
that’s a key feature. You might write a class named Customer
and a class named Account and a class named ShoppingCart.
And what do you know, half of all developers working in
enterprise e-commerce have probably written classes with
those names. In an OO world, that’s just dangerous. If part of
the point of OO is to write reusable components, developers
need to be able to piece together components from a
variety of sources, and build something new out of them.
Your components have to be able to ‘play well with others’,
including those you didn’t write or even know about.
Remember way back in chapter 6 when we discussed how
a package name is like the full name of a class, technically
known as the fully-qualified name. Class ArrayList is really
java.util.ArrayList, JButton is really javax.swing.JButton, and
Socket is really java.net.Socket. Notice that two of those classes,
ArrayList and Socket, both have java as their “first name”.
In other words, the first part of their fully-qualified names
is “java”. Think of a hierarchy when you think of package
structures, and organize your classes accordingly.
package, jars and deployment
Package structure of the Java API for:
java.text.NumberFormat
java.util.ArrayList
java.awt.FlowLayout
java.awt.event.ActionEvent
java.net.Socket
java
101101 10
 net
1001 10 10010
 101 10
 text
101 101
101 101 101
 awt
 101101 1001 10 10110010
 101 101
 10
10
NumberFormat
 util
 101 101 101
Socket
101101 10 1001 10010
 101 10
10
 101101 101 10 1001 10010
 101 101
 10
10
 event
101 101
 101 101 101
101 101 101
ArrayList
 FlowLayout
101101 10
1001 101 10
10 10010
101 101
101 101 101
ActionEvent
What does this picture look like to
you? Doesn’t it look a whole lot like
a directory hierarchy?
you are here�
 587
package naming
...so I finally settled on
foo.bar.Heisenberg for my
quantum baking class
Why, that’s the same name
I was thinking of for my
sub-atomic ironing class!
Guess I’ll just have to come
up with something else.
Packages can prevent name
conflicts, but only if you
choose a package name
that’s guaranteed to be
unique. The best way to
Preventing package name conflicts
 do that is to preface your
Putting your class in a package reduces the chances of naming
 packages with your reverse
conflicts with other classes, but what’s to stop two programmers
from coming up with identical package names? In other words,
 domain name.
what’s to stop two programmers, each with a class named Account,
from putting the class in a package named shopping.customers?
 com.headfirstbooks.Book
Both classes, in that case, would still have the same name:
 package name
 class na
me
shopping.customers.Account
Sun strongly suggests a package naming convention that greatly
reduces that risk—prepend every class with your reverse domain
name. Remember, domain names are guaranteed to be unique.
Two different guys can be named Bartholomew Simpson, but two
different domains cannot be named doh.com.
Reverse com.headfirstjava.projects.Chart
 domain package names
 th alwa
 e
 c
 ys l
a
s
s capit
 nam a e lized
 is
start then domain, structure add the separated your package after own th
 wit by at
 org
 a h anizat
ional
 do
t your(.),
 reverse
 our means name, projects.Chart own but we in-house have adding to might developers. worry com.headf
irstjava
 be about
 a commo
n
 only
588
 chapter 17
package, jars and deployment
To put your class in a package:
1
Choose a package name
We’re using com.headfirstjava as our
example. The class name is PackageExercise,
so the fully-qualified name of the class is now:
com.headfirstjava.PackageExercise.
2
Put a package statement in your class
It must be the first statement in the source
code file, above any import statements. There
can be only one package statement per source
code file, so all classes in a source file must
be in the same package. That includes inner
classes, of course.
package com.headfirstjava;
import javax.swing.*;
public class PackageExercise {
// life-altering code here
}
You must put a class
into a directory
structure that matches
the package hierarchy.
MyProject
classes
 source
3
 Set up a matching directory structure
It’s not enough to say your class is in a package,
by merely putting a package statement in
the code. Your class isn’t truly in a package
until you put the class in a matching directory
structure. So, if the fully-qualified class name
is com.headfirstjava.PackageExercise, you
must put the PackageExercise source code in a
directory named headfirstjava, which must be in
a directory named com.
It is possible to compile without doing that, but
trust us—it’s not worth the other problems
you’ll have. Keep your source code in a directory
structure that matches the package structure,
and you’ll avoid a ton of painful headaches down
the road.
e
 r com
 com
utcurtsegakc a headfirstjava
 headfirstjava
p101101
101101
10101000010
1010 10 0
01010 1
1010101
10101010
1001010101
PackageExercise.class
Lorper
iure eugue
tat vero
conse
eugueroLore
do eliquis
do del dip
PackageExercise.java
Set up a matching directory structure for
both the source and classes trees.
you are here�
 589
compile and run with packages
Compiling and running with packages
When your class is in a package, it’s a little trickier to compile and
run. The main issue is that both the compiler and JVM have to be
capable of finding your class and all of the other classes it uses.
For the classes in the core API, that’s never a problem. Java always
knows where its own stuff is. But for your classes, the solution
of compiling from the same directory where the source files are
simply won’t work (or at least not reliably). We guarantee, though,
that if you follow the structure we describe on this page, you’ll be
successful. There are other ways to do it, but this is the one we’ve
found the most reliable and the easiest to stick to.
Compiling with the -d (directory) flag
%cd MyProject/source
 stay into the in the directory source direct
 where ory the ! Do .ja
va NOT file cd is!
 down
%javac
 -d ../classes
 com/headfirstjava/PackageExercise.java
into compiled tells the the classes code compiler (clas
s
 direct to fi
les
 pu
t ory,
 )
 the
 Now the actual PATH you source
 have to ge
 to file t specify
 .
to the
within the right packag
e
 MyProject
To compile structure!! all the Yes,.java it know files s.
 in the com.headfirstjava
 you’ll from
st here
 ill run
package, use:
%javac
 -d ../classes
 com/headfirstjava/*.java
 classes
file compiles in this ever
 di y re so ct ur or ce y
 (.java)
Running your code
 com
%cd MyProject/classes
 the run ‘classes’ your prog di ra
 rectory.
 m
 from
%java com.headfirstjava.PackageExercise
 headfirstjava
directory, there it (classes) see You expects that, MUST it expects and or and to gi
 even expect ve find im to th
 m
ed
 in a e find “classes”, to iately directory fully-qualified find the look a class. it directory named inside won
’t If cl
 the its headfirstj ass work!
 named curr
 name! class ent is The com, ava, in directory
 the JV
 and where
 M “com”
 in
 will
 PackageExercise.class
 101101
 10 001 0 001 11 110 10
 01
 0
 1
590
 chapter 17
you’ll still compile
from here
source
com
headfirstjava
Lorper
iure
eugue
tat vero
conse
euguero-
PackageExercise.java
The -d flag is even cooler than we said
Compiling with the -d flag is wonderful because not only does
it let you send your compiled class files into a directory other
than the one where the source file is, but it also knows to put
the class into the correct directory structure for the package
the class is in.
But it gets even better!
Let’s say that you have a nice directory
structure all set up for your source
 MyProject
code. But you haven’t set up a
matching directory structure for
your classes directory. Not
a problem! Compiling with
-d tells the compiler to not
 classes
 source
just put your classes into the
correct directory tree, but to
build the directories if they
don’t exist.
If the package directory
 structure
 com
doesn’t directory, directories exist the if under you
 compile use
 th r the e wil
l ‘classes’
 -d
 build flag.
 the
So the physically fact, you ‘classes’ if don’t you create roo
 let actually t
 the th dir
 e
 com
 ect
 dir
ectories have ory
. piler to
 And do under
 it
 in
 headfirstjava
 iure
 Lorper
there’s no chance of a typo.
 eugue
 tat vero
conse
euguero-
PackageExercise.java
The -d flag tells the compiler,
“Put the class into its package
directory structure, using the
class specified after the -d as
the root directory. But... if the
directories aren’t there, create
them first and then put the class
in the right place!”
package, jars and deployment
Dumb there are Questions
 no
Q:
 I tried to cd into the
directory where my main class
was, but now the JVM says it
can’t find my class! But it’s right
THERE in the current directory!
A:
 Once your class is in a
package, you can’t call it by its
‘short’ name. You MUST specify,
at the command-line, the fully-
qualified name of the class whose
main() method you want to run.
But since the fully-qualified name
includes the package structure,
Java insists that the class be in a
matching directory structure. So if
at the command-line you say:
%java com.foo.Book
the JVM will look in its current
directory (and the rest of its
classpath), for a directory named
“com”. It will not look for a class
named Book, until it has found
a directory named “com” with
a directory inside named “foo”.
Only then will the JVM accept that
its found the correct Book class.
If it finds a Book class anywhere
else, it assumes the class isn’t in
the right structure, even if it is! The
JVM won’t for example, look back
up the directory tree to say, “Oh, I
can see that above us is a directory
named com, so this must be the
right package...”
you are here�
 591
JARs and packages
Making an executable JAR with packages
JAR
When your class is in a package, the package directory structure
must be inside the JAR! You can’t just pop your classes in the
JAR the way we did pre-packages. And you must be sure that you
don’t include any other directories above your package. The
first directory of your package (usually com) must be the first
directory within the JAR! If you were to accidentally include the
directory above the package (e.g. the “classes” directory), the JAR
wouldn’t work correctly.
Making an executable JAR
1
 Make sure all of your class files are
within the correct package structure,
under the classes directory.
2
Create a manifest.txt file that states
which class has the main() method,
and be sure to use the fully-qualified
class name!
Make a text file named manifest.txt that has a
single line:
Main-Class: com.headfirstjava.PackageExercise
Put the manifest file into the classes directory
classes
com
headfirstjava
101101
10 110 1
0 11 0
001 10
001 01
PackageExercise.class
classes
com
Main-Class: MiniApp
manifest.txt
101101
10 110 1
0 11 0
headfirstjava
 001 10
001 01
PackageExercise.class
3
 Run the jar tool to create a JAR file
that contains the package directories
 classes
plus the manifest
The only thing you need to include is the ‘com’
%cd directory, will go MyProject/classes
 intoand thethe JAR.
 entire package (and all classes)
 All com get you dire ever
 sp c ything e t c o if ry! y is And in the
 it!
 you’ll
 packEx.jar
%jar -cvmf
 manifest.txt
 packEx.jar
 com
592
 chapter 17
com
101101
10 110 1
headfirstjava
 0 11 0
001 10
001 01
PackageExercise.class
package, jars and deployment
So where did the manifest file go?
Why don’t we look inside the JAR and find out? From the
‘untarring’).
 command-line, JAR. You can extract the jar the tool contents can do of more a JAR than (just just like create ‘unzipping’ and run or
 a
 we directo
 put ry t
h
e
 named J
A
R
 file Skyler
 into a
Imagine you’ve put the packEx.jar into a directory named Skyler.
Skyler
jar commands for listing and extracting
1
 List the contents of a JAR
% jar -tf packEx.jar
% % File jar cd Edit Window Skyler
 -tf “show -tf Help stan
 packEx.jar
 me Pickle
 d a s t f a or ble ‘Table of the File’ JA
R as file”
 in
 the builds directo manifest
 jar
 a t
 ry, META-INF
 o
o
 inside.
 l
 and a
u
tomatically
 puts the
 packEx.jar
META-INF/
 META-INF
 com
META-INF/MANIFEST.MF
com/
com/headfirstjava/
 Main-Class: MiniApp
com/headfirstjava/
PackageExercise.class
 headfirstjava
MANIFEST.MF
101101
10 110 1
0 11 0
001 10
2
 Extract the contents of a JAR (i.e. unjar)
 001 01
% cd Skyler
 PackageExercise.class
% jar -xf packEx.jar
Skyler
 META-INF stands for ‘meta
information’. The jar tool creates
-xf you works the directory stands extract META-INF just
 in like
 for the you
 unz
 ‘Extract r
 packEx.j
 direct cur
 ipp
ing
 ren
 ory
 t or
 File’ ar,
 direct and
 untarring. you’ll and the
 ory
 it
 see
 com
 If
 META-INF
 Main-Class: MiniApp
 com
 the It well into your also META-INF the as manifest the takes MANIFEST.MF MANIFEST.MF the file, directory contents and puts file. as
 of
 file.
 it
 So,
headfirstjava
 your manifest file doesn’t go into
MANIFEST.MF
 the JAR, but the contents of it
101101
 10 110 1
 are put into the ‘real’ manifest
0 001 001 1110
 01
 0
 (MANIFEST.MF).
PackageExercise.class
you are here�
 593
organizing your classes
Sharpen your pencil
MyProject
classes
Main-Class: MiniApp
source
Given the package/directory structure in this
picture, figure out what you should type at the
command-line to compile, run, create a JAR, and
execute a JAR. Assume we’re using the standard
where the package directory structure starts just
below source and classes. In other words, the source
and classes directories are not part of the package.
javaranch
manifest.txt
javaranch
cows
101101
10 110 1
0 11 0
001 10
001 01
Foof.class
cows
Lorper
iure
eugue
tat vero
conse
euguero-
Foof.java
Compile:
%cd source
%javac ________________________________
Run:
%cd ___________
%java _________________________________
Create a JAR
%cd ___________
%______________________________________
Execute a JAR
%cd ___________
% _____________________________________
Bonus question: What’s wrong with the package name?
594
 chapter 17
Dumb there are Questions
 no
Q:
 What happens if you try
to run an executable JAR, and
the end-user doesn’t have java
installed?
A:
 Nothing will run, since
without a JVM, Java code can’t
run. The end-user must have
Java installed.
Q:
 How can I get Java
installed on the end-user’s
machine?
Ideally, you can create a custom
installer and distribute it along
with your application. Several
companies offer installer pro-
grams ranging from simple to
extremely powerful. An installer
program could, for example, de-
tect whether or not the end-user
has an appropropriate version
of Java installed, and if not,
install and configure Java before
installing your application.
Installshield, InstallAnywhere,
and DeployDirector all offer Java
installer solutions.
Another cool thing about some
of the installer programs is that
you can even make a deploy-
ment CD-ROM that includes
installers for all major Java
platforms, so... one CD to rule
them all. If the user’s running on
Solaris, for example, the Solaris
version of Java is installed. On
Windows, the Windows, ver-
sion, etc. If you have the budget,
this is by far the easiest way for
your end-users to get the right
version of Java installed and
configured.
package, jars and deployment
�������������BULLET POINTS
Organize your project so that your source code and class files are not in
the same directory.
A standard organization structure is to create a project directory, and then
put a source directory and a classes directory inside the project directory.
Organizing your classes into packages prevents naming collisions with
other classes, if you prepend your reverse domain name on to the front of
a class name.
To put a class in a package, put a package statement at the top of the
source code file, before any import statements:
package com.wickedlysmart;
To be in a package, a class must be in a directory structure that exactly
matches the package structure. For a class, com.wickedlysmart.Foo,
the Foo class must be in a directory named wickedlysmart, which is in a
directory named com.
To make your compiled class land in the correct package directory
structure under the classes directory, use the -d compiler flag:
% cd source
% javac -d ../classes com/wickedlysmart/Foo.java
To run your code, cd to the classes directory, and give the fully-qualified
name of your class:
% cd classes
% java com.wickedlysmart.Foo
You can bundle your classes into JAR (Java ARchive) files. JAR is based
on the pkzip format.
You can make an executable JAR file by putting a manifest into the JAR
that states which class has the main() method. To create a manifest file,
make a text file with an entry like the following (for example):
Main-Class: com.wickedlysmart.Foo
Be sure you hit the return key after typing the Main-Class line, or your
manifest file may not work.
To create a JAR file, type:
jar -cvfm manifest.txt MyJar.jar com
The entire package directory structure (and only the directories matching
the package) must be immediately inside the JAR file.
To run an executable JAR file, type:
java -jar MyJar.jar
you are here�
 595
wouldn’t it be dreamy...
Executable JAR files are nice, but wouldn’t
it be dreamy if there were a way to make
a rich, stand-alone client GUI that could
be distributed over the Web? So that you
wouldn’t have to press and distribute all those
CD-ROMs. And wouldn’t it be just wonderful if
the program could automatically update itself,
replacing just the pieces that changed?
596
 chapter 17
package, jars and deployment
File Edit View
 File Edit View
File Edit View
 HTTP
 HTTP
 File Edit View
 RMI
 HTTP
Executable
Jar
 Web
 Web Start
 Start
 RMI app
 Servlets
100% Local
 Combination
 100% Remote
Java Web Start
With Java Web Start (JWS), your application is launched for the
first time from a Web browser (get it? Web Start?) but it runs as a
stand-alone application (well, almost), without the constraints of the
browser. And once it’s downloaded to the end-user’s machine (which
happens the first time the user accesses the browser link that starts
the download), it stays there.
Java Web Start is, among other things, a small Java program that lives
on the client machine and works much like a browser plug-in (the
way, say, Adobe Acrobat Reader opens when your browser gets a .pdf
file). This Java program is called the Java Web Start ‘helper app’,
and its key purpose is to manage the downloading, updating, and
launching (executing) of your JWS apps.
When JWS downloads your application (an executable JAR), it
invokes the main() method for your app. After that, the end-user can
launch your application directory from the JWS helper app without
having to go back through the Web page link.
But that’s not the best part. The amazing thing about JWS is its
ability to detect when even a small part of application (say, a single
class file) has changed on the server, and—without any end-user
intervention—download and integrate the updated code.
There’s still an issue, of course, like how does the end-user get Java
and Java Web Start? They need both—Java to run the app, and Java
Web Start (a small Java application itself) to handle retrieving and
launching the app. But even that has been solved. You can set things
up so that if your end-users don’t have JWS, they can download
it from Sun. And if they do have JWS, but their version of Java is
out-of-date (because you’ve specified in your JWS app that you
need a specific version of Java), the Java 2 Standard Edition can be
downloaded to the end-user machine.
Best of all, it’s simple to use. You can serve up a JWS app much like
any other type of Web resource such as a plain old HTML page or a
JPEG image. You set up a Web (HTML) page with a link to your JWS
application, and you’re in business.
In the end, your JWS application isn’t much more than an
executable JAR that end-users can download from the Web.
End-users launch a Java
Web Start app by clicking
on a link in a Web
page. But once the app
downloads, it runs outside
the browser, just like any
other stand-alone Java
application. In fact, a
Java Web Start app is just
an executable JAR that’s
distributed over the Web.
you are here�
 597
Java Web Start
How Java Web Start works
1
 The client clicks on a Web page link
to your JWS application (a .jnlp file).
The Web page link
<a href=”MyApp.jnlp”>Click</a>
Web browser
click
“give me MyApp.jnlp ”
Web Server
Lorper
iure
eugue
tat vero
conse
euguero-
MyApp.jnlp MyApp.jar
2
The Web server (HTTP) gets the
request and sends back a .jnlp file
(this is NOT the JAR).
The .jnlp file is an XML document that
states the name of the application’s
executable JAR file.
click
Lorper
iure
eugue
tat vero
conse
euguero-
MyApp.jnlp
Web Server
Lorper
iure
eugue
tat vero
conse
euguero-
MyApp.jnlp MyApp.jar
3
Java Web Start
Java Web Start (a small ‘helper app’
on the client) is started up by the
 “give me MyApp.jar”
browser. The JWS helper app reads
 JWS
the .jnlp file, and asks the server for
the MyApp.jar file.
Web Server
Lorper
iure
eugue
tat vero
conse
euguero-
MyApp.jnlp MyApp.jar
4
The Web server ‘serves’ up the
requested .jar file.
JWS
MyApp.jar
Web Server
Lorper
iure
eugue
tat vero
conse
euguero-
MyApp.jnlp MyApp.jar
5
HelloWebStart (the app in the JA
R)
Java Web Start gets the JAR and
starts the application by calling the
specified main( ) method (just like an
 Hello
executable JAR).
Next time the user wants to run this app, he can
open the Java Web Start application and from
there launch your app, without even being online.
598
 chapter 17
Web Server
Lorper
iure
eugue
tat vero
conse
euguero-
MyApp.jnlp MyApp.jar
package, jars and deployment
The .jnlp file
To make a Java Web Start app, you need to create a .jnlp (Java
Network Launch Protocol) file that describes your application.
This is the file the JWS app reads and uses to find your JAR and
launch the app (by calling the JAR’s main() method). A .jnlp
you file <?xml <jnlp is can a simple put version=”1.0” spec=”0.2 in, XML but as document a 1.0”
 minimum, encoding=”utf-8”?>
 that it has should several look different like this:
 things
 The of We’re the start say, wh ‘co lo e d testing “
 re cal http://www.w
 ebas
 apps your loopback e
’
 on t
 this web a
 our g
 is
 o st address in n where art
 ternet our s
 localhos t
 y u “127 o ff web u
 s
 is p
 .0 e
 t server, on c
 .com”
 , .0.1”. if
 so y
 the w
 the e
 For this ’r se e rv
 ‘ro using
 e w
 o w r.
 o t’
 e uld
 b
ickedlysmart
codebase=”http://127.0.0.1/~kathy”
 href=”MyApp.jnlp”>
 This codebase. available is the in This location the examp root of directory le th sho e ws
 .jnlp th of at file the MyApp.jnlp relative web server, to is
 the
 not
nested in some other
 directory.
<information>
<homepage <vendor>Wickedly <title>kathy href=”index.html”/>
 App</title>
 Smart</vendor>
 Be the not wants sure work JWS to to helper relaunch correctly! include app, all a Th
 pre mo of e vio
 stly th
 ‘information’ usly-
 ese for tags, down
 displayin
 loa
 or tags de
 your g
 d
 wh
 app
 are en
 app lica
 use
 the mig
 tio
 d
 ht
 use
r
 by
 n.
<description>Head First WebStart demo</description>
<icon href=”kathys.gif”/>
</information>
 <offline-allowed/>
 This it being means means connected the the automati
 user to th can e c-upd int
 run ern
 ating your et. If
 program feature th
e
 use
 won’t wit r
 is hou off
line,
 wo
 t
 rk.
<resources>
 This says that your app needs
 version 1.3
<j2se version=”1.3+”/>
 of Java, or greater.
</resources>
 <jar href=”MyApp.jar”/>
 even The other sounds name JAR of and file yo
 s ur images as executable well, used that by
 hold JAR! your othe You
 app. r might cl
asses have
 or
<application-desc main-class=”HelloWebStart”/>
</jnlp>
 This is like the mainfest Main-Class entry... it says
which class in the JAR has the main() method.
you are here�
 599
deploying with JWS
Steps for making and deploying
a Java Web Start app
1
Make an executable JAR
for your application.
MyApp.jar
Lorper
2
 Write a .jnlp file.
 iure
 eugue
tat vero
conse
euguero-
MyApp.jnlp
3
Place your JAR and .jnlp
files on your Web server.
Web Server
Lorper
iure
eugue
Lorper tat vero
<iure
 conse
eugue
 euguero-
tat vero
conse
 roo.html
MyApp.jnlp MyApp.jar
4
Add a new mime type to your Web server.
application/x-java-jnlp-file
This causes the server to send the .jnlp file with the
correct header, so that when the browser receives
the .jnlp file it knows what it is and knows to start
the JWS helper app.
Web Server
configure
mime type
600
 chapter 17
5
Lorper
Create a Web page with a link
 iure
 eugue
to your .jnlp file
 tat conse
 euguero-
 vero
<HTML>
 MyJWSApp.html
<BODY>
<a href=”MyApp2.jnlp”>Launch My Application</a>
</BODY>
</HTML>
Exercise
What’s
First?
Look at the sequence of events below,
and place them in the order in which they
occur in a JWS application.
1.
2.
3.
package, jars and deployment
the Web server send
s a JAR
the Web browse
r starts up
 file to the JWS helpe
r app
the JWS helper
 app
 the JWS helper
 app requests
the JAR file
the Web server sends
 a .jnlp
 the JWS helper app invokes
file to the browser
 the JAR’s main() method
user clicks a Web page link
 browser requests a .jnlp file
from the Web server
4.
5.
6.
7.
Dumb there are Questions
 no
Q:
 How is Java Web Start different from an applet?
A:
 Applets can’t live outside of a Web browser. An applet is
downloaded from the Web as part of a Web page rather than
simply from a Web page. In other words, to the browser, the applet
is just like a JPEG or any other resource. The browser uses either a
Java plug-in or the browser’s own built-in Java (far less common
today) to run the applet. Applets don’t have the same level of
functionality for things such as automatic updating, and they must
always be launched from the browser. With JWS applications, once
they’re downloaded from the Web, the user doesn’t even have to
be using a browser to relaunch the application locally. Instead,
the user can start up the JWS helper app, and use it to launch the
already-downloaded application again.
Q:
 What are the security restrictions of JWS?
A:
 JWS apps have several limitations including being
restricted from reading and writing to the user’s hard drive. But...
JWS has its own API with a special open and save dialog box so
that, with the user’s permission, your app can save and read its
own files in a special, restricted area of the user’s drive.
�������BULLET POINTS
Java Web Start technology lets you deploy a
stand-alone client application from the Web.
Java Web Start includes a ‘helper app’ that must
be installed on the client (along with Java).
A Java Web Start (JWS) app has two pieces:
an executable JAR and a .jnlp file.
A .jnlp file is a simple XML document that
describes your JWS application. It includes
tags for specifying the name and location of the
JAR, and the name of the class with the main()
method.
When a browser gets a .jnlp file from the server
(because the user clicked on a link to the .jnlp
file), the browser starts up the JWS helper app.
The JWS helper app reads the .jnlp file and
requests the executable JAR from the Web
server.
When the JWS gets the JAR, it invokes the
main() method (specified in the .jnlp file).
you are here�
 601
exercise: True or False
Exercise
We explored packaging, deployment, and JWS
in this chapter. Your job is to decide whether
each of the following statements is true or false.
CTrue or FalseD
1. The Java compiler has a flag, -d, that lets you decide where your .class files should go.
2. A JAR is a standard directory where your .class files should reside.
3. When creating a Java Archive you must create a file called jar.mf.
4. The supporting file in a Java Archive declares which class has the main() method.
5. JAR files must be unzipped before the JVM can use the classes inside.
6. At the command line, Java Archives are invoked using the -arch flag.
7. Package structures are meaningfully represented using hierarchies.
8. Using your company’s domain name is not recommended when naming packages.
9. Different classes within a source file can belong to different packages.
10. When compiling classes in a package, the -p flag is highly recommended.
11. When compiling classes in a package, the full name must mirror the directory tree.
12. Judicious use of the -d flag can help to assure that there are no typos in your class tree.
13. Extracting a JAR with packages will create a directory called meta-inf.
14. Extracting a JAR with packages will create a file called manifest.mf.
15. The JWS helper app always runs in conjunction with a browser.
16. JWS applications require a .nlp (Network Launch Protocol) file to work properly.
17. A JWS’s main method is specified in its JAR file.
602
 chapter 17
package, jars and deployment
Exercise
Summary-Cross 7.0
1
 2
 3
 4
 5
 6
 7
9
10
 11
8
17
12
13
18
14
19
15
 16
Anything in the book
is fair game for this
one!
20
 21
 22
23
 24
25
 26
 27
28
29
 30
 31
 32
33
 34
 35
 36
40
41
 42
Across
6. Won’t travel
 26. Mine is unique
9. Don’t split me
 27. GUI’s target
10. Release-able
 29. Java team
11. Got the key
 30. Factory
12. I/O gang
 32. For a while
15. Flatten
 33. Atomic * 8
17. Encapsulated returner 35. Good as new
18. Ship this one
 37. Pairs event
21. Make it so
 41. Where do I start
22. I/O sieve
 42. A little firewall
25. Disk leaf
37
 38
 39
Down
1. Pushy widgets
2. ____ of my desire
3. ‘Abandoned’ moniker
4. A chunk
5. Math not trig
6. Be brave
7. Arrange well
8. Swing slang
11. I/O canals
13. Organized release
14. Not for an instance
16. Who’s allowed
19. Efficiency expert
20. Early exit
21. Common wrapper
23. Yes or no
24. Java jackets
26. Not behavior
28. Socket’s suite
30. I/O cleanup
31. Milli-nap
34. Trig method
36. Encaps method
38. JNLP format
39. VB’s final
40. Java branch
you are here�
 603
exercise solutions
Exercise
Solutions
1.
2.
3.
4.
5.
6.
7.
user clicks a Web page link
browser requests a .jnlp file
from the Web server
the Web server sends a .jnlp
file to the browser
the Web browser starts up
the JWS helper app
the JWS helper
 app requests
the JAR file
the Web server send
s a JAR
file to the JWS helpe
r app
the JWS helper app invokes
the JAR’s main() method
True
False
False
True
False
False
True
False
False
False
True
True
True
True
False
False
False
604
 chapter1. The Java compiler has a flag, -d, that lets you decide where your .class files should go.
2. A JAR is a standard directory where your .class files should reside.
3. When creating a Java Archive you must create a file called jar,mf.
4. The supporting file in a Java Archive declares which class has the main() method.
5. JAR files must be unzipped before the JVM can use the classes inside.
6. At the command line, Java Archives are invoked using the -arch flag.
7. Package structures are meaningfully represented using hierarchies.
8. Using your company’s domain name is not recommended when naming packages.
9. Different classes within a source file can belong to different packages.
10. When compiling classes in a package, the -p flag is highly recommended.
11. When compiling classes in a package, the full name must mirror the directory tree.
12. Judicious use of the -d flag can help to assure that there are no typos in your tree.
13. Extracting a JAR with packages will create a directory called meta-inf.
14. Extracting a JAR with packages will create a file called manifest.mf.
15. The JWS helper app always runs in conjunction with a browser.
16. JWS applications require a .nlp (Network Launch Protocol) file to work properly.
17. A JWS’s main method is specified in its JAR file.
17
package, jars and deployment
Summary-Cross 7.0
1
 D
 2
O 3
 G
 4
 T
 5
 M
 6
 T R A N 7
 S I E N T W
 8
I
 B
 9 A T O M I C R
 O
 I
A
 10
 J A R
 K
 N
 11 S Y N C H R O N I Z E D
L
 E
 B
 E
 T
 T
 G
O
 12
 C H A I N E 13
 D
 R
 S
 14
 15
 S 16
 A V E
17
 G E T
 G
 18 E X E C U T A 19
 B L E
 C
 T
S
 E P A
 A U
 C
 S
20
 B
 21
 I M P L E M E N T
 22F I L T E R23
 B
 R
 N
 O
 I
 F
 S
 24
 W
25
 D I R E C T O R Y
 26
 S O C K E T 27
 U S E R
N
 A
 E
 T
 R
 28 P
 A
29
 P A C K A G E
 30
 C L A S 31
 S
 32
 L O O P
 P
R
 E
 L
 T
 L
 R
 P
33
 B Y 34
 T E
 35
 R E 36
 S T O R E
 E
 37
 E 38
 X T R 39
 E M E
A
 40
I
 E
 S
 E
 M
 X
 R
41
 M A N I F E S T
 42
 E N C A P S U L A T E
 S
you are here�
 605
18remote deployment with RMI
Distributed
Computing
 but distance with Everyone RMI, relationships it’s says easy. long-
 are No
 hard,
matter how far apart we really
are, RMI makes it seem like
we’re together.
Being remote doesn’t have to be a bad thing. Sure, things are easier when
all the parts of your application are in one place, in one heap, with one JVM to rule them all. But
that’s not always possible. Or desirable. What if your application handles powerful computations,
but the end-users are on a wimpy little Java-enabled device? What if your app needs data
from a database, but for security reasons, only code on your server can access the database?
Imagine a big e-commerce back-end, that has to run within a transaction-management system?
Sometimes, part of your app must run on a server, while another part (usually a client) must
run on a different machine. In this chapter, we’ll learn to use Java’s amazingly simple Remote
Method Invocation (RMI) technology. We’ll also take a quick peek at Servlets, Enterprise Java
Beans (EJB) , and Jini, and look at the ways in which EJB and Jini depend on RMI. We’ll end the
book by writing one of the coolest things you can make in Java, a universal service browser.
this is a new chapter
 607
how many heaps?
File Edit View
Executable
Jar
FileEdit View
WebHTTP
Start
File Edit File
 View
 Edit View
RMI
 RMI
RMI
 RMI
 app
 app
HTTP
Servlets
100% Local
 Combination
 100% Remote
Method calls are always bet ween
t wo objects on the same heap.
So far in this book, every method we’ve invoked has been on
an object running in the same virtual machine as the caller.
In other words, the calling object and the callee (the object
we’re invoking the method on) live on the same heap.
class Foo {
void go() {
Bar b = new Bar();
b.doStuff();
}
public static void main (String[] args) {
Foo f = new Foo();
f.go();
}
}
In the code above, we know that the Foo instance
referenced by f and the Bar object referenced by b are
both on the same heap, run by the same JVM. Remember,
the JVM is responsible for stuffing bits into the reference
variable that represent how to get to an object on the heap.
The JVM always knows where each object is, and how to
get to it. But the JVM can know about references on only
its own heap! You can’t, for example, have a JVM running
on one machine knowing about the heap space of a JVM
running on a different machine. In fact, a JVM running on
one machine can’t know anything about a different JVM
running on the same machine. It makes no difference if
the JVMs are on the same or different physical machines;
it matters only that the two JVMs are, well, two different
invocations of the JVM.
608
 chapter 18
one hea
p, two o
bjects
Foo
f()
do
 S
tuf
objec
t
Bar
 objec
t
In most applications, when one object
calls a method on another, both objects
are on the same heap. In other words,
both are running within the same JVM.
remote deployment with RMI
What if you want to invoke a method on
an object running on another machine?
We know how to get information from one machine to another—
with Sockets and I/O. We open a Socket connection to another
machine, and get an OutputStream and write some data to it.
But what if we actually want to call a method on something running
in another machine... another JVM? Of course we could always build
our own protocol, and when you send data to a ServerSocket the
server could parse it, figure out what you meant, do the work, and
send back the result on another stream. What a pain, though. Think
how the other much machine, nicer it would and call be a to method.
 just get a reference to the object on
 powerful, to calculat
 crunc io hf ns
 thro
 ast, ugh loves
 big
Imagine two computers...
JVM
slow tiny, at wimpy
 calc ,ul pa at infully
 ions
 JVM
Little
 Big
Big has something Little wants.
Compute power.
Little wants to send some data to Big, so that Big can do the
heavy computing.
Little wants simply to call a method...
double doCalcUsingDatabase(CalcNumbers numbers)
and get back the result.
But how can Little get a reference to an object on Big?
you are here�
 609
two objects, two heaps
Object A, running on Little, wants to call
a method on Object B, running on Big.
The question is, how do we get an object on one machine
(which means a different heap/JVM) to call a method on
another machine?
doCalcUsingDat
abase()
B
A
return value
But you can’t do that.
Well, not directly anyway. You can’t get a reference to
something on another heap. If you say:
Dog d = ???
Whatever d is referencing must be in the same heap space as
the code running the statement.
But imagine you want to design something that will use
Sockets and I/O to communicate your intention (a method
invocation on an object running on another machine), yet
still feel as though you were making a local method call.
In other words, you want to cause a method invocation on a
remote object (i.e., an object in a heap somewhere else), but
with code that lets you pretend that you’re invoking a method
on a local object. The ease of a plain old everyday method
call, but the power of remote method invocation. That’s our
goal.
That’s what RMI (Remote Method Invocation) gives you!
But let’s step back and imagine how you would design RMI if
you were doing it yourself. Understanding what you’d have to
build yourself will help you learn how RMI works.
610
 chapter 18
A design for remote method calls
Create four things: server, client,
server helper, client helper
1
 Create client and server apps. The server app is the
remote service that has an object with the method
that the client wants to invoke.
Server heap
Client heap
remote deployment with RMI
Cli
ent object
Se
rvice objec
 t
2
Create client and server ‘helpers’. They’ll handle all
the low-level networking and I/O details so your client
and service can pretend like they’re in the same heap.
Client heap
Server heap
Cli
ent object
Cli
ent helper
Se
rvice help
 r
 e
 Se
rvice objec
 t
you are here�
 611
client and server helpers
The role of the ‘helpers’
The ‘helpers’ are the objects that actually do the communicating.
They make it possible for the client to act as though it’s calling a
method on a local object. In fact, it is. The client calls a method on
the client helper, as if the client helper were the actual service. The client
helper is a proxy for the Real Thing.
In other words, the client object thinks it’s calling a method on
the remote service, because the client helper is pretending to be
the service object. Pretending to be the thing with the method the client
wants to call!
But the client helper isn’t really the remote service. Although the
client helper acts like it (because it has the same method that the
service is advertising), the client helper doesn’t have any of the
actual method logic the client is expecting. Instead, the client
helper contacts the server, transfers information about the method
call (e.g., name of the method, arguments, etc.), and waits for a
return from the server.
On the server side, the service helper receives the request from
the client helper (through a Socket connection), unpacks the
information about the call, and then invokes the real method on
the real service object. So to the service object, the call is local. It’s
coming from the service helper, not a remote client.
The service helper gets the return value from the service, packs it
up, and ships it back (over a Socket’s output stream) to the client
helper. The client helper unpacks the information and returns the
value to the client object.
Client heap
Client to it’s be just the helper a se pr
 rv oxy pr ic et
 e,for ends
 but
 the
Real Thing.
Your client object gets to
act like it’s making remote
method calls. But what
it’s really doing is calling
methods on a heap-local
‘proxy’ object that handles
all the low-level details of
Sockets and streams.
Server heap
Client it’s talki object ng
 to t t hi he
 nks
Real Service. It
thinks helper the is the client
 thing
that do the can real
 ac tually
 work.
612
 chapter 18
Cli
ent object
Cli
ent helper
Se
rvice help
 r
 e
 Se
rvice objec
 t
Real calls helper, request Service Service.
 the unpacks helper from method the gets it, on client
 and
 the
 the
 T the object h
 that real e
 S
 R e
 eal r
 m
 a v
 ctually with ethod ice Servic
 obje the work.
 does e c . t meth It’s IS
 t
h the
 o e d
How the method call happens
1
 Client object calls doBigThing() on the client helper object
Server heap
Client heap
doBigThing()
remote deployment with RMI
Cli
ent object
Cli
ent helper
Se
rvice help
 r
 e
 Se
rvice objec
 t
2
Client helper packages up information about the call
(arguments, method name, etc.) and ships it over the
network to the service helper.
Server heap
Client heap
 “client wants to call a method”
doBigThing()
Cli
ent object
Cli
ent helper
Se
rvice help
 r
 e
 Se
rvice objec
 t
3
Service helper unpacks the information from the client helper,
finds out which method to call (and on which object) and
invokes the real method on the real service object.
Server heap
Client heap
“client wants to call a method”
doBigThing()
doBigThing()
Cli
ent object
Cli
ent helper
Se
rvice help
e
 r
 Se
rvice objec
 t
Remember, object method that does with logi thi t
 c. the
 he s The real is REAL
 the
 guy
 work!
you are here�
 613
RMI helper objects
Java RMI gives you the client and
service helper objects!
In Java, RMI builds the client and service helper
objects for you, and it even knows how to make the
client helper look like the Real Service. In other
words, RMI knows how to give the client helper
object the same methods you want to call on the
remote service.
Plus, RMI provides all the runtime infrastructure to
make it work, including a lookup service so that the
client can find and get the client helper (the proxy
for the Real Service).
With RMI, you don’t write any of the networking
or I/O code yourself. The client gets to call remote
methods (i.e. the ones the Real Service has) just
like normal method calls on objects running in the
client’s own local JVM.
Almost.
There is one difference between RMI calls and local
(normal) method calls. Remember that even though
to the client it looks like the method call is local,
the client helper sends the method call across the
network. So there is networking and I/O. And what
do we know about networking and I/O methods?
They’re risky!
They throw exceptions all over the place.
So, the client does have to acknowledge the risk. The
client has to acknowledge that when it calls a remote
method, even though to the client it’s just a local call
to the proxy/helper object, the call ultimately involves
Sockets and streams. The client’s original call is local,
but the proxy turns it into a remote call. A remote call
just means a method that’s invoked on an object on
another JVM. How the information about that call
gets transferred from one JVM to another depends
on the protocol used by the helper objects.
With RMI, you have a choice of protocols: JRMP or
IIOP. JRMP is RMI’s ‘native’ protocol, the one made
just for Java-to-Java remote calls. IIOP, on the other
hand, is the protocol for CORBA (Common Object
Request Broker Architecture), and lets you make
remote calls on things which aren’t necessarily Java
objects. CORBA is usually much more painful than
RMI, because if you don’t have Java on both ends,
there’s an awful lot of translation and conversion that
has to happen.
But thankfully, all we care about is Java-to-Java, so
we’re sticking with plain old, remarkably easy RMI.
In RMI, the client helper is a ‘stub’
and the server helper is a ‘skeleton’.
614
 chapter 18
Client heap
RMICli
entCli
ent object
STUB
helper
Server heap
RMI SKELETON
Se
rvice help
 r
 e
 Se
rvice objec
 t
remote deployment with RMI
Making the Remote Service
This is an overview of the five steps for making the remote
service (that runs on the server). Don’t worry, each step is
explained in detail over the next few pages.
Step one:
Make a Remote Interface
The remote interface defines the methods
that a client can call remotely. It’s what
the client will use as the polymorphic class
type for your service. Both the Stub and
actual service will implement this!
public interface
MyRemote extends
Remote { }
MyRemote.java
Server
This remot want int
 e c
e
 lients methods r
 f
 a
c
 e
to defines call
 that
the
you
Step Make This It two:
 has is the a the Remote real class implementation that Implementation
 does the Real of the
 Work.
 MyRemoteImpl.java
 public MyRemote Remote interface
 { }
 extends
 The with the the Real re
 remote the al
 wo
 Service. me
 rk
 th
 inter
 .
 od
 It
 s fa
 Th
 im
 th
 ce
 ple
 e at .
 class
 ments
 do
remote methods defined in the remote
interface. It’s the object that the client
Step wants three:
 to call methods on.
 Running service implement
 rmic again at st ion th class...
 e actual
 helper classes spits out objects
 for tw
 t o he new
Generate the stubs and skeletons using rmic
101101
These are the client and server ‘helpers’.
 File Edit Window Help Eat
 10 0 11 110 0
 1
You don’t have to create these classes or ever
 %rmic MyRemoteImpl
 001 001 10
 01
look at the source code that generates them.
 MyRemoteImpl_Stub.class
It’s all handled automatically when you
run the rmic tool that ships with your Java
 101101
 10 110 1
0 11 0
development kit.
 001 10
001 01
MyRemoteImpl_Skel.class
Step four:
File Edit Window Help Drink
The Start phone rmiregistry the book. RMI It’s is registry like where the the white (rmiregistry)
 user pages goes of to a
 get
 %rmiregistry
 run separa
 this t
e in terminal
 a
the proxy (the client stub/helper object).
Step five:
Start the remote service
You have to get the service object up and running.
Your service implementation class instantiates an
instance of the service and registers it with the RMI
registry. Registering it makes the service available for
clients.
File Edit Window Help BeMerry
%java MyRemoteImpl
you are here�
615
a remote interface
Step one: Make a Remote Interface
public interface
MyRemote extends
Remote { }
1
2
3
616
Extend java.rmi.Remote
 MyRemote.java
Remote is a ‘marker’ interface, which means it has no methods. It has
special meaning for RMI, though, so you must follow this rule. Notice
interface.
 that we public say ‘extends’ interface here. One MyRemote interface is allowed extends to extend Remote another
 {
 Your announce remote interface anything, interface met tha ca bu hod t t n’t it
’s has it calls. implement
 can to
 for
 extend
 An
Declare that all methods throw a RemoteException
 other inter
 faces.
The remote interface is the one the client uses as the polymorphic type
for the service. In other words, the client invokes methods on something
that implements the remote interface. That something is the stub, of
course, and since the stub is doing networking and I/O, all kinds of Bad
Things can happen. The client has to acknowledge the risks by handling
or declaring the remote exceptions. If the methods in an interface
declare exceptions, any code calling methods on a reference of that type
(the interface type) must handle or declare the exceptions.
import java.rmi.*;
 the Remote interface is
 in java.rmi
public public interface String MyRemote sayHello() extends throws Remote RemoteException;
 {
 RemoteException Every considered remote ‘risky’. method on Declarin eve
 cal
 ry
 l g is
}
 method forces the clientto pay attention and
acknowledge that things
might not work.
Be sure arguments and return values are primitives or Serializable
Arguments and return values of a remote method must be either primitive
or Serializable. Think about it. Any argument to a remote method has to
be packaged up and shipped across the network, and that’s done through
Serialization. Same thing with return values. If you use primitives, Strings,
and the majority of types in the API (including arrays and collections),
you’ll be fine. If you are passing around your own types, just be sure that
you make your classes implement Serializable.
public String sayHello() throws RemoteException;
That’s packaged This the overclient, return the how wire up so args value and it from and must sent.
 is the gonna return be ser
 Ser
iali
zab
 be ver values ship
 bac ped
 get
 k le.
 to
chapter 18
remote deployment with RMI
Step two: Make a Remote Implementation
 public MyRemote interface
 extends
Remote { }
1
2
Implement the Remote interface
 MyRemoteImpl.java
Your service has to implement the remote interface—the one
with the methods your client is going to call.
public class MyRemoteImpl extends UnicastRemoteObject implements MyRemote {
public String sayHello() {
return “Server says, ‘Hey’”; The compiler will make sure that
}
 // }
 more code in class
 thi
s
 you’ve fro
m case, the implemented there’s interface only all you one.
 the implement. methods
 In
Extend UnicastRemoteObject
In order to work as a remote service object, your object needs some
functionality related to ‘being remote’. The simplest way is to extend
UnicastRemoteObject (from the java.rmi.server package) and let that
class (your superclass) do the work for you.
public class MyRemoteImpl extends UnicastRemoteObject implements MyRemote {
3
4
Write a no-arg constructor that declares a RemoteException
Your new superclass, UnicastRemoteObject, has one little problem—its
constructor throws a RemoteException. The only way to deal with this is
to declare a constructor for your remote implementation, just so that you
have a place to declare the RemoteException. Remember, when a class is
instantiated, its superclass constructor is always called. If your superclass
your constructor public constructor MyRemoteImpl() throws also an throws exception, an throws exception.
 you have RemoteException no choice but to declare { }
 that
 You the way constructor don’t constructor. to declare have th
 to rows th Yo at pu
 u t an your just anything exception.
 ne
 superclass
 ed a
 in
Register the service with the RMI registry
Now that you’ve got a remote service, you have to make it available to
remote clients. You do this by instantiating it and putting it into the RMI
registry (which must be running or this line of code fails). When you
register the implementation object, the RMI system actually puts the stub in
the registry, since that’s what the client really needs. Register your service
using } try catch(Exception {
 MyRemote the Naming.rebind(“Remote static rebind() service ex) method = {...}
 new of Hello”, the MyRemoteImpl();
 java.rmi.Naming service);
 class.
 Give to with service stub look your the and
 it object, RMI service puts
 up in registry. the RMI th ae na stub registry) swaps me When in (that the the and you service registry.
 clients bind register for can
 the
 the
 use
 it
you are here�
 617
stubs and skeletons
Step three: generate stubs and skeletons
1
Run rmic on the remote implementation class
(not the remote interface)
The rmic tool, that comes with the Java software
development kit, takes a service implementation and
creates two new classes, the stub and the skeleton. It uses
a naming convention that is the name of your remote
implementation, with either _Stub or _Skel added to
the end. There are other options with rmic, including
not generating skeletons, seeing what the source code
for these classes looked like, and even using IIOP as
the protocol. The way we’re doing it here is the way
you’ll usually do it. The classes will land in the current
directory (i.e. whatever you did a cd to). Remember, rmic
must be able to see your implementation class, so you’ll
probably run rmic from the directory where your remote
implementation is. (We’re deliberately not using packages
here, to make it simpler. In the Real World, you’ll need
to account for package directory structures and fully-
qualified names).
spits out tw
o new
Notice on the that end. Ju
 you st
 do
 th
 n’t e class say
 “.class”
 name.
 helper classes objects
 for the101101
10 110 1
File Edit Window Help Whuffie
 0 11 0
001 10
%rmic MyRemoteImpl
 001 01
MyRemoteImpl_Stub.class
101101
10 110 1
0 11 0
001 10
001 01
MyRemoteImpl_Skel.class
Step four: run rmiregistry
1
Bring up a terminal and start the rmiregistry.
Be sure you start it from a directory that has access to
your classes. The simplest way is to start it from your
‘classes’ directory.
File Edit Window Help Huh?
%rmiregistry
Step five: start the service
1
Bring up another terminal and start your service
This might be from a main() method in your remote
implementation class, or from a separate launcher class.
In this simple example, we put the starter code in the
implementation class, in a main method that instantiates the
object and registers it with RMI registry.
618
 chapter 18
File Edit Window Help Huh?
%java MyRemoteImpl
Complete code for the server side
remote deployment with RMI
Server
The Remote interface:
import java.rmi.*;
 RemoteException interface ar
 e
 in
 ja an
 va
 d .rmi Remote
 package
Your interface MUST extend
public interface MyRemote extends Remote {
 java.rmi.Remote
All of your remote methods must
public String sayHello() throws RemoteException;
 declare a RemoteException
}
The Remote service (the implementation):
import import java.rmi.*;
 java.rmi.server.*;
 Unicasti. j
a
v
 a
.r
m
R server e
m
o
t
e
O
 package
 b
ject is exte
 easiest in
 the
 nd
in
 way g
 U
ni
 to ca
 mak
 st
R
 e emoteObject a remote ob
 is ject
 the
public class MyRemoteImpl extends UnicastRemoteObject implements MyRemote {
public }
 return String “Server sayHello() says, {
 ‘Hey’”;
 You declare notice interface have that the to methods, RemoteException.
 implement you do of NOT course. all have the
 But
 to
 you remote MUST interf im
pl ac em
 e!!
 ent your
public public MyRemoteImpl() static void main throws (String[] RemoteException args) {
 { }
 your YOU UnicastRemoteObject) that superclass must your constructor write constructor a constructor is declare calling
 (for s , because an risky exception, code it means
 (its
 so
super constructor)
try {
MyRemote service = new MyRemoteImpl();
Naming.rebind(“Remote Hello”, service);
}
 }
 } }
 catch(Exception ex.printStackTrace();
 ex) {
 need name Make rmiregistry to you the look register remote
 using it up th it ob in eje unde th st ct, e atic r rmi then is Naming,rebind(). registry.
 the ‘bind’ name it clients to the
 The
 will
you are here�
 619
getting the stub
How does the client get the stub object?
The client has to get the stub object, since that’s the thing the
client will call methods on. And that’s where the RMI registry
comes in. The client does a ‘lookup’, like going to the white pages
the of a stub phone that book, goesand with essentially that name.”
 says, “Here’s lookup() a name, isand a static I’d like
 method of
 that This must the serv be
 under ic the
 e
 w
 name
 as
registeredthe Naming class
MyRemote service = (MyRemote)
 Naming.lookup(“rmi://127.0.0.1/Remote Hello”);
type The remote client of implementation the always service. uses In the
 fac
 as the
 t,
 You interface, method havereturns to since cast the type it to lookup
 Object.
 the
 your address host goes name here
 or IP
the the actual client never class name needs of toyour kno
w
remote service.
Server
Client
ello(
 )
sayH
3
Stub
 stu
b Skel
eton
 Se
rvice objec
 t
Cli
ent object
 r
et
ur
ne
d
2
1 RMI registry (on server)
lo
oku
p(
 )
 Remote
Hello
1 Client does a lookup on the RMI registry
Naming.lookup(“rmi://127.0.0.1/Remote Hello”);
Stub
2 RMI registry returns the stub object
(as the return value of the lookup method) and RMI
deserializes the stub automatically. You MUST have
the stub class (that rmic generated for you) on the
client or the stub won’t be deserialized.
3
 Client invokes a method on the stub, as
though the stub IS the real service
620
 chapter 18
remote deployment with RMI
How does the client get the stub class?
Now we get to the interesting question. Somehow, someway, the
client must have the stub class (that you generated earlier using
rmic) at the time the client does the lookup, or else the stub won’t
be deserialized on the client and the whole thing blows up. In a
simple system, you can simply hand-deliver the stub class to the
client.
There’s a much cooler way, though, although it’s beyond the
scope of this book. But just in case you’re interested, the cooler
way is called “dynamic class downloading”. With dynamic class
downloading, a stub object (or really any Serialized object) is
‘stamped’ with a URL that tells the RMI system on the client
where to find the class file for that object. Then, in the process of
deserializing an object, if RMI can’t find the class locally, it uses
that URL to do an HTTP Get to retrieve the class file. So you’d
need a simple Web server to serve up class files, and you’d also
need to change some security parameters on the client. There are
a few other tricky issues with dynamic class downloading, but that’s
the overview.
Complete client code
import java.rmi.*;
 java.rmi The remiregistry Naming package clas
 look s up) (for is in doing the
the
public class MyRemoteClient {
public static void main (String[] args) {
new MyRemoteClient().go();
public }
 void go() {
 It Obje
 comes c
 t
,
 so out
 don’t o
f
 t
 forget h
e
 r
egistry the
 c
 a a s st t
ype
try {
MyRemote service = (MyRemote) Naming.lookup(“rmi://127.0.0.1/RemoteString s = service.sayHello();
 or you hostname need the IP
 address
System.out.println(s);
}
 } catch(Exception ex.printStackTrace();
 ex) {
 RemoteException)
 call! It looks (Except just lik
 itemu a st regu ac lar knowledge old method
 the
}
}
Hello”);
and bind/rebind the name th
 us eed serv to ice
you are here�
 621
RMI class files
Be sure each machine has the class
files it needs.
The top three things programmers do wrong with RMI are:
1) Forget to start rmiregistry before starting remote service
(when you register the service using Naming.rebind(), the
rmiregistry must be running!)
2) Forget to make arguments and return types serializable
(you won’t know until runtime; this is not something the
compiler will detect.)
3) Forget to give the stub class to the client.
Client
Server
Skel
eton
 t
Stub
 Se
rvice objec
Cli
ent object
Stub
Don’t forget, the client
 101101
 101101
uses the interface to call
 0 10 101101
 11 110 0
 1
 101101
 10 110 1
 10 001 0 11 110 10
 0
 1
 0 001 1011 110 10
 0
 1
methods on the stub. The
 001 001 10
 01
 001 0 001 1110
 01
 0
 001 01
 001 01
client JVM needs the stub
 MyRemoteImpl.class
 MyRemoteImpl_Stub.class
class, but the client never
 Client.class
 MyRemoteImpl_Stub.class
refers to the stub class
 101101
 10 110 1
 101101
in code. The client always
 10 101101
 110 1
 0 001 1110
 0
 10 0 11 110 0
 1
uses the remote interface,
 0 001 1110
 0
 001 01
 001 001 10
 01
as though the remote
 001 01
 MyRemoteImpl_Skel.class
interface WERE the
 MyRemote.class
 MyRemote.class
actual remote object.
Server needs both the Stub and Skeleton
The re
mote int
erface.
 remote classes, as interface. well as the It needs service the and stub the
 class
because remember, the stub is substituted
for the real service, when the real service
is bound to the RMI registry.
622
 chapter 18
Sharpen your pencil
What’s
First?
Look at the sequence of events below, and
place them in the order in which they
occur in a Java RMI application.
The client gets the stu
 b from
The stub
 se
nd
 s
 th
e
 m
et
ho
d
 the RMI registry
call to the server
 The client invo
kes a method
on the stub
The client does a lookup
 on
 The remote service is regis-
the RMI Registry
 tered with the RMI registry
The RMI registry is started
 The remote service (remote
implementation) is instantiated
remote deployment with RMI
1.
2.
3.
4.
5.
6.
7.
�������BULLET POINTS
An object on one heap cannot get a normal Java
reference to an object on a different heap (which means
running on a different JVM)
Java Remote Method Invocation (RMI) makes it seem like
you’re calling a method on a remote object (i.e. an object
in a different JVM), but you aren’t.
When a client calls a method on a remote object, the
client is really calling a method on a proxy of the remote
object. The proxy is called a ‘stub’.
A stub is a client helper object that takes care of the low-
level networking details (sockets, streams, serialization,
etc.) by packaging and sending method calls to the
server.
To build a remote service (in other words, an object that
a remote client can ultimately call methods on), you must
start with a remote interface.
A remote interface must extend the java.rmi.Remote
interface, and all methods must declare
RemoteException.
Your remote service implements your remote interface.
�������Your remote service should extend UnicastRemoteObject.
(Technically there are other ways to create a remote ob-
ject, but extending UnicastRemoteObject is the simplest).
Your remote service class must have a constructor,
and the constructor must declare a RemoteException
(because the superclass constructor declares one).
Your remote service must be instantiated, and the object
registered with the RMI registry.
To register a remote service, use the static
Naming.rebind(“Service Name”, serviceInstance);
The RMI registry must be running on the same machine
as the remote service, before you try to register a remote
object with the RMI registry.
The client looks up your remote service using the static
Naming.lookup(“rmi://MyHostName/ServiceName”);
Almost everything related to RMI can throw a
RemoteException (checked by the compiler). This
includes registering or looking up a service in the registry,
and all remote method calls from the client to the stub.
you are here�
 623
uses for RMI
Yeah, but who really uses RMI?
We use it
for our cool
new decision-support
system.
I heard your ex-
wife still uses
plain sockets.
I use it
for serious B-to-B,
e-commerce back-
ends, running on J2EE
technology.
We’ve got an
EJB-based hotel
reservation system.
And EJB uses RMI!
I just can’t imagine
life without our Jini-
enabled home network
and applicances.
Me too! How
did anyone get
by? I just love RMI
for giving us Jini
technology.
624
 chapter 18
File Edit View
 FileEdit View
 HTTP
 File Edit View
 RMI
 HTTP
 HTTP
Executable
 Servlets
 Servlets
Jar
 Web Start
 RMI app
 Servlets
100% Local
 Combination
 100% Remote
remote deployment with RMI
What about Servlets?
Servlets are Java programs that run on (and with) an HTTP web server. When a client uses a
web browser to interact with a web page, a request is sent back to the web server. If the request
needs the help of a Java servlet, the web server runs (or calls, if the servlet is already running)
the servlet code. Servlet code is simply code that runs on the server, to do work as a result of
whatever the client requests (for example, save information to a text file or database on the
server). If you’re familiar with CGI scripts written in Perl, you know exactly what we’re talking
about. Web developers use CGI scripts or servlets to do everything from sending user-submitted
info to a database, to running a web-site’s discussion board.
And even servlets can use RMI!
By far, the most common use of J2EE technology is to mix servlets and EJBs together, where
servlets are the client of the EJB. And in that case, the servlet is using RMI to talk to the EJBs.
(Although the way you use RMI with EJB is a little different from the process we just looked at.)
1
Client fills out a registration form and clicks ‘submit’.
The HTTP server (i.e. web server) gets the request, sees that
it’s for a servlet, and sends the request to the servlet.
Web Server
Web Browser
(client)
 “client requests RegisterServlet”
101101
10 110 1
0 11 0
Bob’s
 001 10
Pets
 001 01
Kathy
Sierra
MyServlet.class
2
Servlet (Java code) runs, adds data to the database,
composes a web page (with custom info) and sends it back to
the client where it displays in the browser.
Web Server
Web Browser
(client)
 “client requests RegisterServlet”
101101
10 110 1
0 11 0
001 10
Thanks
 Kathy
 001 01
Sierra
MyServlet.class
“here’s a confirmation page”
 <HTML>
 <BODY>
Java
rules!
<BODY>
<HTML>
confirm.html
you are here�
 625
very simple servlet
Steps for making and running a servlet
1
Find out where your servlets need to be placed.
For these examples, we’ll assume that you already have a web server up
and running, and that it’s already configured to support servlets. The
most important thing is to find out exactly where your servlet class files
have to be placed in order for your server to ‘see’ them. If you have a web
site hosted by an ISP, the hosting service can tell you where to put your
servlets, just as they’ll tell you where to place your CGI scripts.
Web Server
Servlets
101101
10 110 1
0 11 0
001 10
001 01
MyServletA.class
2
Get the servlets.jar and add it to your classpath
Servlets aren’t part of the standard Java libraries; you need the servlets
classes packaged into the servlets.jar file. You can download the servlets
classes from java.sun.com, or you can get them from your Java-enabled
web server (like Apache Tomcat, at the apache.org site). Without these
classes, you won’t be able to compile your servlets.
3
Write a servlet class by extending HttpServlet
A servlet is just a Java class that extends HttpServlet (from the javax.
servlet.http package). There are other types of servlets you can make, but
most of the time we care only about HttpServlet.
public class MyServletA extends HttpServlet { ... }
4
Write an HTML page that invokes your servlet
When the user clicks a link that references your servlet, the web server
will find the servlet and invoke the appropriate method depending on the
HTTP command (GET, POST, etc.)
<a href=”servlets/MyServletA”>This is the most amazing5
Make your servlet and HTML page available to your server
This is completely dependent on your web server (and more specifically,
on which version of Java Servlets that you’re using). Your ISP may simply
tell you to drop it into a “Servlets” directory on your web site. But if
you’re using, say, the latest version of Tomcat, you’ll have a lot more work
to do to get the servlet (and web page) into the right location. (We just
happen to have a book on this too .)
626
 chapter 18
servlets.jar
101101
10 110 1
0 11 0
001 10
001 01
MyServletA.class
<HTML>
<BODY>
Java
rules!
<BODY>
<HTML>
MyPage.html
servlet.</a>
Web Server
Servlets
101101
10 110 1
<HTML>
 0 11 0
<BODY>
 001 10
Java
 001 01
rules!
<HTML>
 <BODY>
 MyServletA.class
MyPage.html
remote deployment with RMI
A very simple Servlet
import import import javax.servlet.http.*;
 javax.servlet.*;
 java.io.*;
 Besides Remem
 standard be
 io, r,
 librari
 w t e he ne se ed es two -- to packages impo
 you have
 rt
 tw
 are to
 o
 of
 d
 N ow
 OT t
 nl
 he
 oa
 part servlet d
 t
he
 of m t pack se
parately
 he
 J ag av es a
 .
Most ‘normal’ servlets will extend
public class MyServletA extends HttpServlet {
 HttpServlet, then override one or
more methods.
Override HTTP GET the me
 do ss Ge ag t es fo .
 r simple
 The request object web that (you serv er
 you’l
 can ca
 l get lls
 use th
 data to is
 me
 send out th
od
 back of , handing it) a response and you a ‘resp the (a on pa
 cli
 se
 ge ’ ent’s
 ).
public void doGet (HttpServletRequest request, HttpServletResponse response)
throws ServletException, IOException {
response.setContentType(“text/html”);
 ‘thing’ This this servlet tells is coming the running.
 server back (and frombrowser)
 the server what
 as
 a
 kind result of
 of
PrintWriter out = response.getWriter();
 T
 ‘write’ he
 re
sponse information object ba
 giv ck es ou us t an to output the server.
 stream to
String message = “If you’re reading this, it worked!”;
}
 }
 out.println(“</BODY></HTML>”);
 out.close();
 out.println(“<H1>” out.println(“<HTML><BODY>”);
 + message + “</H1>”);
 What gets th
 even until somewhere e
 br
 delivered though we now. ow
se
 ‘write’ In r,
 with
 this ju
 ot th
 st
 he is this ro is lik
 r an ugh a wo e
 stuff HTML pa any rds, the ge other that there’s server in page! it.
 never
 HTML back no The .html ex to
 page
 pa
 ist
 ge,
 ed
 file
What the web page lo
oks
 like:
HTML page with a link to this servlet
 click the link
 This an amazing servlet.
to trigger the
servlet
<HTML>
<BODY>
<a href=”servlets/MyServletA”>This is an amazing servlet.</a>
</BODY>
</HTML>
you are here�
 627
servlets and JSP
���������BULLET POINTS
Servlets are Java classes that run entirely on
(and/or within) an HTTP (web) server.
Servlets are useful for running code on the
server as a result of client interaction with a
web page. For example, if a client submits
information in a web page form, the servlet can
process the information, add it to a database,
and send back a customized, confirmation
response page.
To compile a servlet, you need the servlet
packages which are in the servlets.jar file. The
servlet classes are not part of the Java standard
libraries, so you need to download the servlets.
jar from java.sun.com or get them from a servlet-
capable web server. (Note: the Servlet library
is included with the Java 2 Enterprise Edition
(J2EE))
To run a servlet, you must have a web server
capable of running servlets, such as the Tomcat
server from apache.org.
Your servlet must be placed in a location that’s
specific to your particular web server, so you’ll
need to find that out before you try to run your
servlets. If you have a web site hosted by an ISP
that supports servlets, the ISP will tell you which
directory to place your servlets in.
A typical servlet extends HttpServlet and
overrides one or more servlet methods, such as
doGet() or doPost().
The web server starts the servlet and calls the
appropriate method (doGet(), etc.) based on the
client’s request.
The servlet can send back a response by getting
a PrintWriter output stream from the response
parameter of the doGet() method.
The servlet ‘writes’ out an HTML page, complete
with tags).
628
 chapter 18
Q:
Dumb there are Questions
 no
What’s a JSP, and how does it relate to servlets?
A:
 JSP stands for Java Server Pages. In the end, the web server
turns a JSP into a servlet, but the difference between a servlet and
a JSP is what YOU (the developer) actually create. With a servlet,
you write a Java class that contains HTML in the output statements
(if you’re sending back an HTML page to the client). But with a
JSP, it’s the opposite—you write an HTML page that contains Java
code!
This gives you the ability to have dynamic web pages where you
write the page as a normal HTML page, except you embed Java
code (and other tags that “trigger” Java code at runtime) that
gets processed at runtime. In other words, part of the page is
customized at runtime when the Java code runs.
The main benefit of JSP over regular servlets is that it’s just a lot
easier to write the HTML part of a servlet as a JSP page than to
write HTML in the torturous print out statements in the servlet’s
response. Imagine a reasonably complex HTML page, and now
imagine formatting it within println statements. Yikes!
But for many applications, it isn’t necessary to use JSPs because
the servlet doesn’t need to send a dynamic response, or the
HTML is simple enough not to be such a big pain. And, there are
still many web servers out there that support servlets but do not
support JSPs, so you’re stuck.
Another benefit of JSPs is that you can separate the work by
having the Java developers write the servlets and the web page
developers write the JSPs. That’s the promised benefit, anyway.
In reality, there’s still a Java learning curve (and a tag learning
curve) for anyone writing a JSP, so to think that an HTML web page
designer can bang out JSPs is not realistic. Well, not without tools.
But that’s the good news—authoring tools are starting to appear,
that help web page designers create JSPs without writing the
code from scratch.
Q:
 Is this all you’re gonna say about servlets? After such a
huge thing on RMI?
A:
 Yes. RMI is part of the Java language, and all the classes for
RMI are in the standard libraries. Servlets and JSPs are not part of
the Java language; they’re considered standard extensions. You
can run RMI on any modern JVM, but Servlets and JSPs require a
properly configured web server with a servlet “container”. This is
our way of saying, “it’s beyond the scope of this book.” But you can
read much more in the lovely Head First Servlets & JSP.
Just for fun, let’s make the Phrase-O-Matic
work as a servlet
Now that we told you that we won’t
say any more about servlets, we can’t
resist servletizing (yes, we can verbify
it) the Phrase-O-Matic from chapter 1.
A servlet is still just Java. And Java code
can call Java code from other classes.
So a servlet is free to call a method on
the Phrase-O-Matic. All you have to do
is drop the Phrase-O-Matic class into
the same directory as your servlet, and
you’re in business. (The Phrase-O-
Matic code is on the next page).
remote deployment with RMI
Try my
new web-enabled
phrase-o-matic and you’ll
be a slick talker just like
the boss or those guys in
marketing.
import java.io.*;
import javax.servlet.*;
import javax.servlet.http.*;
public class KathyServlet extends HttpServlet {
public void doGet (HttpServletRequest request, HttpServletResponse response)
throws ServletException, IOException {
String title = “PhraseOMatic has generated the following phrase.”;
response.setContentType(“text/html”);
PrintWriter out = response.getWriter();
out.println(“</TITLE></HEAD><BODY>”);
 out.println(“PhraseOmatic”);
 out.println(“<H1>” out.println(“<HTML><HEAD><TITLE>”);
 + title + “</H1>”);
 See? another the PhraseOMatic
 static Your cla servlet make ss
.
 In
 class Phrase() th ca is n (on case, ca
ll the method methods we’re next calling
 of page)
 on
 the
out.println(“<P>” + PhraseOMatic.makePhrase());
out.println(“<P><a href=\”KathyServlet\”>make another phrase</a></p>”);
out.println(“</BODY></HTML>”);
out.close();
}
}
you are here�
 629
Phrase-O-Matic code
Phrase-O-Matic code, servlet-friendly
This is a slightly different version from the code in chapter one. In the
original, we ran the entire thing in a main() method, and we had to rerun
the program each time to generate a new phrase at the command-line. In this
version, the code simply returns a String (with the phrase) when you invoke
the static makePhrase() method. That way, you can call the method from any
other code and get back a String with the randomly-composed phrase.
Please note that these long String[] array assignments are a victim of word-
processing here—don’t type in the hyphens! Just keep on typing and let your
code editor do the wrapping. And whatever you do, don’t hit the return key in
the middle of a String (i.e. something between double quotes).
public class PhraseOMatic {
public static String makePhrase() {
// make three sets of words to choose from
String[] wordListOne = {“24/7”,”multi-Tier”,”30,000 foot”,”B-to-B”,”win-win”,”front-
end”, “web-based”,”pervasive”, “smart”, “six-sigma”,”critical-path”, “dynamic”};
String[] wordListTwo = {“empowered”, “sticky”, “valued-added”, “oriented”, “centric”,
“distributed”, “clustered”, “branded”,”outside-the-box”, “positioned”, “networked”, “fo-
cused”, “leveraged”, “aligned”, “targeted”, “shared”, “cooperative”, “accelerated”};
String[] wordListThree = {“process”, “tipping point”, “solution”, “architecture”,
“core competency”, “strategy”, “mindshare”, “portal”, “space”, “vision”, “paradigm”, “mis-
sion”};
// find out how many words are in each list
int oneLength = wordListOne.length;
int twoLength = wordListTwo.length;
int threeLength = wordListThree.length;
// generate three random numbers, to pull random words from each list
int rand1 = (int) (Math.random() * oneLength);
int rand2 = (int) (Math.random() * twoLength);
int rand3 = (int) (Math.random() * threeLength);
// now build a phrase
String phrase = wordListOne[rand1] + “ “ + wordListTwo[rand2] + “ “ +
wordListThree[rand3];
// now return it
return (“What we need is a “ + phrase);
}
}
630
 chapter 18
remote deployment with RMI
Enterprise JavaBeans: RMI on steroids
RMI you wouldn’t is great for run writing something and running like an Amazon remote services. or eBay on But
 RMI
 An EJB server adds a bunch
alone. For a large, deadly serious, enterprise application, you
 of services that you don’t get
need something more. You need something that can handle
transactions, heavy concurrency issues (like a gazillion
 with straight RMI. Things
people dog kibbles), are hitting security your (not server just at anyone once to should buy those hit your
 organic
 like transactions, security,
payroll an enterprise database), application and data server.
 management. For that, you need
 concurrency, database
In Java, that means a Java 2 Enterprise Edition (J2EE) server.
 management, and networking.
A J2EE server includes both a web server and an Enterprise
JavaBeans(EJB) server, so that you can deploy an application
 An EJB server steps into the
that includes both servlets and EJBs. Like servlets, EJB is
way beyond the scope of this book, and there’s no way to
 middle of an RMI call and
show a quick “just look a little” at how EJB it works. example (For with a much code,more but we detailed
 will take
 layers in all of the services.
treatment of EJB, we can recommend the lively Head First
EJB certification study guide.)
This typically running client in an the
 could EJ same
 B be
 client ANYTHING, J2EE is aserver.
 servlet
 but
 server all the the involved! Here’s the real calls (security, services where business The to the the EJB provided bean transactio logic) EJ
 ob B jec (the and ser
 t
 by ver intercepts
 ns, bean layers the etc.)
 gets
 EJB
 holds
 in
 The “Whoa! direct can lets the method...” for HERE, bean actually security the inclient This an where server object EJB Almost clien talk acce cle th
 do se arance is t ss! e to rver doesn’t pr things everything server the On
ly otected happens to bean. the like steps call have
 from
 server
 say,
 you This
 this
 right
 in!
 pay
EJB server
Client
RMI SKELETON
RMI STUB
Se
rvice help
 r
 e
 n
Cli
ent helper
 EJB
 objec
t
 en
terpris
e a be
Cli
ent object
DB
This is only a small part of the EJB picture!
you are here�
 631
a little Jini
For our final trick... a little Jini
We love Jini. We think Jini is pretty much the best thing in Java. If EJB is RMI
on steroids (with a bunch of managers), Jini is RMI with wings. Pure Java bliss.
Like the EJB material, we can’t get into any of the Jini details here, but if you
know RMI, you’re three-quarters of the way there. In terms of technology,
anyway. In terms of mindset, it’s time to make a big leap. No, it’s time to fly.
Jini uses RMI (although other protocols can be involved), but gives you a few
key features including:
Adaptive discovery
Self-healing networks
With RMI, remember, the client has to know the
name and location of the remote service. The
client code for the lookup includes the IP address or
hostname of the remote service (because that’s where
the RMI registry is running) and the logical name the
service was registered under.
But with Jini, the client has to know only one thing: the
interface implemented by the service! That’s it.
So how do you find things? The trick revolves around Jini lookup
services. Jini lookup services are far more powerful and flexible than the
RMI registry. For one thing, Jini lookup services announce themselves to the network,
automatically. When a lookup service comes online, it sends a message (using IP
multicast) out to the network saying, “I’m here, if anyone’s interested.”
But that’s not all. Let’s say you (a client) come online after the lookup service has already
announced itself, you can send a message to the entire network saying, “Are there any
lookup services out there?”
Except that you’re not really interested in the lookup service itself—you’re interested in
the services that are registered with the lookup service. Things like RMI remote services,
other serializable Java objects, and even devices such as printers, cameras, and coffee-
makers.
And here’s where it gets even more fun: when a service comes online, it will dynamically
discover (and register itself with) any Jini lookup services on the network. When the
service registers with the lookup service, the service sends a serialized object to be placed
in the lookup service. That serialized object can be a stub to an RMI remote service, a
driver for a networked device, or even the whole service itself that (once you get it from
the lookup service) runs locally on your machine. And instead of registering by name, the
service registers by the interface it implements.
Once you (the client) have a reference to a lookup service, you can say to that lookup
service, “Hey, do you have anything that implements ScientificCalculator?” At that point,
the lookup service will check its list of registered interfaces, and assuming it finds a
match, says back to you, “Yes I do have something that implements that interface. Here’s
the serialized object the ScientificCalculator service registered with me.”
632
 chapter 18
remote deployment with RMI
Adaptive discovery in action
1
 Jini lookup service is launched somewhere on the network, and
announces itself using IP multicast.
Hey everybody,
I’m here!
Jini Lookup Service
another machine on the network
machine on the network
somewhere...
another machine on the network
2
An already-running Jini service on
another machine asks to be registered
with this newly-announced lookup
service. It registers by capability,
rather than by name. In other words,
it registers as the service interface it
implements. It sends a serialized object
to be placed in the lookup service.
Jini Lookup Service
Register
me as something
that implements
ScientificCalculator. Here’s a
serialized object that represents
my service. Send it to
anybody who asks...
Jini Service
another machine on the network
machine on the network
somewhere...
another machine on the network
you are here�
633
adaptive discovery in Jini
Adaptive discovery in action, continued...
3
A client on the network wants
something that implements the
ScientificCalculator interface. It has
no idea where (or if) that thing exists,
so it asks the lookup service.
Jini Service
another machine
on the network
Jini Lookup Service
machine on the network
somewhere...
Java app
Do you
have anything
that implements
ScientificCalculator?
another machine on the network
4The lookup service responds, since it does have something
registered as a ScientificCalculator interface.
Yes, I do
have something.
I’m sending you the
serialized object
now...
Jini Lookup Service
Jini Service
another machine on the network
634
machine on the network
somewhere...
chapter 18
Java app
another machine on the network
remote deployment with RMI
Self-healing net work in action
1
 A Jini Service has asked to register with the lookup service. The lookup
service responds with a “lease”. The newly-registered service must keep
renewing the lease, or the lookup service assumes the service has gone
offline. The lookup service wants always to present an accurate picture
to the rest of the network about which services are available.
I’ll
register you,
and here’s your
 another machine
lease. If you don’t
 Jini Service
on the network
renew it, I’ll drop you.
lease
Jini Lookup Service
machine on the network
somewhere...
another machine on the network
2The service goes offline (somebody shuts it down), so it fails to
renew its lease with the lookup service. The lookup service drops it.
Hmmmm... I
didn’t get a lease
renewal from that one... it
must be down. I’ll drop it. If it
comes back, it will automatically
rediscover me.
Jini Lookup Service
another machine on the network
machine on the network
somewhere...
another machine on the network
you are here�
635
universal service project
Final Project: the Universal Service browser
We’re going to make something that isn’t Jini-enabled, but quite easily could be.
It will give you the flavor and feeling of Jini, but using straight RMI. In fact the
main difference between our application and a Jini application is how the service is
discovered. Instead of the Jini lookup service, which automatically announces itself and
lives anywhere on the network, we’re using the RMI registry which must be on the same
machine as the remote service, and which does not announce itself automatically.
And instead of our service registering itself automatically with the lookup service, we
have to register it in the RMI registry (using Naming.rebind()).
But once the client has found the service in the RMI registry, the rest of the application
is almost identical to the way we’d do it in Jini. (The main thing missing is the lease that
would let us have a self-healing network if any of the services go down.)
The universal service browser is like a specialized web browser, except instead of HTML
pages, the service browser downloads and displays interactive Java GUIs that we’re
calling universal services.
Choose a service from the
list. The RMI remote service
has a getServiceList()
method that sends back this
list of services.
When the user selects one,
the client asks for the
actual service (DiceRolling,
DayOfTheWeek, etc.) to
be sent back from the RMI
remote service.
W
 it h
will e
n
 y
o
 show
 u
 s
e
lect up her
 a e se ! rvic
 e,
636
 chapter 18
How it works:
1
Client starts up and
does a lookup on the
RMI registry for
the service called
“ServiceServer”, and
gets back the stub.
remote deployment with RMI
Service Browser
(client)
“Pl
“S ease
erv giv
e m
eiceSe
rve the
r”
 thi
ng
nam
ed
“O
K,
he
re’
st
he
stu
b”
Server
r
e
vre
Servic
e
 SRMI registry (on server)
Service
Server
Stub
2
Client calls getServiceList() on the stub. The ServiceServer
returns an array of services
Server
Service Browser
(client)
 “getServiceList()”
“OK, here’s an array of services”
r
e
vre
Servic
e
 S3
Client displays the list of services in a GUI
Service Browser
(client)
Server
r
e
vre
Servic
 S e
you are here�
 637
universal service browser
How it works, continued...
4
User selects from the list, so client calls the getService()
method on the remote service. The remote service returns a
serialized object that is an actual service that will run inside
the client browser.
Service Browser
(client)
“getService(selectedSvc)”
“OK, here’s the service”
Server
r
e
vre
Servic
e
 S5
Client calls the getGuiPanel() on the serialized service object it
just got from the remote service. The GUI for that service is
displayed inside the browser, and the user can interact with it
locally. At this point, we don’t need the remote service unless/until
the user decides to select another service.
Service Browser
(client)
638
 chapter 18
The classes and interfaces:
1
interface ServiceServer implements Remote
A regular old RMI remote interface for the remote service (the
remote service has the method for getting the service list and
returning a selected service).
2
class ServiceServerImpl implements ServiceServer
The actual RMI remote service (extends UnicastRemoteObject).
Its job is to instantiate and store all the services (the things
that will be shipped to the client), and register the server itself
(ServiceServerImpl) with the RMI registry.
3
class ServiceBrowser
The client. It builds a very simple GUI, does a lookup in the RMI
registry to get the ServiceServer stub, then calls a remote method on
it to get the list of services to display in the GUI list.
4
 interface Service
This is the key to everything. This very simple interface has just one
method, getGuiPanel(). Every service that gets shipped over to the
client must implement this interface. This is what makes the whole thing
UNIVERSAL! By implementing this interface, a service can come over
even though the client has no idea what the actual class (or classes)
are that make up that service. All the client knows is that whatever
comes over, it implements the Service interface, so it MUST have a
getGuiPanel() method.
The client gets a serialized object as a result of calling
getService(selectedSvc) on the ServiceServer stub, and all the client
says to that object is, “I don’t know who or what you are, but I DO
know that you implement the Service interface, so I know I can call
getGuiPanel() on you. And since getGuiPanel() returns a JPanel, I’ll just
slap it into the browser GUI and start interacting with it!
5
class DiceService implements Service
Got dice? If not, but you need some, use this service to roll anywhere
from 1 to 6 virtual dice for you.
6
7
class MiniMusicService implements Service
Remember that fabulous little ‘music video’ program from the first
GUI Code Kitchen? We’ve turned it into a service, and you can play it
over and over and over until your roommates finally leave.
class DayOfTheWeekService implements Service
Were you born on a Friday? Type in your birthday and find out.
DayOfTheWeekService
getGuiPanel()
remote deployment with RMI
ServiceServer
getServicesList()
getService()
ServiceServerImpl
getServicesList()
getService()
ServiceBrowser
main()
Service
getGuiPanel()
DiceService
getGuiPanel()
MiniMusicService
getGuiPanel()
you are here�
 639
universal service code
interface ServiceServer (the remote interface)
import public Object[] java.rmi.*;
 interface getServiceList() ServiceServer throws extends RemoteException;
 Remote {
 A defines remote
 normal
 the service R
M
 two I
 r
e
 will m
 methods
 ote have.
 intet rf h a e
 ce,
Service getService(Object serviceKey) throws RemoteException;
}
interface Service (what the GUI services implement)
}
 import import public public java.io.*;
 javax.swing.*;
 interface JPanel Service getGuiPanel();
 extends Serializable {
 automatically class interface service defines A plain implemen must old the ex
 te (i
 on have— be ti .e nd . e ng Se no
 s m
et
 the Serializable, n-remote) rializable.
 ge
tGuiPanel(). hod Service that interfac
 interface any so The
 that universal e,any
 will
 th
at
the result That’s shipped remote of a over must, the Se
rviceServer. the client be
 w cause ire calling
 from the getService() services the server, get
 as on
a
640
 chapter 18
remote deployment with RMI
class ServiceServerImpl (the remote implementation)
import java.rmi.*;
import import java.util.*;
 java.rmi.server.*;
 A normal R
MI
 implementat
ion
public class ServiceServerImpl extends UnicastRemoteObject implements ServiceServer
 {
HashMap serviceList;
 value The object services object in the (whatever will collection be st
 ored you , you want). in put a HashMap TWO (see append
 -- collection. a key ix B object for Instea
 more (li d ke
of on a St
 HashMap)
 putting ring) and ONE
 a
public ServiceServerImpl() throws RemoteException {
private }
 setUpServices();
 void setUpServices() {
 When universal these
 cr o vi nstructo
 ces (DiceService, r
 is
 c
a
ll
e
d
,
 MiniM
 in
itialize usicSe
 th r e vi a ce ct , ual
 etc.)
serviceList = new HashMap();
serviceList.put(“Dice Rolling Service”, new DiceService());
serviceList.put(“Day of the Week Service”, new DayOfTheWeekService());
}
 serviceList.put(“Visual Music Service”, new MiniMusicService());
 Make obje
 H
a
sh
 ct
 M
 the s)
 a
p
 a
 ,
 se n w
 d r it
 vi
 put h
 ces a them String (the into actua
 name
 the l service
 (f
 or
the ‘key’).
public }
 System.out.println(“in return Object[] serviceList.keySet().toArray();
 getServiceList() remote”);
 {
 unless in inside) send display Client the an the HashMap. by calls in array client making the this browse of asks We in
 an type or wo ar for r der n’t ray (so Object it to send the of by get just (even calling user an aactual the list can though getSer
vice().
 KEYS of select Service services it that one). has object
 Strings
 to
 are
 We
public Service getService(Object serviceKey) throws RemoteException {
}
 Service return theService;
 theService = (Service) serviceList.get(serviceKey);
 Client from method originally service the calls out
 above). displayed sent this of to the method This the list HashMap.
 code client) of a
f
t
 uses service e
r
 to
 t
 t h
 h get e
 s e (that us
 key e
 the r (the selects corresponding
 it got
 same a f se
 r
 key
 o
 rvice
 m the
public static void main (String[] args) {
try {
Naming.rebind(“ServiceServer”, new ServiceServerImpl());
} catch(Exception ex) {
ex.printStackTrace();
}
System.out.println(“Remote service is running”);
}
}
 you are here�
 641
ServiceBrowser code
class ServiceBrowser (the client)
import java.awt.*;
import javax.swing.*;
import java.rmi.*;
import java.awt.event.*;
public class ServiceBrowser {
JPanel mainPanel;
JComboBox serviceList;
ServiceServer server;
public void buildGUI() {
JFrame frame = new JFrame(“RMI Browser”);
mainPanel = new JPanel();
frame.getContentPane().add(BorderLayout.CENTER, Object[] services = getServicesList();
 this gets (The mainPanel);
 method the actua
 st
 l ub, d
 method oe
 and s
 t
he
 calls is R
 on M
 getServiceL I the registry next
 lo page).
 is ok
 t
()
 up,
 .
serviceList = new JComboBox(services);
 Ad
 JComboBox make d
 th
 disp
 e
 se
 lay
 rv
 ab
 (the ice
 le
 s
 (a
 St
 list). n
 rin
 ar
 gs
 The ray ou
t
 of JCombo of
 Objects) ea
ch Bo th
 x
 to
 ing
 kn
ow
 the
 in
 s the how array.
 to
frame.getContentPane().add(BorderLayout.NORTH, serviceList);
serviceList.addActionListener(new MyListListener());
frame.setSize(500,500);
frame.setVisible(true);
}
void loadService(Object serviceSelection) {
try {
Service svc = server.getService(serviceSelection);
}
 }
 } mainPanel.validate();
 mainPanel.repaint();
 mainPanel.add(svc.getGuiPanel());
 catch(Exception mainPanel.removeAll();
 ex.printStackTrace();
 ex) {
 the and (serialized), getServiceList
()). String String remote listener use
r
 Here’s
 result we ha
s simply we that where server on selected (a originally the which JPanel) was call we (the JComboBo
 displaye
 add the is Th
 one. got stub
 au to e ge the to (This server the tGuiPanel() fr
 d
 matically fo
r
 actual
 om
 x)
.
 in
 browser’s meth th
e
 Se
rv
ice
Server) th
e
 We
 returns service list
 ca
 od deserialized ser
ve
r ll on is (which getService() mainPanel.
 the the called to when actual service the is and by (thanks we the GUI, pass service
 the called
 on and SAME
 after event
 it the
 to add
 the
 RMI)
 the
642
 chapter 18
remote deployment with RMI
Object[] getServicesList() {
Object obj = null;
Object[] services = null;
 Do the RMI look
up
, and get the st
ub
try {
obj = Naming.lookup(“rmi://127.0.0.1/ServiceServer”);
}
catch(Exception ex) {
ex.printStackTrace();
}
server = (ServiceServer) obj;
 so Cast that the we stub
 canto call the getServiceList() remote interface on
 type,
 it
try {
services = server.getServiceList();
} catch(Exception ex) {
ex.printStackTrace();
 getServiceList() gives us the array of Objects,
}
 that we display in the JComboBox for the user to
return services;
 select from.
}
class MyListListener implements ActionListener {
public void actionPerformed(ActionEvent ev) {
}
 public }
 new }
 Object ServiceBrowser().buildGUI();
 static loadService(selection);
 selection void main(String[] = serviceList.getSelectedItem();
 args) {
 If selection take appropria on the we’re the service
 the h
 pre from e
 te selection re
 vious ,
 service. that it
 the m
 page, e
 corresponds a
 they JComboBo ns (see that the made the use asks x loa r an wit
 li m dService d the st. ade h load this So,
 server a
 the
 selection)
 method
 for
}
you are here�
 643
DiceService code
class DiceService (a universal service, implements Service)
import javax.swing.*;
import java.awt.event.*;
import java.io.*;
public class DiceService implements Service {
JLabel label;
JComboBox numOfDice;
public JPanel getGuiPanel() {
JPanel panel = new JPanel();
JButton button = new JButton(“Roll ‘em!”);
String[] choices = {“1”, “2”, “3”, “4”, “5”};
numOfDice = new JComboBox(choices);
label = new JLabel(“dice values here”);
button.addActionListener(new RollEmListener());
}
 return panel.add(numOfDice);
 panel.add(label);
 panel.add(button);
 panel;
 JPanel, wa
 this Service Here’s nt
service in
 the so the interface-- it one is getGuiPanel() builds selected import the th ant
 and
 actual
 e one meth
 method, loaded. the dice-rolling od! client’s The as
 You long can method gonna
 GUI.
 as do you
 wh
 call of return atever the
 when
 a
 you
public class RollEmListener implements ActionListener {
public void actionPerformed(ActionEvent ev) {
// roll the dice
String diceOutput = “”;
String selection = (String) numOfDice.getSelectedItem();
int numOfDiceToRoll = Integer.parseInt(selection);
for (int i = 0; i < numOfDiceToRoll; i++) {
int r = (int) ((Math.random() * 6) + 1);
diceOutput += (“ “ + r);
}
label.setText(diceOutput);
}
}
Sharpen your pencil
Think about ways to improve the DiceService. One
suggestion: using what you learned in the GUI chapters,
make the dice graphical. Use a rectangle, and draw the
appropriate number of circles on each one, corresponding
to the roll for that particular die.
644
 chapter 18
remote deployment with RMI
class MiniMusicService (a universal service, implements Service)
import javax.sound.midi.*;
import java.io.*;
import javax.swing.*;
import java.awt.*;
import java.awt.event.*;
public public MyDrawPanel myPanel JPanel class JPanel MiniMusicService mainPanel =myPanel;
 new getGuiPanel() MyDrawPanel();
 = new JPanel();
 implements {
 Service The does the the be painte
 service
 draw recta is dis in {
 p d).
 ngles g lay method! service a will butt
 eventually
 A (where
 o ll n it and
JButton playItButton = new JButton(“Play it”);
playItButton.addActionListener(new PlayItListener());
mainPanel.add(myPanel);
mainPanel.add(playItButton);
return mainPanel;
}
public class PlayItListener implements ActionListener {
 This is all the music stuff from the
public void actionPerformed(ActionEvent ev) {
 Code Kitchen in chapter 12, so
 we
won’t annotate it again here.try {
Sequencer sequencer = MidiSystem.getSequencer();
sequencer.open();
sequencer.addControllerEventListener(myPanel, new int[] {127});
Sequence seq = new Sequence(Sequence.PPQ, 4);
Track track = seq.createTrack();
for (int i = 0; i < 100; i+= 4) {
int rNum = (int) ((Math.random() * 50) + 1);
if (rNum < 38) { // so now only do it if num <38 (75% of the time)
track.add(makeEvent(144,1,rNum,100,i));
track.add(makeEvent(176,1,127,0,i));
track.add(makeEvent(128,1,rNum,100,i + 2));
}
} // end loop
sequencer.setSequence(seq);
sequencer.start();
sequencer.setTempoInBPM(220);
} catch (Exception ex) {ex.printStackTrace();}
} // close actionperformed
} // close inner class
you are here�
 645
MiniMusicService code
class MiniMusicService, continued...
public MidiEvent makeEvent(int comd, int chan, int one, int two, int tick) {
MidiEvent event = null;
try {
ShortMessage a = new ShortMessage();
a.setMessage(comd, chan, one, two);
event = new MidiEvent(a, tick);
}catch(Exception e) { }
return event;
}
class MyDrawPanel extends JPanel implements ControllerEventListener {
// only if we got an event do we want to paint
boolean msg = false;
public void controlChange(ShortMessage event) {
}
}
 public return msg repaint();
 = Dimension true;
 new Dimension(300,300);
 getPreferredSize() {
 Nothing seen If annotatin compare the you it “A wa all n
 ve
 e
 nt it g w
 in ry this with o
 another the n
 graphic t
 code h
 graphics the is
 entire exercise, yourself, story” CodeKitch Cod pag cha
 e e t K . then
 ry
 pter.
 en it Y ou’ve
 chen.
 in
public void paintComponent(Graphics g) {
if (msg) {
Graphics2D g2 = (Graphics2D) g;
int r = (int) (Math.random() * 250);
int gr = (int) (Math.random() * 250);
int b = (int) (Math.random() * 250);
g.setColor(new Color(r,gr,b));
int ht = (int) ((Math.random() * 120) + 10);
int width = (int) ((Math.random() * 120) + 10);
int x = (int) ((Math.random() * 40) + 10);
int y = (int) ((Math.random() * 40) + 10);
g.fillRect(x,y,ht, width);
msg = false;
} // close if
} // close method
} // close inner class
} // close class
646
 chapter 18
remote deployment with RMI
class DayOfTheWeekService (a universal service, implements Service)
import javax.swing.*;
import java.awt.event.*;
import java.awt.*;
import java.io.*;
import java.util.*;
import java.text.*;
public class DayOfTheWeekService implements Service {
JLabel outputLabel;
JComboBox JTextField JTextFieldmonth;
 year;
 day;
 The that Servi buil
d ce s t in he terface GUI
 met
hod
public JPanel getGuiPanel() {
JPanel panel = new JPanel();
JButton button = new JButton(“Do it!”);
button.addActionListener(new DoItListener());
outputLabel = new JLabel(“date appears here”);
DateFormatSymbols dateStuff = new DateFormatSymbols();
month = new JComboBox(dateStuff.getMonths());
day = new JTextField(8);
year = new JTextField(8);
JPanel inputPanel = new JPanel(new GridLayout(3,2));
inputPanel.add(new JLabel(“Month”));
inputPanel.add(month);
inputPanel.add(new JLabel(“Day”));
inputPanel.add(day);
inputPanel.add(new JLabel(“Year”));
inputPanel.add(year);
panel.add(inputPanel);
panel.add(button);
panel.add(outputLabel);
return panel;
}
public public int c.set(Calendar.MONTH, class int int c.set(Calendar.DAY_OF_MONTH, Calendar void monthNum yearNum dayNum DoItListener actionPerformed(ActionEvent c = ==Calendar.getInstance();
 = Integer.parseInt(day.getText()); Integer.parseInt(year.getText()); month.getSelectedIndex();
 implements monthNum);
 dayNum);
 ActionListener ev) {
 {
 Refer T
 of because SimpleDa for h
is
 h
ow c
 how o
 to d
 n
 e
 it u c m t
 te h is
 he a u b F se er p
 sl
 ormat t
 ightly date s e
 and r
 the 10
 should date Ca
 lets diff if
 le
 y
 n us e ou format d re print
 ar specif nt,
 need class. h t o o y a ut in wever,
 g re a Also, .
 pattern
 w m orks.
 inder
 the
c.set(Calendar.YEAR, yearNum);
Date date = c.getTime();
String dayOfWeek = (new SimpleDateFormat(“EEEE”)).format(date);
outputLabel.setText(dayOfWeek);
}
}
}
 you are here�
 647
the end... sort of
Wouldn’t it be
dreamy if this were the end
of the book? If there were no
more bullet points or puzzles
or code listings or anything else?
But that’s probably just a
fantasy...
Congratulations!
You made it to the end.
Of course, there’s still the two appendices.
And the index.
And then there’s the web site...
There’s no escape, really.
648
 chapter 18
Appendix A:
Final Code Kitchen
dance beat
Andy: groove #2
Chris: groove2 revised
Nigel: dance beat
Finally, the complete version of the BeatBox!
It connects to a simple MusicServer so that you can
send and receive beat patterns with other clients.
Yo
 the with pattern, u
r
 other m
 your e
s
s
a
g
 wh
 p c e
 layers, urrent en gets you sent along
 beat
 hit
 to
“sendIt”.
‘Start’ with the players. Incoming pattern it, to and Clic p me
 lay k then ssages that one it.
 click
 goes
 to from
 load
this is a new appendix 649
final BeatBox code
Final BeatBox client program
Most of this code is the same as the code from the CodeKitchens in the previous
chapters, so we don’t annotate the whole thing again. The new parts include:
GUI - two new components are added for the text area that displays incoming
messages (actually a scrolling list) and the text field.
NETWORKING - just like the SimpleChatClient in this chapter, the BeatBox now
connects to the server and gets an input and output stream.
THREADS - again, just like the SimpleChatClient, we start a ‘reader’ class that
keeps looking for incoming messages from the server. But instead of just text, the
messages coming in include TWO objects: the String message and the serialized
ArrayList (the thing that holds the state of all the checkboxes.)
import java.awt.*;
import javax.swing.*;
import java.io.*;
import javax.sound.midi.*;
import java.util.*;
import java.awt.event.*;
import java.net.*;
import javax.swing.event.*;
public class BeatBoxFinal {
JFrame theFrame;
JPanel mainPanel;
JList incomingList;
JTextField userMessage;
ArrayList<JCheckBox> checkboxList;
int nextNum;
Vector<String> listVector = new Vector<String>();
String userName;
ObjectOutputStream out;
ObjectInputStream in;
HashMap<String, boolean[]> otherSeqsMap = new HashMap<String, boolean[]>();
Sequencer sequencer;
Sequence sequence;
Sequence mySequence = null;
Track track;
String[] instrumentNames = {“Bass Drum”, “Closed Hi-Hat”, “Open Hi-Hat”,”Acoustic
Snare”, “Crash Cymbal”, “Hand Clap”, “High Tom”, “Hi Bongo”, “Maracas”, “Whistle”,
“Low Conga”, “Cowbell”, “Vibraslap”, “Low-mid Tom”, “High Agogo”, “Open Hi Conga”};
int[] instruments = {35,42,46,38,49,39,50,60,70,72,64,56,58,47,67,63};
650
 appendix A
appendix A Final Code Kitchen
public static void main (String[] args) {
new BeatBoxFinal().startUp(args[0]); // args[0] is your user ID/screen name
}
 Add a command-line argumen
t fo
r your screen name.
public void startUp(String name) {
 Example: % java BeatBoxF
inal theFlash
userName = name;
// open connection to the server
try {
Socket out in == new new sock ObjectInputStream(sock.getInputStream());
 ObjectOutputStream(sock.getOutputStream());
 = new Socket(“127.0.0.1”, 4242);
 Nothing netw
 start) or
kin
 the new... g,
 reade
 I/O
 set ,
 an
 r
 up d th
 ma
 rea
 the
 ke d.
 (and
Thread remote = new Thread(new RemoteReader());
remote.start();
} catch(Exception ex) {
System.out.println(“couldn’t connect - you’ll have to play alone.”);
}
setUpMidi();
buildGUI();
} // close startUp
public void buildGUI() {
 GUI code, nothing new here
theFrame = new JFrame(“Cyber BeatBox”);
BorderLayout layout = new BorderLayout();
JPanel background = new JPanel(layout);
background.setBorder(BorderFactory.createEmptyBorder(10,10,10,10));
checkboxList = new ArrayList<JCheckBox>();
Box buttonBox = new Box(BoxLayout.Y_AXIS);
JButton start = new JButton(“Start”);
start.addActionListener(new MyStartListener());
buttonBox.add(start);
JButton stop = new JButton(“Stop”);
stop.addActionListener(new MyStopListener());
buttonBox.add(stop);
JButton upTempo = new JButton(“Tempo Up”);
upTempo.addActionListener(new MyUpTempoListener());
buttonBox.add(upTempo);
JButton downTempo = new JButton(“Tempo Down”);
downTempo.addActionListener(new MyDownTempoListener());
buttonBox.add(downTempo);
JButton sendIt = new JButton(“sendIt”);
sendIt.addActionListener(new MySendListener());
buttonBox.add(sendIt);
userMessage = new JTextField();
you are here�
 651
final BeatBox code
buttonBox.add(userMessage);
incomingList = new JList();
incomingList.addListSelectionListener(new MyListSelectionListener());
incomingList.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);
JScrollPane theList = new JScrollPane(incomingList);
buttonBox.add(theList);
incomingList.setListData(listVector); // no data to start with
background.add(BorderLayout.EAST, for background.add(BorderLayout.WEST, }
 Box (int nameBox nameBox.add(new i ==0; new i Box(BoxLayout.Y_AXIS);
 < 16; Label(instrumentNames[i]));
 i++) {
 nameBox);
 buttonBox);
 JList used incoming Only where messages, SELECT to load before. instead is you a and messages component in a just This this message of play
 LOOK
 a app th
 is ar no wh e e rmal fr
 we you displayed.
 at at ere om tach
ed
 ha can
 chat
 the
 the
 ven’t
 the list
theFrame.getContentPane().add(background);
 beat patter
n.
GridLayout grid = new GridLayout(16,16);
grid.setVgap(1);
grid.setHgap(2);
mainPanel = new JPanel(grid);
background.add(BorderLayout.CENTER, mainPanel);
for (int i = 0; i < 256; i++) {
JCheckBox c.setSelected(false);
 c = new JCheckBox();
 Nothing else on this page is new
checkboxList.add(c);
mainPanel.add(c);
} // end loop
theFrame.setBounds(50,50,300,300);
theFrame.pack();
theFrame.setVisible(true);
} // close buildGUI
public void setUpMidi() {
try {
sequencer = MidiSystem.getSequencer();
sequencer.open();
sequence sequencer.setTempoInBPM(120);
 track = sequence.createTrack();
 = new Sequence(Sequence.PPQ,4);
 Get Sequence,
 the Se a q n u d encer, make make a Track
 a
} catch(Exception e) {e.printStackTrace();}
} // close setUpMidi
652
 appendix A
appendix A Final Code Kitchen
public void buildTrackAndStart() {
ArrayList<Integer> trackList = null; // this will hold the instruments for each
for sequence.deleteTrack(track);
 track for trackList (int (int = sequence.createTrack();
 i j === 0; new 0; i ArrayList<Integer>();
 < j < 16; 16; i++) j++) {
 {
 Build to instrumen This was CodeKitc
 get in a ist tr th pr h
e t a e etty ir hens ck (and previous state, by
 to complex, walking making get and chapters, the mapping the but throu
 full M it gh so id e
 is t iE xplanation refer the h EXACTLY vent at
 chec t
 o
 to for a k n previous
 b
 again.
 it).
 oxes
 as it
JCheckBox jc = (JCheckBox) checkboxList.get(j + (16*i));
if (jc.isSelected()) {
int key = instruments[i];
trackList.add(new Integer(key));
} else {
trackList.add(null); // because this slot should be empty in the track
}
} // close inner loop
makeTracks(trackList);
} // close outer loop
track.add(makeEvent(192,9,1,0,15)); // - so we always go to full 16 beats
try {
sequencer.setSequence(sequence);
sequencer.setLoopCount(sequencer.LOOP_CONTINUOUSLY);
sequencer.start();
sequencer.setTempoInBPM(120);
} catch(Exception e) {e.printStackTrace();}
} // close method
public class MyStartListener implements ActionListener {
public void actionPerformed(ActionEvent a) {
buildTrackAndStart();
} // } close // close inner actionPerformed
 class
 The Exactly previous GUI c
 li h st e e same ners.
 as version.
 the
thapter’spublic class MyStopListener implements ActionListener {
public void actionPerformed(ActionEvent a) {
sequencer.stop();
} // close actionPerformed
} // close inner class
public class MyUpTempoListener implements ActionListener {
public void actionPerformed(ActionEvent a) {
float tempoFactor = sequencer.getTempoFactor();
sequencer.setTempoFactor((float)(tempoFactor * 1.03));
} // close actionPerformed
} // close inner class
you are here�
 653
final BeatBox code
public class MyDownTempoListener implements ActionListener {
public void actionPerformed(ActionEvent a) {
float tempoFactor = sequencer.getTempoFactor();
sequencer.setTempoFactor((float)(tempoFactor * .97));
}
}
public class MySendListener implements ActionListener {
public void actionPerformed(ActionEvent a) {
// make an arraylist of just the STATE of the checkboxes
boolean[] checkboxState = new boolean[256];
for (int i = 0; i < 256; i++) {
String } }
 // JCheckBox if close checkboxState[i] messageToSend (check.isSelected()) loop
 check = = null;
 (JCheckBox) = true;
 {
 checkboxList.get(i);
 T
 instead (the
 t
 h
 w
 is
 o
 is objects S
t
 new... of r
ing se message n
 it to d ’s ing th
 a e lot a socket
 String and lik
e
 the t
 o
 message, h
 u
 e
 be t
p S
 a u im
 t t p
 p
 stream le
 a w tt ChatClient eern) seriali
 (to and z
 the e , write t e wo xcept
 server
 objects
 those
 ).
try {
out.writeObject(userName + nextNum++ + “: “ + userMessage.getText());
out.writeObject(checkboxState);
} catch(Exception ex) {
System.out.println(“Sorry dude. Could not send it to the server.”);
}
userMessage.setText(“”);
} // close actionPerformed
} // close inner class
public class MyListSelectionListener implements ListSelectionListener {
public void valueChanged(ListSelectionEvent le) {
if (!le.getValueIsAdjusting()) {
String selected = (String) incomingList.getSelectedValue();
if (selected != null) {
// now go to the map, and change the sequence
boolean[] selectedState = (boolean[]) otherSeqsMap.get(selected);
changeSequence(selectedState);
sequencer.stop();
buildTrackAndStart();
}
} // } close // }
 close inner valueChanged
 class
 ListSelec if called load When when This tests the other the the isb t a
 e io ass ls cause use nEvents.
 u o SeqsMap) se ociated new r r selects made of -- little beat a a and a selection ListSelec
 messag quir patte sta k rt y rn e, things tionListener on playing we (it’s the IMMEDIATE abo
 in it. list the ut The of gettin that
 Hmess re ash ’s Map
 LY
 tells ages.
 so
 g
 me
 us
654
 appendix A
appendix A Final Code Kitchen
public Object public boolean[] String try class while((obj=in.readObject()) {
 obj nameToShow void RemoteReader checkboxState =run() null;
= {
 null;
 implements = null;
 != Runnable null) {
{
 This from always String ArrayLis
 is the the be me t se ssage two t rver. of h
r
e
 serialized checkbox a
 and d
 In jo
 this the b
 -
- code, beat state read p ‘d va
 s: in a lues)
 the
 ta’ dat will
 a
 (an
objectatternSystem.out.println(“got an object from server”);
} public // } close } // class catch(Exception } close incomingList.setListData(listVector);
 listVector.add(nameToShow);
 otherSeqsMap.put(nameToShow, // inner checkboxState String System.out.println(obj.getClass());
 close MyPlayMineListener run
 class
 nameToShow while
 = ex) (boolean[]) = {ex.printStackTrace();}
 (String) implements in.readObject();
 obj;
 checkboxState);
 ActionListener for JList f
 of ashioned is t
he checkbox message {
 what
 a When the two-st JList to list to u a se A an s me
 rrayList), st displa c ep t
 data d omponent. ate ha
 ssage ) the thing: the ty values) Vector (Vector in
 ArrayLis comes two you the and Add obje and as k list.
 in, is then
 eep ing t it’s cts an w
 add of e a to old-
 t
 so (the
 read
 Vector
 e Boolean
 it u a ll rce
 the
 to
 JList
(
deserializepublic void actionPerformed(ActionEvent a) {
if (mySequence != null) {
sequence = mySequence;
 // restore to my original
}
public } // } close // void close inner changeSequence(boolean[] actionPerformed
 class
 checkboxState) change something This {
 method thefrom pattern is ca
 th lle e to d lis t. wh
 the en We one the IMMEDIATELY
 they user selected.
 selects
for (int i = 0; i < 256; i++) {
JCheckBox check = (JCheckBox) checkboxList.get(i);
if (checkboxState[i]) {
check.setSelected(true);
} else {
check.setSelected(false);
} // }
 close loop
 All the MIDI stuff is exactly
 the same as it
} // close changeSequence
 was in the previous version.
public void makeTracks(ArrayList list) {
Iterator it = list.iterator();
for (int i = 0; i < 16; i++) {
Integer num = (Integer) it.next();
if (num != null) {
int numKey = num.intValue();
track.add(makeEvent(144,9,numKey, 100, i));
track.add(makeEvent(128,9,numKey,100, i + 1));
}
} // close loop
} // close makeTracks()
you are here�
 655
final BeatBox code
public MidiEvent makeEvent(int comd, int chan, int one, int two, int tick) {
MidiEvent event = null;
try {
ShortMessage a = new ShortMessage();
a.setMessage(comd, chan, one, two);
event = new MidiEvent(a, tick);
}catch(Exception return event;
 e) { }
 Nothing new
.
 J
u
st
 li
k
e
 t
he last versio
n.
} // close makeEvent
} // close class
Sharpen your pencil
What are some of the ways you can improve this program?
Here are a few ideas to get you started:
1) Once you select a pattern, whatever current pattern was playing is blown
away. If that was a new pattern you were working on (or a modification of
another one), you’re out of luck. You might want to pop up a dialog box that
asks the user if he’d like to save the current pattern.
2) If you fail to type in a command-line argument, you just get an exception
when you run it! Put something in the main method that checks to see if
you’ve passed in a command-line argument. If the user doesn’t supply one,
either pick a default or print out a message that says they need to run it
again, but this time with an argument for their screen name.
3) It might be nice to have a feature where you can click a button and it will
generate a random pattern for you. You might hit on one you really like.
Better yet, have another feature that lets you load in existing ‘foundation’
patterns, like one for jazz, rock, reggae, etc. that the user can add to.
You can find existing patterns on the Head First Java web start.
656
 appendix A
Final BeatBox server program
Most of this code is identical to the SimpleChatServer we made in the
Networking and Threads chapter. The only difference, in fact, is that this server
receives, and then re-sends, two serialized objects instead of a plain String
(although one of the serialized objects happens to be a String).
appendix A Final Code Kitchen
import java.io.*;
import java.net.*;
import java.util.*;
public class MusicServer {
ArrayList<ObjectOutputStream> clientOutputStreams;
public static void main (String[] args) {
new MusicServer().go();
}
public class ClientHandler implements Runnable {
ObjectInputStream in;
Socket clientSocket;
public ClientHandler(Socket socket) {
try {
clientSocket = socket;
in = new ObjectInputStream(clientSocket.getInputStream());
} catch(Exception ex) {ex.printStackTrace();}
} // close constructor
public void run() {
Object o2 = null;
Object o1 = null;
try {
while ((o1 = in.readObject()) != null) {
o2 = in.readObject();
System.out.println(“read two objects”);
tellEveryone(o1, o2);
} // close while
} catch(Exception ex) {ex.printStackTrace();}
} // close run
} // close inner class
you are here�
 657
final BeatBox code
public void go() {
clientOutputStreams = new ArrayList<ObjectOutputStream>();
try {
ServerSocket serverSock = new ServerSocket(4242);
while(true) {
Socket clientSocket = serverSock.accept();
ObjectOutputStream out = new ObjectOutputStream(clientSocket.getOutputStream());
clientOutputStreams.add(out);
Thread t = new Thread(new ClientHandler(clientSocket));
t.start();
System.out.println(“got a connection”);
}
}catch(Exception ex) {
ex.printStackTrace();
}
} // close go
public void tellEveryone(Object one, Object two) {
Iterator it = clientOutputStreams.iterator();
while(it.hasNext()) {
try {
ObjectOutputStream out = (ObjectOutputStream) it.next();
out.writeObject(one);
out.writeObject(two);
}catch(Exception ex) {ex.printStackTrace();}
}
} // close tellEveryone
} // close class
658
 appendix A
Appendix B
The Top Ten Topics that almost made it into the Real Book...
You mean, there’s still
MORE? Doesn’t this
book EVER end?
We covered a lot of ground, and you’re almost finished with this book. We’ll miss you, but before
we let you go, we wouldn’t feel right about sending you out into JavaLand without a little more
preparation. We can’t possibly fit everything you’ll need to know into this relatively small appendix.
Actually, we did originally include everything you need to know about Java (not already covered by
the other chapters), by reducing the type point size to .00003. It all fit, but nobody could read it. So,
we threw most of it away, but kept the best bits for this Top Ten appendix.
This really is the end of the book. Except for the index (a must-read!).
this is a new appendix 659
bit manipulation
#10 Bit Manipulation
Why do you care?
We’ve talked about the fact that there are 8 bits in a byte,
16 bits in a short, and so on. You might have occasion to
turn individual bits on or off. For instance you might find
yourself writing code for your new Java enabled toaster,
and realize that due to severe memory limitations, certain
toaster settings are controlled at the bit level. For easier
reading, we’re showing only the last 8 bits in the comments
rather than the full 32 for an int).
Bitwise NOT Operator: ~
This operator ‘flips all the bits’ of a primitive.
int x = 10;
 //
 bits are 00001010
x = ~x;
 // bits are now 11110101
The next three operators compare two primitives on a bit by
bit basis, and return a result based on comparing these bits.
We’ll use the following example for the next three opera-
tors:
int x = 10;
int y =
 6;
// bits are 00001010
// bits are 00000110
Bitwise AND Operator: &
This operator returns a value whose bits are turned on only
if both original bits are turned on:
int a = x & y;
 // bits are 00000010
Bitwise OR Operator: |
This operator returns a value whose bits are turned on only
if either of the original bits are turned on:
int a = x | y;
 // bits are 00001110
Bitwise XOR (exclusive OR) Operator: ^
This operator returns a value whose bits are turned on only
if exactly one of the original bits are turned on:
int a = x ^ y;
 // bits are 00001100
660
 appendix B
The Shift Operators
These operators take a single integer primitive and shift (or
slide) all of its bits in one direction or another. If you want
to dust off your binary math skills, you might realize that
shifting bits left effectively multiplies a number by a power of
two, and shifting bits right effectively divides a number by a
power of two.
We’ll use the following example for the next three operators:
int x = -11;
 //
 bits are 11110101
Ok, ok, we’ve been putting it off, here is the world’s
shortest explanation of storing negative numbers, and
two’s complement. Remember, the leftmost bit of an integer
number is called the sign bit. A negative integer number in
Java always has its sign bit turned on (i.e. set to 1). A positive
integer number always has its sign bit turned off (0). Java
uses the two’s complement formula to store negative numbers.
To change a number’s sign using two’s complement, flip all
the bits, then add 1 (with a byte, for example, that would
mean adding 00000001 to the flipped value).
Right Shift Operator: >>
This operator shifts all of a number’s bits right by a certain
number, and fills all of the bits on the left side with whatever
the original leftmost bit was. The sign bit does not change:
int y = x >> 2;
 // bits are 11111101
Unsigned Right Shift Operator: >>>
Just like the right shift operator BUT it ALWAYS fills the
leftmost bits with zeros. The sign bit might change:
int y = x >>> 2;
 // bits are 00111101
Left Shift Operator: <<
Just like the unsigned right shift operator, but in the other
direction; the rightmost bits are filled with zeros. The sign bit
might change.
int y = x << 2;
 // bits are 11010100
#9 Immutability
Why do you care that Strings are Immutable?
When your Java programs start to get big, you’ll
inevitably end up with lots and lots of String objects.
For security purposes, and for the sake of conserving
memory (remember your Java programs can run on
teeny Java-enabled cell phones), Strings in Java are
immutable. What this means is that when you say:
String s = “0”;
for (int x = 1; x < 10; x++) {
s = s + x;
}
What’s actually happening is that you’re creating ten
String objects (with values “0”, “01”, “012”, through
“0123456789”). In the end s is referring to the String
with the value “0123456789”, but at this point there
are ten Strings in existence!
Whenever you make a new String, the JVM puts it
into a special part of memory called the ‘String Pool’
(sounds refreshing doesn’t it?). If there is already
a String in the String Pool with the same value, the
JVM doesn’t create a duplicate, it simply refers your
reference variable to the existing entry. The JVM can
get away with this because Strings are immutable; one
reference variable can’t change a String’s value out
from under another reference variable referring to
the same String.
The other issue with the String pool is that the
Garbage Collector doesn’t go there. So in our example,
unless by coincidence you later happen to make a
String called “01234”, for instance, the first nine
Strings created in our for loop will just sit around
wasting memory.
How does this save memory?
Well, if you’re not careful, it doesn’t! But if you un-
derstand how String immutability works, than you
can sometimes take advantage of it to save memory.
If you have to do a lot of String manipulations (like
concatenations, etc.), however, there is another class
StringBuilder, better suited for that purpose. We’ll
talk more about StringBuilder in a few pages.
appendix B Top Ten Reference
Why do you care that Wrappers are
Immutable?
In the Math chapter we talked about the two main
uses of the wrapper classes:
• Wrapping a primitive so it can pretend to be an
object.
• Using the static utility methods (for example,
Integer.parseInt()).
It’s important to remember that when you create a
wrapper object like:
Integer iWrap = new Integer(42);
That’s it for that wrapper object. Its value will always
be 42. There is no setter method for a wrapper object.
You can, of course, refer iWrap to a different wrapper
object, but then you’ll have two objects. Once you
create a wrapper object, there’s no way to change
the value of that object!
Make it Stic
k
Strings Roses are are re
d,
 violets are blue.
immutable
, wrappers
 are
 too.
Right Oh look! here A in bonu
s t
he Make appendix.
 it Stic
k.
you are here�
 661
assertions
#8 Assertions
We haven’t talked much about how to debug your Java
program while you’re developing it. We believe that
you should learn Java at the command line, as we’ve
been doing throughout the book. Once you’re a Java
pro, if you decide to use an IDE*, you might have
other debugging tools to use. In the old days, when
a Java programmer wanted to debug her code, she’d
stick a bunch of System.out.println( ) statements
throughout the program, printing current variable
values, and “I got here” messages, to see if the flow
control was working properly. (The ready-bake code
in chapter 6 left some debugging ‘print’ statements
in the code.) Then, once the program was working
correctly, she’d go through and take all those System.
out.println( ) statements back out again. It was
tedious and error prone. But as of Java 1.4 (and 5.0),
debugging got a whole lot easier. The answer?
Assertions
Assertions are like System.out.println( ) statements
on steroids. Add them to your code as you would
add println statements. The Java 5.0 compiler
assumes you’ll be compiling source files that are 5.0
compatible, so as of Java 5.0, compiling with assertions
is enabled by default.
At runtime, if you do nothing, the assert statements
you added to your code will be ignored by the JVM,
and won’t slow down your program. But if you tell the
JVM to enable your assertions, they will help you do
your debugging, without changing a line of code!
Some folks have complained about having to leave
assert statements in their production code, but
leaving them in can be really valuable when your
code is already deployed in the field. If your client
is having trouble, you can instruct the client to run
the program with assertions enabled, and have the
client send you the output. If the assertions were
stripped out of your deployed code, you’d never
have that option. And there is almost no downside;
when assertions are not enabled, they are completely
ignored by the JVM, so there’s no performance hit to
worry about.
How to make Assertions work
Add assertion statements to your code wherever you
believe that something must be true. For instance:
assert (height > 0);
// if true, program continues normally
// if false, throw an AssertionError
You can add a little more information to the stack
trace by saying:
assert (height > 0) : “height = “ +
height + “ weight = “ + weight;
The expression after the colon can be any legal
Java expression that resolves to a non-null value. But
whatever you do, don’t create assertions that change an
object’s state! If you do, enabling assertions at runtime
might change how your program performs.
Compiling and running with
Assertions
To compile with assertions:
javac TestDriveGame.java
(Notice that no command line options were
necessary.)
To run with assertions:
java -ea TestDriveGame
* IDE stands for Integrated Development Environment
and includes tools such as Eclipse, Borland’s JBuilder, or
the open source NetBeans (netbeans.org).
662
 appendix B
appendix B Top Ten Reference
#7 Block Scope
In chapter 9, we talked about how local variables live
only as long as the method in which they’re declared
stays on the stack. But some variables can have even
shorter lifespans. Inside of methods, we often create
blocks of code. We’ve been doing this all along, but we
haven’t explicitly talked in terms of blocks. Typically,
blocks of code occur within methods, and are
bounded by curly braces { }. Some common examples
of code blocks that you’ll recognize include loops (for,
while) and conditional expressions (like if statements).
Let’s look at an example:
 start of t
he me
thod block
voidint doStuff() x = 0;
 {
 local variable scoped to
 the entire method
for(int y = 0; y < 5; y++) {
 beginning scoped toof only a for the loo
 for
 p loop!
 block, and y is
x = x + y;
 No problem, x
 and y are both in scope
}
end of the for loop block
x = x * y;
 the Aack! way Won’t it works compile! in som y
 is e out
 other oflanguages, scope here! so (th bew is are
!)
 is not
}
end of the method block, now x
 is also out of scope
In the previous example, y was a block variable,
declared inside a block, and y went out of scope as
soon as the for loop ended. Your Java programs will
be more debuggable and expandable if you use local
variables instead of instance variables, and block
variables instead of local variables, whenever possible.
The compiler will make sure that you don’t try to use
a variable that’s gone out of scope, so you don’t have
to worry about runtime meltdowns.
you are here�
 663
linked invocations
#6 Linked Invocations
While you did see a little of this in this book, we tried to keep our syntax as clean and
readable as possible. There are, however, many legal shortcuts in Java, that you’ll no doubt
be exposed to, especially if you have to read a lot code you didn’t write. One of the more
common constructs you will encounter is known as linked invocations. For example:
StringBuffer sb = new StringBuffer(“spring”);
sb = sb.delete(3,6).insert(2,”umme”).deleteCharAt(1);
System.out.println(“sb = “ + sb);
// result is sb = summer
What in the world is happening in the second line of code? Admittedly, this is a contrived
example, but you need to learn how to decipher these.
1 - Work from left to right.
2 - Find the result of the leftmost method call, in this case sb.delete(3,6). If you
look up StringBuffer in the API docs, you’ll see that the delete() method returns a
StringBuffer object. The result of running the delete() method is a StringBuffer object
with the value “spr”.
3 - The next leftmost method (insert())is called on the newly created StringBuffer
object “spr”. The result of that method call (the insert() method), is also a StringBuffer
object (although it doesn’t have to be the same type as the previous method return), and so
it goes, the returned object is used to call the next method to the right. In theory, you can
link as many methods as you want in a single statement (although it’s rare to see more than
three linked methods in a single statement). Without linking, the second line of code from
above would be more readable, and look something like this:
sb = sb.delete(3,6);
sb = sb.insert(2,”umme”);
sb = sb.deleteCharAt(1);
But here’s a more common, and useful example, that you saw us using, but we thought
we’d point it out again here. This is for when your main() method needs to invoke an
instance method of the main class, but you don’t need to keep a reference to the instance of
the class. In other words, the main() needs to create the instance only so that main() can
invoke one of the instance’s methods.
class Foo {
}
 public void newgo() Foo().go();
 static {
 void main(String we
 the the wa
 Foo new nt
 instance to
 Foo [] ca
 ob
 ll
 je go
 args) , ct
 so ()
,to we but a don’t [
 referenc
 we don’t both e. er
 care as
signing
 about
// here’s what we REALLY want...
}
}
664
 appendix B
#5 Anonymous and Static Nested Classes
appendix B Top Ten Reference
Nested classes come in many flavors
In the GUI event-handling section of the book, we started using inner (nested) classes as a
solution for implementing listener interfaces. That’s the most common, practical, and read-
able form of an inner class—where the class is simply nested within the curly braces of another
enclosing class. And remember, it means you need an instance of the outer class in order to get
an instance of the inner class, because the inner class is a member of the outer/enclosing class.
But there are other kinds of inner classes including static and anonymous. We’re not going
into the details here, but we don’t want you to be thrown by strange syntax when you see it in
someone’s code. Because out of virtually anything you can do with the Java language, perhaps
nothing produces more bizarre-looking code than anonymous inner classes. But we’ll start with
something simpler—static nested classes.
Static nested classes
You already know what static means—something tied to the class, not a particular instance. A
static nested class looks just like the non-static classes we used for event listeners, except they’re
marked with the keyword static.
public static class class FooOuter BarInner {
 {
 A class marked static enclosed with nested th
 wi e th clas static in s another, is just modifier.
 that—a
 and
void sayIt() {
System.out.println(“method of a static inner class”);
}
class }
 }
 Test {
 methods name use Because an of instance or the a stat
 acce cl
as ic of ss s, ne st th the sted atic e outer same class variables.
 way class. is...static, you You invoke just
 you static use don’t
 th
e
public static void main (String[] args) {
FooOuter.BarInner foo = new FooOuter.BarInner();
foo.sayIt();
}
}
Static nested classes are more like regular non-nested classes in that they don’t enjoy a special relation-
ship with an enclosing outer object. But because static nested classes are still considered a member of
the enclosing/outer class, they still get access to any private members of the outer class... but only the
ones that are also static. Since the static nested class isn’t connected to an instance of the outer class, it
doesn’t have any special way to access the non-static (instance) variables and methods.
you are here�
 665
when arrays aren’t enough
#5 Anonymous and Static Nested Classes, continued
The difference between nested and inner
Any Java class that’s defined within the scope of another class is known as a nested class. It
doesn’t matter if it’s anonymous, static, normal, whatever. If it’s inside another class, it’s
technically considered a nested class. But non-static nested classes are often referred to as inner
classes, which is what we called them earlier in the book. The bottom line: all inner classes are
nested classes, but not all nested classes are inner classes.
Anonymous inner classes
Imagine you’re writing some GUI code, and suddenly realize that you need an instance
of a class that implements ActionListener. But you realize you don’t have an instance of an
ActionListener. Then you realize that you also never wrote a class for that listener. You have two
choices at that point:
1) Write an inner class in your code, the way we did in our GUI code, and then instantiate it
and pass that instance into the button’s event registration (addActionListener()) method.
OR
2) Create an anonymous inner class and instantiate it, right there, just-in-time. Literally right
where you are at the point you need the listener object. That’s right, you create the class and the
instance in the place where you’d normally be supplying just the instance. Think about that for
a moment—it means you pass the entire class where you’d normally pass only an instance into a
method argument!
public import import This ends }
 public down
 statemen
 button.addActionListener frame.getContentPane().add(button);
 JButton // JFrame java.awt.event.*;
 class javax.swing.*;
 public }
button.addActionListener(quitListener);
 h
 static System.exit(0);
 er
 t
 e!
 frame TestAnon :
 button void No
 void though can
 really that and
 interface tic
 = ’t
 actionPerformed(ActionEvent by e
 = new implements MA
 means, {
 main th
 ActionL
 the new at
 KE
 methods JFrame();
 way, we
 “create an
 JButton(“click”);
 (String[] ist
 say
 ins
 the her ene
 ta
 .actio
 “ne
 e’s nce r
 Ac
 a w
 is
 new th
 tionListener of nP an
 ActionListener() Ac
 eargs) erf
 tio
 class implementation it! int
 nListener()” ormed().
 erf
 But (with ace
 {
 this interf and
 no syn
 ev) nam
 We need so Except ace ActionList of
 even
 tax
 you
 made e) ,
the
 {
 t
 {
 o
 we re
 Normally a a inner the gi referen f en
 never st
 rame
 er
 actionP
 er
 But reference, class class write tionListen NEED instance int
 a
 made a
 we’d n
 ce
 no nd
 er
 that action defin erformed() to w
 the f
 a
 ace
...
 do IT. d
 in
 a an d
 o
 st
 we ition!! implements class class
 er f ed
 so
 ea
 The listen instance the m
 pass RIGHT d
 a
 et
 that th o
 button, syntax In h
 er class f
 method).
 at in... in
 passing other g
 with of implements implements HERE li
 Actio
 the automatically
 k
e
 also an and t
 words, this—passing w he in inne nL
 WHER h
 creates no o
 button.
 a is le n w r tener o A the
 new
 class
 we
 we
 bject
 c-
 E .
 an
 WE
 ... (and
 in an
(new});
}
666
 appendix B
access levels
 appendix B Top Ten Reference
#4 Access Levels and Access Modifiers (Who Sees What)
Java has four access levels and three access modifiers. There are only three modifiers because
the default (what you get when you don’t use any access modifier) is one of the four
access levels.
Access Levels (in order of how restrictive they are, from least to most restrictive)
public
 public means any code anywhere can access the public thing (by
‘thing’ we mean class, variable, method, constructor, etc.).
protected
 protected works just like default (code in the same package has access), EXCEPT it
also allows subclasses outside the package to inherit the protected thing.
default
default access means that only code within the same package as
the class with the default thing can access the default thing.
private
private means that only code within the same class can access the private thing.
Keep in mind it means private to the class, not private to the object. One Dog
can see another Dog object’s private stuff, but a Cat can’t see a Dog’s privates.
Access modifiers
public
protected
private
Most of the time you’ll use only public and private access levels.
public
Use public for classes, constants (static final variables), and methods that you’re
exposing to other code (for example getters and setters) and most constructors.
private
Use private for virtually all instance variables, and for methods that you don’t want
outside code to call (in other words, methods used by the public methods of your class).
But although you might not use the other two (protected and default), you still need to
know what they do because you’ll see them in other code.
you are here�
 667
when arrays aren’t enough
#4 Access Levels and Access Modifiers, cont.
default and protected
default
Both protected and default access levels are tied to packages. Default access is simple—it
means that only code within the same package can access code with default access. So a de-
fault class, for example (which means a class that isn’t explicitly declared as public) can
be accessed by only classes within the same package as the default class.
But what does it really mean to access a class? Code that does not have access to a class is
not allowed to even think about the class. And by think, we mean use the class in code.
For example, if you don’t have access to a class, because of access restriction, you aren’t
allowed to instantiate the class or even declare it as a type for a variable, argument, or
return value. You simply can’t type it into your code at all! If you do, the compiler will
complain.
Think about the implications—a default class with public methods means the public
methods aren’t really public at all. You can’t access a method if you can’t see the class.
Why would anyone want to restrict access to code within the same package? Typically,
packages are designed as a group of classes that work together as a related set. So it
might make sense that classes within the same package need to access one another’s
code, while as a package, only a small number of classes and methods are exposed to
the outside world (i.e. code outside that package).
OK, that’s default. It’s simple—if something has default access (which, remember,
means no explicit access modifier!), only code within the same package as the default
thing (class, variable, method, inner class) can access that thing.
Then what’s protected for?
protected
Protected access is almost identical to default access, with one exception: it allows
subclasses to inherit the protected thing, even if those subclasses are outside the package of the
superclass they extend. That’s it. That’s all protected buys you—the ability to let your sub-
classes be outside your superclass package, yet still inherit pieces of the class, including
methods and constructors.
Many developers find very little reason to use protected, but it is used in some designs,
and some day you might find it to be exactly what you need. One of the interesting
things about protected is that—unlike the other access levels—protected access applies
only to inheritance. If a subclass-outside-the-package has a reference to an instance of the
superclass (the superclass that has, say, a protected method), the subclass can’t access
the protected method using that superclass reference! The only way the subclass can
access that method is by inheriting it. In other words, the subclass-outside-the-package
doesn’t have access to the protected method, it just has the method, through inheritance.
668
 appendix B
String and StringBuffer
 appendix B Top Ten Reference
#3 String and StringBuffer/StringBuilder Methods
Two of the most commonly used classes in the Java API are String and StringBuffer (remember from
#9 a few pages back, Strings are immutable, so a StringBuffer/StringBuilder can be a lot more efficient
if you’re manipulating a String). As of Java 5.0 you should use the StringBuilder class instead of
StringBuffer, unless your String manipulations need to be thread-safe, which is not common. Here’s a
brief overview of the key methods in these classes:
Both String and StringBuffer/StringBuilder classes have:
char charAt(int index);
 // what char is at a certain position
int length();
 // how long is this
String substring(int start, int end);
 // get a part of this
String toString();
 // what’s the String value of this
To concatenate Strings:
String concat(string);
String append(String);
// for the String class
// for StringBuffer & StringBuilder
The String class has:
String replace(char old, char new);
 // replace all occurences of a char
String substring(int begin, int end);
 // get a portion of a String
char [] toCharArray();
 // convert to an array of chars
String toLowerCase();
 // convert all characters to lower case
String toUpperCase();
 // convert all characters to upper case
String trim();
 // remove whitespace from the ends
String valueOf(char [])
 // make a String out of a char array
String valueOf(int i)
 // make a String out of a primitive

 // other primitives are supported as well
The StringBuffer & StringBuilder classes have:
StringBxxxx delete(int start, int end);
 // delete a portion
StringBxxxx insert(int offset, any primitive or a char []);
 // insert something
StringBxxxx replace(int start, int end, String s);
 // replace this part with this String
StringBxxxx reverse();
 // reverse the SB from front to back
void setCharAt(int index, char ch);
 // replace a given character
Note: StringBxxxx refers to either StringBuffer or StringBuilder, as appropriate.
you are here�
669
when arrays aren’t enough
#2 Multidimensional Arrays
In most languages, if you create, say, a 4 x 2 two-dimensional array, you would visualize a
rectangle, 4 elements by 2 elements, with a total of 8 elements. But in Java, such an array
would actually be 5 arrays linked together! In Java, a two dimensional array is simply an array
of arrays. (A three dimensional array is an array of arrays of arrays, but we’ll leave that for
you to play with.) Here’s how it works
int[][] a2d
 = new int [4][2];
The JVM creates an array with 4 elements. Each of these four elements is actually a reference
variable referring to a (newly created), int array with 2 elements.
a2d[1]
a2d[0][0 a2d
 ]
 [0][1]
regular of the ints eight go ele
 ins me id nt e s
 each
 a2d[2][1]
a2d[3][
0]
int
 int
 int
 int
int
int
 int array (int[])
 int array (int[])
int array (i
nt[]
)
 int
int
int arr
ay(int
[])
4
 in
t
[
]
 r
 eference v
aria
bles
int[]
 int[]
 int[]
 int[]
a2d
int[][]
 int array object (int[][])
 (an Remember array holding that the refe ar
 rences ray itself to int is an arrays)
 object
Working with multidimensional arrays
- To access the second element in the third array: int x = a2d[2][1];
 // remember, 0 based!
- To make a one-dimensional reference to one of the sub-arrays: int[] copy = a2d[1];
- Short-cut initialization of a 2 x 3 array:
 int[][] x = { { 2,3,4 }, { 7,8,9 } };
- To make a 2d array with irregular dimensions:
int[][] y = new int [2][];
 // makes only the first array, with a length of 2
y[0] = new int [3];
 // makes the first sub-array 3 elements in length
y[1] = new int [5];
 // makes the second sub-array 5 elements in length
670
 appendix B
enumerations
 appendix B Top Ten Reference
And the number one topic that didn’t quite make it in...
#1 Enumerations (also called Enumerated Types or Enums)
We’ve talked about constants that are defined in the API, for instance,
JFrame.EXIT_ON_CLOSE. You can also create your own constants by
marking a variable static final. But sometimes you’ll want to create a set
of constant values to represent the only valid values for a variable. This set of
valid values is commonly referred to as an enumeration. Before Java 5.0 you
could only do a half-baked job of creating an enumeration in Java. As of Java
5.0 you can create full fledged enumerations that will be the envy of all your
pre-Java 5.0-using friends.
Who’s in the band?
Let’s say that you’re creating a website for your favorite band, and you want to
make sure that all of the comments are directed to a particular band member.
The old way to fake an “enum”:
public static final int JERRY = 1;
public static final int BOBBY = 2;
public static final int PHIL = 3;
// later in the code
 We’re hoping that by the time we got here
if (selectedBandMember == JERRY) {
 “selectedBandMember” has a valid value!
// do JERRY related stuff
}
The good news about this technique is that it DOES make the code easier to
read. The other good news is that you can’t ever change the value of the fake
enums you’ve created; JERRY will always be 1. The bad news is that there’s
no easy or good way to make sure that the value of selectedBandMember
will always be 1, 2, or 3. If some hard to find piece of code sets
selectedBandMember equal to 812, it’s pretty likely your code will break...
you are here�
 671
when arrays aren’t enough
#1 Enumerations, cont.
The same situation using a genuine Java 5.0 enum. While this is a very basic
A enumeration, public public new, official enum Members most “enum”:
 Members enumerations selectedBandMember;
 { JERRY, usually BOBBY, are this simple.
 PHIL };
 This doesn’t special enumerate
 kind kin it of ?
 d
 d
 It of looks type turns class. li
 called ke
 out Here a
 s
 that “Memb
 im
 we’v p
le
 enums e
 class e
 created rs”.
 defin
 ARE
 a ition
 a
 new
// later in the code
The “selectedBandMember” variable is of type
if (selectedBandMember == Members.JERRY) {
 “Members”, and can ONLY have a value of
// do JERRY related stuff
 “JERRY”, “BOBBY”, or “PHIL”.
}
No need to worry about this variable’s value!
 The syntax to refer to an enum “instance”.
Your enum extends java.lang.Enum
When you create an enum, you’re creating a new class, and you’re implicitly extending
java.lang.Enum. You can declare an enum as its own standalone class, in its own
source file, or as a member of another class.
Using “if” and “switch” with Enums
Using the enum we just created, we can perform branches in our code using either
the if or switch statement. Also notice that we can compare enum instances using
either == or the .equals() method. Usually == is considered better style.
Assigning an enum value to a variable.
Members n = Members.BOBBY;
if if (n.equals(Members.JERRY)) (n == Members.BOBBY) System.out.println(“Rat System.out.println(“Jerrrry!”);
 Dog”);
 Bot
h
 o
f
 t
h
e
se work fin
e!
“Rat Dog”
 is printed.
Members ifName = Members.PHIL;
switch (ifName) {
case JERRY: System.out.print(“make it sing “);
case PHIL: System.out.print(“go deep “);
 Pop Quiz! What’s the output?
case BOBBY: System.out.println(“Cassidy! ”);
}
Answer:
 !
ydissaC peed og672
 appendix B
enumerations
 appendix B Top Ten Reference
#1 Enumerations, completed
A really tricked-out version of a similar enum
You can add a bunch of things to your enum like a constructor, methods,
variables, and something called a constant-specific class body. They’re
not common, but you might run into them:
public enum BOBBY(“rhythm JERRY(“lead Names class {
 HfjEnum guitar”) },
 guitar”) },
 {
return return { public { public “plaintively”; “hoarsely”; String String This the sings() constru
 is sings() }
 an }
 ar
 ctor g
u
m
 {
 e
 declared n
t
 {
 p
a
ssed below.
 These in “constant-specif
 Think basic the called to
 “sing()” enum are of on them the amethod variable method), so-calle as ic
 over cl
 (in wi as
 d
 if s riding th this bodies”.
 sing() an case
 enum
 the
 is
PHIL(“bass”);
 value of JERRY or B
OBBY.
private String instrument;
Names(String instrument) {
 This is the enum’s constructor. It runs
this.instrument = instrument;
 once for each declared enum value (in
}
 this case it runs three times).
public String getInstrument() {
return this.instrument;
}
public String sings() {
 You’ll see these methods being called from “main()”.
return “occasionally”;
}
}
public static void main(String [] args) {
 Every enum comes with a
for System.out.print(n);
 (Names n : Names.values()) {
 built-in whic
h is “values()” typically used
 method
 in a
System.out.print(“, instrument: “+ n.getInstrument()); “for” loop as shown.
System.out.println(“, sings: “ + n.sings());
}
}
}
 File Edit Window Help Bootleg
%java HfjEnum
Notice that the basic “sing()”
JERRY, BOBBY, instrument: instrument: rhythm lead guitar, guitar, sings: sings: plaintively
 hoarsely
 method is only called when the
PHIL, instrument: bass, sings: occasionally
 enum value has no constant-
%
 specific class body.
you are here�
 673
when arrays aren’t enough
Five-Minute
MysteryA Long Trip Home
Captain Byte of the Flatland starship “Traverser” had received an urgent, Top Secret transmission
from headquarters. The message contained 30 heavily encrypted navigational codes that the
Traverser would need to successfully plot a course home through enemy sectors. The enemy
Hackarians, from a neighboring galaxy, had devised a devilish code-scrambling ray that was capable
of creating bogus objects on the heap of the Traverser’s only navigational computer. In
addition, the alien ray could alter valid reference variables so that they referred to these
bogus objects. The only defense the Traverser crew had against this evil Hackarian ray was
to run an inline virus checker which could be imbedded into the Traverser’s state of the art
Java 6 code.
Captain Byte gave Ensign Smith the following programming instructions to process the critical
navigational codes:
“Put the first five codes in an array of type ParsecKey. Put the last 25 codes in a five by five, two
dimensional array of type QuadrantKey. Pass these two arrays into the plotCourse() method of the
public final class ShipNavigation. Once the course object is returned run the inline virus checker
against all the programs reference variables and then run the NavSim program and bring me the
results.”
A few minutes later Ensign Smith returned with the NavSim output. “NavSim output ready for
review, sir”, declared Ensign Smith. “Fine”, replied the Captain, “Please review your work”. “Yes
sir!”, responded the Ensign, “First I declared and constructed an array of type ParsecKey with the
following code; ParsecKey [] p = new ParsecKey[5]; , next I declared and constructed an array
of type QuadrantKey with the following code: QuadrantKey [] [] q = new QuadrantKey [5] [5]; .
Next, I loaded the first 5 codes into the ParsecKey array using a ‘for’ loop, and then I loaded the last
25 codes into the QuadrantKey array using nested ‘for’ loops. Next, I ran the virus checker against
all 32 reference variables, 1 for the ParsecKey array, and 5 for its elements, 1 for the QuadrantKey
array, and 25 for its elements. Once the virus check returned with no viruses detected, I ran the
NavSim program and re-ran the virus checker, just to be safe... Sir ! “
Captain Byte gave the Ensign a cool, long stare and said calmly, “Ensign, you are confined to
quarters for endangering the safety of this ship, I don’t want to see your face on this bridge again
until you have properly learned your Java! Lieutenant Boolean, take over for the Ensign and do this
job correctly!”
674
Why did the captain confine the Ensign to his quarters?
appendix B
puzzle answers
appendix B Top Ten Reference
Five-Minute Mystery Solution
A Long Trip Home
Captain Byte knew that in Java, multidimensional arrays are actu-
ally arrays of arrays. The five by five QuadrantKey array ‘q’, would
actually need a total of 31 reference variables to be able to access
all of its components:
1 - reference variable for ‘q’
5 - reference variables for q[0] - q[4]
25 - reference variables for q[0][0] - q[4][4]
The ensign had forgotten the reference variables for the five one
dimensional arrays embedded in the ‘q’ array. Any of those five
reference variables could have been corrupted by the Hackarian
ray, and the ensign’s test would never reveal the problem.
you are here�
 675
the index
h
 h
d
 Index
 h
 d
Symbols
 appendix beat box A final 649–658
 client 650
beat box final server 657
&, &&, |. || (boolean operators) 151, 660
appendix B
&, <<, >>, >>>, ^, |, ~ (bitwise operators) 660
access levels and modifiers 667
++ -- (increment/decrement) 105, 115
assertions 662
+ (String concatenation operator) 17
 bit manipulation 660
. (dot operator) 36
 block scope 663
immutability 661
reference 54
linked invocations 664
<, <=, ==,!=, >, >= (comparison operators) 86, 114,
multidimensional arrays 670
151
String and StringBuffer methods 669
<, <=, ==, >, >= (comparison operators) 11
apples and oranges 137
A
 arguments
 method 74, 76, 78
abandoned objects. See garbage collection
 polymorphic 187
abstract
 ArrayList 132, 133–138, 156, 208, 558
class 200–210
 API 532
class modifier 200
 ArrayList<Object> 211–213
abstract methods
 autoboxing 288–289
casting 229
declaring 203
arrays
access
about 17, 59, 135
and inheritance 180
assigning 59
class modifiers 667
compared to ArrayList 134–137
method modifiers 81, 667
creation 60
variable modifiers 81, 667
accessors and mutators. See getters and setters
 declaring 59
length attribute 17
ActionListener interface 358, 358–361
 multidimensional 670
addActionListener() 359–361
 objects, of 60, 83
advice guy 480, 484
 primitives, of 59
assertions
AeronTM 28
assertions 662
animation 382–385
assignments, primitive 52
API 154–155, 158–160
assignments, reference variables 55, 57, 83
ArrayList 532
collections 558
 atomic code blocks 510–512. See also threads
you are here� 677
the index
audio. See midi
autoboxing 288–291
and operators 291
assignments 291
B
bark different 73
bathtub 177
beat box 316, 347, 472. See also appendix A
beer 14
behavior 73
Bela Fleck 30
bitwise operators 660
bit shifting 660
block scope 663
boolean 51
boolean expressions 11, 114
logical 151
BorderLayout manager 370–371, 401, 407
BoxLayout manager 411
brain barbell 33, 167, 188
break statement 105
BufferedReader 454, 478
BufferedWriter 453
buffers 453, 454
byte 51
bytecode 2
C
Calendar 303–305
methods 305
casting
explicit primitive 117
explicit reference 216
implicit primitive 117
catching exceptions 326
678
 index
catch 338
catching multiple exceptions 329, 330, 332
try 321
catch blocks 326, 338
catching multiple exceptions 329, 330, 332
chair wars 28, 166
char 51
chat client 486
with threads 518
chat server (simple) 520
checked exceptions
runtime vs. 324
checking account. See Ryan and Monica
check box (JCheckBox) 416
class
abstract 200–210
concrete 200–210
designing 34, 41, 79
final 283
fully qualified names 154–155, 157
client/server 473
code kitchen
beat box save and restore 462
final beat box. See appendix A
making the GUI 418
music with graphics 386
playing sound 339
coffee cups 51
collections 137, 533
API 558
ArrayList 137
ArrayList<Object> 211–213
Collections.sort() 534, 539
HashMap 533
HashSet 533
LinkedHashMap 533
LinkedList 533
List 557
Map 557, 567
parameterized types 137
Set 557
TreeSet 533
Collections.sort() 534, 539
Comparator 551
compare() 553
Comparable 547, 566
and TreeSet 566
compareTo() method 549
Comparator 551, 566
and TreeSet 566
compare() 553
compareTo() 549
comparing with == 86
compiler 2
about 18
java -d 590
concatenate 17
concrete classes 200–210
conditional expressions 10, 11, 13
constants 282
constructors
about 240
chaining 250–256
overloaded 256
superclass 250–256
contracts 190–191, 218
cups 51
curly braces 10
D
daily advice client 480
daily advice server 484
dancing girl 316
dates
Calendar 303
methods 305
formatting 301
GregorianCalendar 303
the index
java.util.Date 303
deadlock 516
deadly diamond of death 223
declarations
about 50
exceptions 335–336
instance variables 50
default access 668
default value 84
deployment options 582, 608
deserialized objects 441. See also serialization
directory structures
packages 589
servlets 626
doctor 169
dot operator
reference 54
double 51
duck 277
construct 242
garbage collect 261
ducking exceptions 335
E
EJB 631
encapsulation
about 79–82
benefits 80
end of book 648
enumerations 671–672
enums 671–672
equality 560
and hashCode() 561
equals() 561
equals( )
about 209
Object class 209
you are here� 679
the index
event handling 357–361
event object 361
listener interface 358–361
using inner classes 379
event source 359–361
exceptions
about 320, 325, 338
catch 321, 338
catching multiple exceptions 329, 332
checked vs. runtime 324
declaring 335–336
ducking 335–336
finally 327
flow control 326
handle or declare law 337
propagating 335–336
remote exceptions 616
throwing 323–326
try 321, 338
executable JAR 585–586, 586
with packages 592, 592–593
exercises
be the... 88, 118, 266, 310, 395
code magnets 20, 43, 64, 119, 312, 349, 467,
524–525
honeypot 267
true or false 311, 348, 466, 602
what’s the declaration 231
what’s the picture 230
which layout manager? 424
who am I 45, 89, 394
Extreme Programming 101
F
File 452
FileInputStream 441. See also I/O
FileOutputStream 432
FileReader 454. See also I/O
files
File class 452
680
 index
reading from 441, 454
source file structure 7
writing to 432, 447
FileWriter 447
File class 452
final
class 189, 283
methods 189, 283
static variables 282
variables 282, 283
finally block 327
fireside chats
about 18
five minute mystery. See puzzles
float 51
FlowLayout 403, 408–410
flow control
exceptions 326
font 406
formatting
dates 301–302
format specifiers 295–296
argument 300
numbers 294–295
printf() 294
String.format() 294
for loops 105
fully qualified name 154, 157
packages 587
G
garbage collection
about 40
eligible objects 260–263
heap 57, 58
nulling references 58
reassigning references 58
generics 540, 542, 568–574
methods 544
wildcards 574
getters and setters 79
ghost town 109
giraffe 50
girl dreaming
inner classes 375
Java Web Start 596
girl in a tub 177
girl who isn’t getting it 182–188
graphics 364–366. See also GUI
Graphics2D class 366
Graphics object 364
GregorianCalendar 303
guessing game 38
GUI 406
about 354, 400
animation 382–385
BorderLayout 370–371, 401, 407
box layout 403, 411
buttons 405
components 354, 363–368, 400
event handling 357–361, 379
flow layout 403, 408
frames 400
graphics 363–367
ImageIcon class 365
JButton 400
JLabel 400
JPanel 400, 401
JTextArea 414
JTextField 413
layout managers 401–412
listener interface 358–361
scrolling (JScrollPane) 414
Swing 354
GUI Constants
ScrollPaneConstants.HORIZONTAL_SCROLL-
BAR_NEVER 415
ScrollPaneConstants.VERTICAL_SCROLLBAR_
ALWAYS 415
GUI methods
the index
drawImage() 365
fillOval() 365
fillRect() 364
gradientPaint(). See also GUI
paintComponent() 364
setColor() 364
setFont() 406
GUI Widgets 354
JButton 354, 405
JCheckBox 416
JFrame 354, 400, 401
JList 417
JPanel 400, 401
JScrollPane 414, 417
JTextArea 414
JTextField 413
H
HAS-A 177–181
hashCode() 561
HashMap 533, 558
HashSet 533, 558
Hashtable 558
heap
about 40, 57, 236–238
garbage collection 40, 57, 58
I
I/O
BufferedReader 454, 478
BufferedWriter 453
buffers 453
deserialization 441
FileInputStream 441
FileOutputStream 432
FileWriter 447
InputStreamReader 478
ObjectInputStream 441
ObjectOutputStream 432, 437
serialization 432, 434–439, 437, 446, 460
you are here� 681
the index
streams 433, 437
with sockets 478
if -else 13
if statement 13
immutability, Strings
immutability 661
implements 224
imports
static imports 307
import statement 155, 157
increment 105
inheritance
about 31, 166–192
and abstract classes 201
animals 170–175
IS-A 214, 251
super 228
initializing
instance variables 84
primitives 84
static variables 281
inner classes
about 376–386
events 379
inner class threesome 381
InputStreamReader 478
instance variables
about 34, 73
declaring 84
default values 84
initializing 84
life and scope 258–263
local variables vs. 236–238, 239
static vs. 277
instantiation. See objects
int 50
primitive 51
Integer. See wrapper
interfaces
682
 index
about 219–227
for serialization 437
implementing 224, 437
implementing multiple 226
java.io.Serializable 437
IP address. See networking
IS-A 177–181, 251
J
J2EE 631
JAR files
basic commands 593
executable 585–586, 592
manifest 585
running executable 586, 592
tool 593
with Java Web Start 598
Java, about 5, 6
javac. See compiler
Java in a Nutshell 158–159
java sound 317, 340
Java Web Start 597–601
jnlp file 598, 599
Jini 632–635
JNLP 598
jnlp file 599
JPEG 365
JVM
about 2, 18
JWS. See Java Web Start
K
keywords 53
L
l 264
layout managers 401–412
BorderLayout 370–371, 403, 407
BoxLayout 403, 411
FlowLayout 403, 408–410
lingerie, exceptions 329
LinkedHashMap 533, 558
LinkedHashSet 558
LinkedList 533, 558
linked invocations 664
List 557
listeners
listener interface 358–361
literals, assigning values
primitive 52
local
variables 85, 236, 236–238, 258–263
locks
object 509
threads 509
long 51
loops
about 10
break 105
for 105
while 115
lost update problem. See threads
M
main() 9, 38
make it stick 53, 87, 157, 179, 227, 278
manifest file 585
Map 557, 567
Math class
methods 274–278, 286
random() 111
memory
garbage collection 260–263
metacognitive tip 33, 108, 325
methods
about 34, 78
the index
abstract 203
arguments 74, 76, 78
final 283
generic arguments 544
on the stack 237
overloading 191
overriding 32, 167–192
return 75, 78
static 274–278
midi 317, 340–346, 387–390
midi sequencer 340–346
MINI Cooper 504
modifiers
class 200
method 203
multidimensional arrays 670
multiple inheritance 223
multiple threads. See threads
music. See midi
mystery. See puzzles
N
naming 53. See also RMI
classes and interfaces 154–155, 157
collisions 587
packages 587
networking
about 473
ports 475
sockets 475
new 55
null
reference 262
numbers
formatting 294–295
O
ObjectOutputStream 432, 437
you are here� 683
the index
objects
about 55
arrays 59, 60, 83
comparing 209
creation 55, 240–256
eligible for garbage collection 260–263
equality 560
equals() 209, 561
life 258–263
locks 509
Object class
about 208–216
equals() 561
hashCode() 561
overriding methods 563
object graph 436, 438
object references 54, 56
assignment 55, 262
casting 216
comparing 86
equality 560
nulling 262
polymorphism 185–186
OO
contracts 190–191, 218
deadly diamond of death 223
design 34, 41, 79, 166–191
HAS-A 177–181
inheritance 166–192
interfaces 219–227
IS-A 177–181, 251
overload 191
override 167–192
polymorphism 183, 183–191, 206–217
superclass 251–256
operators
and autoboxing 291
bitwise 660
comparison 151
conditional 11
decrement 115
684
 index
increment 105, 115
logical 151
shift 660
overload 191
constructors 256
override
about 32, 167–192
polymorphism. See polymorphism
P
packages 154–155, 157, 587–593
directory structure 589
organizing code 589
paintComponent() 364–368
parameter. See arguments
parameterized types 137
parsing an int. See wrapper
parsing text with String.split() 458
pass-by-copy. See pass-by-value
pass-by-value 77
phrase-o-matic 16
polymorphism 183–191
abstract classes 206–217
and exceptions 330
arguments and return types 187
references of type Object 211–213
pool puzzle. See puzzles
ports 475
prep code 99–102
primitives 53
== operator 86
autoboxing 288–289
boolean 51
byte 51
char 51
double 51
float 51
int 51
ranges 51
short 51
type 51
primitive casting
explicit primitive 117
printf() 294
PrintWriter 479
private
access modifier 81
protected 668
public
access modifier 81, 668
puzzles
five minute mystery 92, 527, 674
Java cross 22, 120, 162, 350, 426, 603
pool puzzle 24, 44, 65, 91, 194, 232, 396
Q
quiz card builder 448, 448–451
R
rabbit 50
random() 111
ready-bake code 112, 152–153, 520
reference variables. See object references
casting 216
registry, RMI 615, 617, 620
remote control 54, 57
remote interface. See RMI
reserved words 53
return types
about 75
polymorphic 187
values 78
risky code 319–336
RMI
about 614–622
client 620, 622
the index
compiler 618
Jini. See also Jini
Naming.lookup() 620
Naming.rebind(). See also RMI
registry 615, 617, 620
remote exceptions 616
remote implementation 615, 617
remote inteface 615, 616
rmic 618
skeleton 618
stub 618
UnicastRemoteObject 617
universal service browser 636–648
rmic. See RMI
run()
overriding in Runnable interface 494
Runnable interface 492
about 493
run() 493, 494
threads 493
runnable thread state 495
Ryan and Monica 505–506
introduction 505–506
S
scary objects 200
scheduling threads
scheduling 496–498
scope
variables 236–238, 258–263
scrolling (JScrollPane) 414
serialization 434–439, 446
deserialization 460
interface 437
ObjectInputStream. See I/O
objectOutputStream 432
objects 460
object graph 436
reading. See I/O
restoring 460. See also I/O
you are here� 685
the index
saving 432
serialVersionUID 461
transient 439
versioning 460, 461
writing 432
server
socket 483. See also socket
servlet 625–627
Set 557
importance of equals() 561
importance of hashCode() 561
short 51
short circuit logical operators 151
sink a dot com 96–112, 139–150
skeleton. See RMI
sleep() 501–503
sleeping threads 501–503
snowboard 214
socket
about 475
addresses 475
creating 478
I/O 478
ports 475
reading from 478
server 483
TCP/IP 475
writing to 479
sorting
Collections.sort() 534, 539, 547
Comparable interface 547, 549
Comparator 551, 553
TreeSet 564–566
source files
structure of 7
specifiers
format specifiers 295, 298
argument specifier 300
686
 index
stack
heap vs. 236
methods on 237
scope 236
threads 490
trace 323
static
enumerated types 671
initializer 282
Math class methods 274–278
methods 274–278
static imports 307
variables 282
streams 433. See also I/O
String
arrays 17
concatenating 17
methods 669
parsing 458
String.format() 294–297
String.split() 458
StringBuffer/StringBuilder
methods 669
stub. See RMI
subclass
about 31, 166–192
super 228
about 31
superclass
about 166–192, 214–217, 228
super constructor 250–256
Swing. See GUI
synchronized
methods 510. See also threads
syntax
about 10, 12
System.out.print() 13
System.out.println() 13
T
talking head 203
TCP ports 475
Telluride 30
testing
extreme programming 101
text
parsing with String.split() 458 458
read from a file. See also I/O
write to a file 447
text area (JTextArea) 414
text field (JTextField) 413
Thread.sleep() 501–503
threads
about 489–515
deadlock 516
locks 509
lost update problem 512–514
run() 493, 494
Runnable 492, 493, 494
Ryan and Monica problem 505–507
scheduling 496, 496–498
sleep() 501–503
stack 490–491
start() 492
starting 492
states 495, 496
summary 500, 517
synchronized 510–512
unpredictability 498–499
throw
exceptions 323–326
throws 323–326
transient 439
TreeMap 558
TreeSet 533, 558, 564–566, 566
the index
try
blocks 321, 326
type 50
parameter 137, 542, 544
type-safety 540
and generics 540
U
universal service browser 636–648
V
variables
assigning 52, 262
declaring 50, 54, 84, 236–238
local 85, 236–238
nulling 262
primitive 51, 52
references 54, 55, 56, 185–186
scope 236–238
static. See static
variable declarations 50
instance 84
primitive 51
reference 54
virtual method invocation 175
W
web start. See Java Web Start
while loops 11, 115
wildcard 574
wine 202
wrapper 287
autoboxing 288–289
conversion utilities 292
Integer.parseInt() 104, 106, 117
writing. See I/O
you are here� 687
the index
Don’t you know about the web site?
We’ve got answers to some of the
Sharpens, examples, the Code Kitchens,
Ready-bake Code, and daily updates
from the Head First author blogs!
This isn’t goodbye
Bring your brain over to
wickedlysmart.com
688
 index

